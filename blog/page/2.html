<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-list-page plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.4.0">
<title data-rh="true">Blog | Ayanami&#x27;s Cave</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://ayanami1314.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://ayanami1314.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://ayanami1314.github.io/blog/page/2"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" property="og:title" content="Blog | Ayanami&#x27;s Cave"><meta data-rh="true" name="description" content="Blog"><meta data-rh="true" property="og:description" content="Blog"><meta data-rh="true" name="docusaurus_tag" content="blog_posts_list"><meta data-rh="true" name="docsearch:docusaurus_tag" content="blog_posts_list"><link data-rh="true" rel="icon" href="/img/ayanami.jpg"><link data-rh="true" rel="canonical" href="https://ayanami1314.github.io/blog/page/2"><link data-rh="true" rel="alternate" href="https://ayanami1314.github.io/blog/page/2" hreflang="en"><link data-rh="true" rel="alternate" href="https://ayanami1314.github.io/blog/page/2" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"Blog","@id":"https://ayanami1314.github.io/blog/page/2","mainEntityOfPage":"https://ayanami1314.github.io/blog/page/2","headline":"Blog","description":"Blog","blogPost":[{"@type":"BlogPosting","@id":"https://ayanami1314.github.io/blog/精读 AAAI 2025 Fit and Prune Fast and Training-free Visual Token Pruning for Multi-modal Large Language Models","mainEntityOfPage":"https://ayanami1314.github.io/blog/精读 AAAI 2025 Fit and Prune Fast and Training-free Visual Token Pruning for Multi-modal Large Language Models","url":"https://ayanami1314.github.io/blog/精读 AAAI 2025 Fit and Prune Fast and Training-free Visual Token Pruning for Multi-modal Large Language Models","headline":"Paper reading - Fit and Prune Fast and Training-free Visual Token Pruning for Multi-modal Large Language Models","name":"Paper reading - Fit and Prune Fast and Training-free Visual Token Pruning for Multi-modal Large Language Models","description":"任务","datePublished":"2025-06-02T00:00:00.000Z","author":{"@type":"Person","name":"ayanami"},"keywords":[]},{"@type":"BlogPosting","@id":"https://ayanami1314.github.io/blog/精读：Eagle Exploring The Design Space for Multi- modal LLMs with Mixture of Encoders","mainEntityOfPage":"https://ayanami1314.github.io/blog/精读：Eagle Exploring The Design Space for Multi- modal LLMs with Mixture of Encoders","url":"https://ayanami1314.github.io/blog/精读：Eagle Exploring The Design Space for Multi- modal LLMs with Mixture of Encoders","headline":"Paper reading-Eagle Exploring The Design Space for Multi- modal LLMs with Mixture of Encoders","name":"Paper reading-Eagle Exploring The Design Space for Multi- modal LLMs with Mixture of Encoders","description":"nvidia的论文, 主要还是实践训练MLLM上的一堆经验","datePublished":"2025-06-02T00:00:00.000Z","author":{"@type":"Person","name":"ayanami"},"keywords":[]},{"@type":"BlogPosting","@id":"https://ayanami1314.github.io/blog/RAG的一些思考和细节","mainEntityOfPage":"https://ayanami1314.github.io/blog/RAG的一些思考和细节","url":"https://ayanami1314.github.io/blog/RAG的一些思考和细节","headline":"RAG的一些思考与细节","name":"RAG的一些思考与细节","description":"Langchain needle in haystack 实验","datePublished":"2025-05-30T00:00:00.000Z","author":{"@type":"Person","name":"ayanami"},"keywords":[]},{"@type":"BlogPosting","@id":"https://ayanami1314.github.io/blog/精读  Interleaved Scene Graph for Interleaved Text-and-Image Generation Assessment","mainEntityOfPage":"https://ayanami1314.github.io/blog/精读  Interleaved Scene Graph for Interleaved Text-and-Image Generation Assessment","url":"https://ayanami1314.github.io/blog/精读  Interleaved Scene Graph for Interleaved Text-and-Image Generation Assessment","headline":"Paper reading - Interleaved Scene Graph for Interleaved Text-and-Image Generation Assessment","name":"Paper reading - Interleaved Scene Graph for Interleaved Text-and-Image Generation Assessment","description":"开发了一个交错文本和图像生成综合评估框架ISG","datePublished":"2025-05-30T00:00:00.000Z","author":{"@type":"Person","name":"ayanami"},"keywords":[]},{"@type":"BlogPosting","@id":"https://ayanami1314.github.io/blog/ColBERT","mainEntityOfPage":"https://ayanami1314.github.io/blog/ColBERT","url":"https://ayanami1314.github.io/blog/ColBERT","headline":"ColBERT-后期交互方法","name":"ColBERT-后期交互方法","description":"如果简单引入语义搜索，那么第一时间想到的肯定是向量搜索的方法","datePublished":"2025-05-29T00:00:00.000Z","author":{"@type":"Person","name":"ayanami"},"keywords":[]},{"@type":"BlogPosting","@id":"https://ayanami1314.github.io/blog/2025/05/26/技术博客阅读 ","mainEntityOfPage":"https://ayanami1314.github.io/blog/2025/05/26/技术博客阅读 ","url":"https://ayanami1314.github.io/blog/2025/05/26/技术博客阅读 ","headline":"美团技术博客阅读","name":"美团技术博客阅读","description":"美团外卖基于GPU的向量检索系统实践","datePublished":"2025-05-26T00:00:00.000Z","author":{"@type":"Person","name":"ayanami"},"keywords":[]},{"@type":"BlogPosting","@id":"https://ayanami1314.github.io/blog/Milvus","mainEntityOfPage":"https://ayanami1314.github.io/blog/Milvus","url":"https://ayanami1314.github.io/blog/Milvus","headline":"稀疏神经嵌入","name":"稀疏神经嵌入","description":"下午在看milvus文档的时候看到着重提了稀疏检索，注意到bge-m3是有神经稀疏检索的支持的，于是学习了一下，下面属于纯入门笔记。","datePublished":"2025-05-25T00:00:00.000Z","author":{"@type":"Person","name":"ayanami"},"keywords":[]},{"@type":"BlogPosting","@id":"https://ayanami1314.github.io/blog/RocketMQ","mainEntityOfPage":"https://ayanami1314.github.io/blog/RocketMQ","url":"https://ayanami1314.github.io/blog/RocketMQ","headline":"RocketMQ学习","name":"RocketMQ学习","description":"mq: 异步，解耦，削峰填谷","datePublished":"2025-05-24T00:00:00.000Z","author":{"@type":"Person","name":"ayanami"},"keywords":[]},{"@type":"BlogPosting","@id":"https://ayanami1314.github.io/blog/cs186-database-WIP","mainEntityOfPage":"https://ayanami1314.github.io/blog/cs186-database-WIP","url":"https://ayanami1314.github.io/blog/cs186-database-WIP","headline":"ucb cs186 课程笔记(更新中)","name":"ucb cs186 课程笔记(更新中)","description":"lec2","datePublished":"2025-02-12T13:56:21.000Z","author":{"@type":"Person","name":"ayanami"},"keywords":["cs186","database"]},{"@type":"BlogPosting","@id":"https://ayanami1314.github.io/blog/jyy-os-虚拟化","mainEntityOfPage":"https://ayanami1314.github.io/blog/jyy-os-虚拟化","url":"https://ayanami1314.github.io/blog/jyy-os-虚拟化","headline":"NJU操作系统(jyy OS)课程笔记-虚拟化部分","name":"NJU操作系统(jyy OS)课程笔记-虚拟化部分","description":"lec14 操作系统上的进程","datePublished":"2025-02-12T13:56:21.000Z","author":{"@type":"Person","name":"ayanami"},"keywords":["os","jyy"]}]}</script><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Ayanami&#39;s Cave RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Ayanami&#39;s Cave Atom Feed">



<link rel="alternate" type="application/rss+xml" href="/personal-essays/rss.xml" title="Ayanami&#39;s Cave RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/personal-essays/atom.xml" title="Ayanami&#39;s Cave Atom Feed">


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.00dd3480.css">
<script src="/assets/js/runtime~main.48fa886f.js" defer="defer"></script>
<script src="/assets/js/main.b242fce0.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/logo.svg" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Ayanami&#x27;s Cave</b></a><a class="navbar__item navbar__link" href="/docs/Chcore源码阅读">课程笔记</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">技术博客</a><a class="navbar__item navbar__link" href="/personal-essays">个人随笔</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/facebook/docusaurus" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><div class="navbar__search searchBarContainer_NW3z" dir="ltr"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">All our posts</div><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/AI limu">李沐dl笔记</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/code-search&amp;code-embedding">从现代Coding Agent视角回看代码搜索与嵌入</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/llm for code paper notes">paper-reading, code&amp;rl方向</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/speculative-decode-overview">投机解码简述</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/Paper reading Context Pruning and beyond hard pruning">Paper reading - Context Pruning and beyond hard pruning</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/结构化输出">结构化输出与AI工具与Agent</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/context-engineering">context-engineering</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/从微调reranker到搜推">从微调reranker到搜推工程实践</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/llm-tech-report">部分llm技术报告的阅读</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/Ask in Any Modality A Comprehensive Survey on Multimodal Retrieval-Augmented Generation">Paper reading-Ask in Any Modality A Comprehensive Survey on Multimodal Retrieval-Augmented Generation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/精读 AAAI 2025 Fit and Prune Fast and Training-free Visual Token Pruning for Multi-modal Large Language Models">Paper reading - Fit and Prune Fast and Training-free Visual Token Pruning for Multi-modal Large Language Models</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/精读：Eagle Exploring The Design Space for Multi- modal LLMs with Mixture of Encoders">Paper reading-Eagle Exploring The Design Space for Multi- modal LLMs with Mixture of Encoders</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/RAG的一些思考和细节">RAG的一些思考与细节</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/精读  Interleaved Scene Graph for Interleaved Text-and-Image Generation Assessment">Paper reading - Interleaved Scene Graph for Interleaved Text-and-Image Generation Assessment</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/ColBERT">ColBERT-后期交互方法</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/2025/05/26/技术博客阅读">美团技术博客阅读</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/Milvus">稀疏神经嵌入</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/RocketMQ">RocketMQ学习</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/cs186-database-WIP">ucb cs186 课程笔记(更新中)</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/jyy-os-虚拟化">NJU操作系统(jyy OS)课程笔记-虚拟化部分</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/local-llm">来本地部署大模型!</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/ostep-chapter42-44">ostep阅读笔记：单机fs的崩溃一致性(chapter42-44)</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/系统架构设计笔记">system-design-interview笔记</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/JUC">JUC</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/cs144 labs">cs144 labs(Winter 2024)</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/jyy-os：并发">NJU操作系统(jyy OS)课程笔记-并发部分</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/nginx">nginx基础</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/cs144/cs144 lec notes">CS144 Lecture Notes</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/django-mosh">Django_mosh</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/splay-tree">splay tree</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/xv6book-notes">xv6book Notes(C1-4)</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/Go-Gin学习">Go,Gin学习</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/godis源码阅读">godis源码阅读</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/hibernate-jpa">hibernate&amp;jpa</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/linking-复习">linking 复习</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/ts基础">ts基础</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/实战2-mosh-gamehub">react practice:mosh gamehub</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/浅入理解断点和调试器">浅入理解断点和调试器</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/黑马点评">黑马点评(速通版)</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/js基础">js基础</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/11-14-11-26学习双周记">11-14-11-26学习双周记</a></li></ul></nav></aside><main class="col col--7"><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/blog/精读 AAAI 2025 Fit and Prune Fast and Training-free Visual Token Pruning for Multi-modal Large Language Models">Paper reading - Fit and Prune Fast and Training-free Visual Token Pruning for Multi-modal Large Language Models</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-06-02T00:00:00.000Z">June 2, 2025</time> · <!-- -->10 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><div class="avatar__intro"><div class="avatar__name"><span>ayanami</span></div></div></div></div></div></header><div class="markdown"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="任务">任务<a class="hash-link" aria-label="Direct link to 任务" title="Direct link to 任务" href="/blog/page/2#任务">​</a></h2>
<p>当前MLLM依赖于<strong>大量的视觉token</strong>做出高精度的视觉推理，例如LLaVa使用576 image patches as visual tokens，这相较于纯文本带来了<strong>6.2</strong>倍的计算时长开销。此外，一些其他工作正在使用提高图像分辨率的方法来缓解MLLM的视觉缺陷，但<strong>进一步加剧了计算量</strong>。</p>
<p>作者想要得到一种方法来在MLLM的<strong>图像token输入</strong>中，进行压缩，从而进行<strong>推理时的加速</strong>，且不能太影响下游任务精度。</p>
<p>同时，作者认为先前的方法依赖于大量的实验来确定超参数，他提出的方法需要具有一定的<strong>泛化能力</strong>，并且<strong>超参数确认简单</strong> <code>can be obtained in about 5 minutes for all VL tasks</code></p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="motivation">motivation<a class="hash-link" aria-label="Direct link to motivation" title="Direct link to motivation" href="/blog/page/2#motivation">​</a></h2>
<ol>
<li>大规模视觉token在MLLM中的存在明显的冗余，MLLMs 的多头注意力机制是单向的，而非真正“全局”的。简而言之，MLLMs 仅将信息从前一个标记传递到后一个标记，其视觉标记通常置于文本问题之前。在这种情况下，它们主要作用是为文本标记提供视觉语义，但实际上其中大部分并未被激活。</li>
</ol>
<p><img decoding="async" loading="lazy" src="https://pic4.zhimg.com/v2-8df5d0c2852ba56e89010c2fe91b9bb9_1440w.jpg" alt="img" class="img_ev3q"></p>
<hr>
<p>如图，大部分蓝色部分（不相关语义）实际上几乎不参加推理，图像到文本注意力非常集中。</p>
<p><img decoding="async" loading="lazy" alt="image-20250527131819120" src="/assets/images/image-20250527131819120-c14d369820575726608d4915d598be9a.png" width="813" height="618" class="img_ev3q"></p>
<hr>
<ol start="2">
<li>作者将确定压缩比例这一超参数的问题转换成一个统计问题。将压缩问题转换为这样的问题：<strong>给定一个采样样本集合</strong><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.02778em">D</span></span></span></span>, 再给定一个计算开销<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi></mrow><annotation encoding="application/x-tex">\delta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.03785em">δ</span></span></span></span> ，设压缩策略为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi></mrow><annotation encoding="application/x-tex">P</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P</span></span></span></span>， 目标是找到<strong>一个压缩比够大</strong>（满足计算开销到<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi></mrow><annotation encoding="application/x-tex">\delta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.03785em">δ</span></span></span></span>以下）的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi></mrow><annotation encoding="application/x-tex">P</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P</span></span></span></span>，<strong>使得在<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.02778em">D</span></span></span></span>上整体的注意力分布变化最小</strong></li>
</ol>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="方法">方法<a class="hash-link" aria-label="Direct link to 方法" title="Direct link to 方法" href="/blog/page/2#方法">​</a></h2>
<p><strong>作者只对多头注意力层进行修剪</strong></p>
<p><img decoding="async" loading="lazy" alt="image-20250527155650905" src="/assets/images/image-20250527155650905-1e815d1abeb04e6857086fe406cca405.png" width="1609" height="741" class="img_ev3q"></p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="得到修剪策略">得到修剪策略<a class="hash-link" aria-label="Direct link to 得到修剪策略" title="Direct link to 得到修剪策略" href="/blog/page/2#得到修剪策略">​</a></h2>
<p>对于采样样本集<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.02778em">D</span></span></span></span>, 计算每一层的视觉token自注意力和视觉-文本交叉注意力。假设视觉token数N，文本token数M，第i层的第j个视觉token的平均注意力为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>a</mi><mrow><mi>s</mi><mo separator="true">,</mo><mi>c</mi></mrow><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow></msubsup><mo>=</mo><msubsup><mo>∑</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><msubsup><mi>A</mi><mrow><mi>m</mi><mo separator="true">,</mo><mi>j</mi></mrow><mi>i</mi></msubsup></mrow><annotation encoding="application/x-tex">a_{s,c}^{i,j}=\sum_{m=1}^{N}A_{m,j}^{i}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2078em;vertical-align:-0.3831em"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8247em"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">s</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">c</span></span></span></span><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3831em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.376em;vertical-align:-0.3948em"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9812em"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">N</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2997em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8247em"><span style="top:-2.4413em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3948em"><span></span></span></span></span></span></span></span></span></span>, s和c分别代表自注意和交叉注意，A代表是在<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.02778em">D</span></span></span></span>上取的平均</p>
<p>移除策略P可以建模成<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><msubsup><mi>t</mi><mn>1</mn><mo>∗</mo></msubsup><mo separator="true">,</mo><msubsup><mi>t</mi><mn>2</mn><mo>∗</mo></msubsup><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><msubsup><mi>t</mi><mi>k</mi><mo>∗</mo></msubsup><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[t_1^*, t_2^*,...t_k^*]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0331em;vertical-align:-0.2831em"></span><span class="mopen">[</span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6887em"><span style="top:-2.4519em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">∗</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2481em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6887em"><span style="top:-2.4519em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">∗</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2481em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">...</span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6887em"><span style="top:-2.4169em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k</span></span></span><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">∗</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2831em"><span></span></span></span></span></span></span><span class="mclose">]</span></span></span></span> (假设模型有k层)</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>t</mi><mi>i</mi><mo>∗</mo></msubsup></mrow><annotation encoding="application/x-tex">t_i^*</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9474em;vertical-align:-0.2587em"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6887em"><span style="top:-2.4413em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">∗</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2587em"><span></span></span></span></span></span></span></span></span></span>表示在<strong>第i层新移除的token数量</strong>，注意前面层移除的token也不会传递给后面层，也就是说移除的总数是单调增的</p>
<p>采用一个注意力相差阈值<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span></span></span></span>和计算开销<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi></mrow><annotation encoding="application/x-tex">\delta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.03785em">δ</span></span></span></span>两者一起控制裁剪，具体来说，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi></mrow><annotation encoding="application/x-tex">\delta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.03785em">δ</span></span></span></span>是提前给定的，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span></span></span></span>是二分查找计算出来的值</p>
<hr>
<p><img decoding="async" loading="lazy" alt="height:600 width:500" src="/assets/images/image-20250527153710785-90079defd43229432666e6978e287317.png" width="746" height="932" class="img_ev3q"></p>
<hr>
<p>用通俗的话翻译就是:</p>
<ol>
<li>将注意力分布的差别简化为平均每个token的自注意力/交叉注意力<strong>之和</strong>的差别，即是否删除某个token，注意力和的<strong>相对变化</strong>需要小于阈值<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span></span></span></span></li>
<li>由于只计算和，所以可以对自注意力、交叉注意力两个集合分别按照大小排序 —— 注意力分布变化最小的保证转化为，总是优先考虑删除注意力最小的token</li>
<li>给定一个阈值<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span></span></span></span>, 对于每一层遍历，对于自注意力、交叉注意力分别不断尝试删除token，直到注意力变化达到阈值<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span></span></span></span>, 而这一层最后的策略P，即token删除数量为自注意力删除集合<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>T</mi><mi>s</mi></msub></mrow><annotation encoding="application/x-tex">T_s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>和交叉注意力集合<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>T</mi><mi>c</mi></msub></mrow><annotation encoding="application/x-tex">T_c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>的<strong>交集的大小</strong></li>
<li>现在有了一个删除策略<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi></mrow><annotation encoding="application/x-tex">P</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P</span></span></span></span>, 计算它是否满足计算开销约束（文中并没有具体说是怎么计算的，应该是根据模型的删除后token和参数量估算FLOPS，或者是某种直接测量计算量的工具，用的显卡是单张A100）</li>
</ol>
<hr>
<ol start="5">
<li>
<p>如果满足，说明删除策略<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi></mrow><annotation encoding="application/x-tex">P</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P</span></span></span></span>是可行的，但说不定<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span></span></span></span>太大删除太多了，需要调小<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span></span></span></span>；如果不满足，说明删除策略<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi></mrow><annotation encoding="application/x-tex">P</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P</span></span></span></span>不可行，说明<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span></span></span></span>太小了，需要调大<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span></span></span></span>。因此，二分查找<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span></span></span></span>直到找到一个满足计算开销约束的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span></span></span></span>，且这个<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span></span></span></span>的左右区间长度小于阈值<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">ϵ</span></span></span></span>(后文实验是0.01)，则这个<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span></span></span></span>对应的删除策略<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi></mrow><annotation encoding="application/x-tex">P</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P</span></span></span></span>就是最终的删除策略。</p>
</li>
<li>
<p>最后效果是在满足计算开销约束<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi></mrow><annotation encoding="application/x-tex">\delta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.03785em">δ</span></span></span></span>的情况下，尽可能保留更多的视觉token</p>
</li>
</ol>
<hr>
<p>关于这样的算法最后带来的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mo>−</mo><mi>α</mi></mrow><annotation encoding="application/x-tex">\delta - \alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.0833em"></span><span class="mord mathnormal" style="margin-right:0.03785em">δ</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span></span></span></span>关系，作者附了这么一个曲线</p>
<p><img decoding="async" loading="lazy" alt="image-20250527162141135" src="/assets/images/image-20250527162141135-8bdddf8722d17a88c421e695dc843834.png" width="534" height="458" class="img_ev3q"></p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="根据策略在推理时修剪">根据策略在推理时修剪<a class="hash-link" aria-label="Direct link to 根据策略在推理时修剪" title="Direct link to 根据策略在推理时修剪" href="/blog/page/2#根据策略在推理时修剪">​</a></h2>
<p>在实际推理时，作者将得到的删除策略<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi></mrow><annotation encoding="application/x-tex">P</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P</span></span></span></span>应用到模型中。具体来说，对于每一层的视觉token，按照<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi></mrow><annotation encoding="application/x-tex">P</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P</span></span></span></span>中给定的删除数量进行修剪。</p>
<p>具体删除哪些token呢？作者的方法是，</p>
<p>对于第i层</p>
<p>计算第i层<strong>剩余</strong>视觉token j的自注意力和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>a</mi><mi>s</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow></msubsup></mrow><annotation encoding="application/x-tex">a_s^{i,j}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0717em;vertical-align:-0.247em"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8247em"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em"><span></span></span></span></span></span></span></span></span></span>和交叉注意力和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>a</mi><mi>c</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow></msubsup></mrow><annotation encoding="application/x-tex">a_c^{i,j}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0717em;vertical-align:-0.247em"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8247em"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em"><span></span></span></span></span></span></span></span></span></span>，然后将这两个和的<strong>乘积</strong>作为用于排序的参考，排序之后<strong>去除</strong>最小的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span></span></span></span>个token（<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span></span></span></span>是删除数量）</p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="实验结果">实验结果<a class="hash-link" aria-label="Direct link to 实验结果" title="Direct link to 实验结果" href="/blog/page/2#实验结果">​</a></h2>
<p>作者使用 LLaVA-655k 数据集（Liu et al. 2023b）中的 655 个样本（0.1%）来生成剪枝策略</p>
<p>在LLaVA, LLaVA-HR,LLaVA-NEXT三个具有不同大小的视觉token（7B模型，576，1024，2880 tokens）的模型上进行测试，十余个下游任务数据集上进行测试</p>
<hr>
<p><img decoding="async" loading="lazy" alt="image-20250527160437182" src="/assets/images/image-20250527160437182-35bb2b550fd308086647c96c255c4f7b.png" width="1789" height="979" class="img_ev3q"></p>
<hr>
<p>可以看到，剪枝之后，在保持准确率几乎不下降  的情况下， 能够带来计算量的大幅下降</p>
<p>作者还做了其他几组实验</p>
<ol>
<li>
<p><strong>视觉冗余在不同层级的变化</strong></p>
<p>采用在不同层级上，随机删除裁剪视觉Token的方法。作者发现，深层次token的冗余度更高，裁剪深层次token几乎不影响准确度，可视化图也表明深层次的注意力几乎集中在最关键的元素中。但具体到每一层的最佳剪枝比例，层间也有比较大的不同</p>
</li>
</ol>
<hr>
<p><img decoding="async" loading="lazy" alt="image-20250527161223832" src="/assets/images/image-20250527161223832-a7dedefa5a7d792b5b4df9d98c4c3abb.png" width="916" height="409" class="img_ev3q"></p>
<hr>
<p><img decoding="async" loading="lazy" alt="image-20250527161358014" src="/assets/images/image-20250527161358014-df758aa4a01ac89ba28d6406b516bf03.png" width="689" height="509" class="img_ev3q"></p>
<hr>
<ol start="2">
<li>
<p><strong>与baseline的对比</strong></p>
<p>对比了FastV和ToMe两种裁剪方法，表明了自身的SOTA性质。同时指出，在裁剪程度低的时候大家都差不多，裁剪程度高的时候才显露方法的性能差距</p>
<p><img decoding="async" loading="lazy" alt="image-20250527161538762" src="/assets/images/image-20250527161538762-cf4680fadb2324a68d23a06ed98d886e.png" width="1783" height="806" class="img_ev3q"></p>
</li>
</ol>
<hr>
<ol start="3">
<li>
<p><strong>样本数量的消融实验</strong></p>
<p>作者将&quot;LLaVA-655k 数据集（Liu et al. 2023b）中的 655 个样本（0.1%）来生成剪枝策略&quot; 换成1%的数据，发现性能相当。<strong>作者进一步推测MLLM层间信息交换的模式可能更多地依赖于模型本身的特性</strong>，而在不同的输入样本上有较高的泛化性，FitPrune 方法可以有效地捕捉这种模式。同时下面的表还表明，这个方法有着很强的少样本泛化性，确实是模型的特性而不是样本数据集的特性，在仅有10个样本的时候也能得到非常优秀的策略</p>
</li>
</ol>
<p><img decoding="async" loading="lazy" alt="image-20250527162201012" src="/assets/images/image-20250527162201012-c5d2cddb5c89b8435daf12cf36c90db6.png" width="632" height="316" class="img_ev3q"></p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="结论">结论<a class="hash-link" aria-label="Direct link to 结论" title="Direct link to 结论" href="/blog/page/2#结论">​</a></h2>
<p>作者介绍了一种FitPrune的无训练方法，用于对 MLLMs 进行视觉标记剪枝。通过将标记剪枝问题表述为一个统计问题，FitPrune 旨在最小化注意力分布的偏差，从而实现冗余视觉token的高效剪枝，进而提高计算效率。FitPrune 能够基于少量数据生成最优的剪枝策略，避免了昂贵的手动试验。</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/mllm">mllm</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/ai">ai</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/llm">llm</a></li></ul></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/blog/精读：Eagle Exploring The Design Space for Multi- modal LLMs with Mixture of Encoders">Paper reading-Eagle Exploring The Design Space for Multi- modal LLMs with Mixture of Encoders</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-06-02T00:00:00.000Z">June 2, 2025</time> · <!-- -->8 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><div class="avatar__intro"><div class="avatar__name"><span>ayanami</span></div></div></div></div></div></header><div class="markdown"><p>nvidia的论文, 主要还是实践训练MLLM上的一堆经验</p>
<hr>
<h1>任务</h1>
<p>探究通过使用<strong>不同的视觉编码器 和分辨率</strong>来提高MLLM系统性能的不同设计带来的效果</p>
<hr>
<h1>motivation</h1>
<ol>
<li>解读<strong>高分辨率的精细视觉信息</strong>是MLLM重要的课题，常用的CLIP-ViT 预训练时候的分辨率只有如224*224或者336*336，<strong>对OCR等细粒度信息不够好</strong></li>
<li>近期研究发现<code>enhanced visual perception</code>显著减少幻觉和提高性能，许多近期MLLM用了混合视觉编码器<!-- -->
<ul>
<li>有扩大视觉编码器的预训练量和参数的</li>
<li>有将高分辨率编码器和CLIP融合的</li>
<li>也有更复杂的融合和路由，根据任务选用不同编码器，&quot;视觉MoE&quot;的</li>
</ul>
</li>
<li>但缺乏对此类方法设计的通用考量, 以及综合性的大benchmark</li>
</ol>
<hr>
<h1>方法</h1>
<ol>
<li>对<strong>不同的视觉编码器</strong>进行<strong>基准测试</strong>，寻找更<strong>高分辨率自适应</strong>的方案</li>
<li>对<strong>不同的视觉编码器混合策略</strong>做同类比较(论文将近期的混合策略归为了CC,SA,LH等几类)</li>
<li>寻找多个视觉编码器的<strong>最优组合</strong></li>
<li>改进<strong>pre-alignment</strong>和数据混合</li>
</ol>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="增加输入分辨率的做法">增加输入分辨率的做法<a class="hash-link" aria-label="Direct link to 增加输入分辨率的做法" title="Direct link to 增加输入分辨率的做法" href="/blog/page/2#增加输入分辨率的做法">​</a></h2>
<ul>
<li>Tiling 将输入分割为子图，CLIP-ViT单独编码</li>
<li>直接放大输入分辨率，并对位置编码进行进行插值</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="eagle做的实验">Eagle做的实验：<a class="hash-link" aria-label="Direct link to Eagle做的实验：" title="Direct link to Eagle做的实验：" href="/blog/page/2#eagle做的实验">​</a></h2>
<p>预训练，LLaVA-1.5 + CLIP 基础模型，和LLaVA相  同的 595k 图文对，<strong>冻结整个模型，只训练projection layer</strong></p>
<p>SFT： 1809k 多模态对话数据</p>
<p>评估：11个任务，包含VQA任务， OCR/文档/图表理解，视觉中心任务，基于知识的任务</p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="结果---strong-clip">结果 - Strong CLIP<a class="hash-link" aria-label="Direct link to 结果 - Strong CLIP" title="Direct link to 结果 - Strong CLIP" href="/blog/page/2#结果---strong-clip">​</a></h2>
<ol>
<li>
<p><strong>如果插值，需要unfrozen视觉编码器，否则损害性能</strong>。这个结论和以前实验不同。</p>
</li>
<li>
<p>输入分辨率和预训练分辨率差越大，插值越掉点</p>
</li>
<li>
<p>672分辨率下，插值和子图方法性能差不多，但是考虑效率的话还是插值更好</p>
</li>
<li>
<p>进行分辨率adaption，300M的CLIP-ViT性能接近6B的InternVL</p>
</li>
</ol>
<p><strong>按照下表，nvidia着重提了448*448+解锁视觉编码器的方案，300M就达到非常接近SOTA的性能了。</strong></p>
<hr>
<p><img decoding="async" loading="lazy" alt="image-20250601233933871" src="/assets/images/image-20250601233933871-a091334ceb9515da01d836fd92b8ea12.png" width="1166" height="592" class="img_ev3q"></p>
<hr>
<h1>Vision Encoder</h1>
<p>选取了以下的encoder</p>
<ul>
<li>
<p>视觉语言对比学习的视觉Encoder，比如CLIP的ViT和OpenCLIP的ConxNeXt；</p>
</li>
<li>
<p>以目标检测为中心的任务预训练的视觉Encoder，EVA-02</p>
</li>
<li>
<p>OCR上训练的Pix2Struct</p>
</li>
<li>
<p>分割上预训练的SAM</p>
</li>
<li>
<p>自监督训练的DINO-V2</p>
</li>
</ul>
<p>对不同预训练的视觉encoder输出的特征图进行resize和插值，使得视觉token数量相同.</p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="结果">结果：<a class="hash-link" aria-label="Direct link to 结果：" title="Direct link to 结果：" href="/blog/page/2#结果">​</a></h2>
<p><img decoding="async" loading="lazy" alt="image-20250601234936395" src="/assets/images/image-20250601234936395-09f01aa111506ba3b9c5c12a40b0d2e0.png" width="1136" height="643" class="img_ev3q"></p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="分析">分析：<a class="hash-link" aria-label="Direct link to 分析：" title="Direct link to 分析：" href="/blog/page/2#分析">​</a></h2>
<ul>
<li>在freeze的情况下他们通常能<strong>在和自己预训练任务相近的MLLM benchmark上实现最佳性能</strong>。例如来自CLIP的ConvNeXt进行了图文对齐，因此在TextVQA、SQA任务上时所有编码器里表现的最好的。而Text Recognition任务上训练所得的Pix2Struct视觉编码器，在OCR任务上是表现的最好的。</li>
<li>当跟随CLIP-ViT高分辨率拓展策略，<strong>unfreeze视觉编码器时，基本都能有性能提升</strong>，也有反超对应domain上训练的视觉编码器的可能性，例如CLIP-ConvNeXt微调后在OCR上性能超过了Pix2Struct。</li>
</ul>
<hr>
<h1>融合策略：</h1>
<p><img decoding="async" loading="lazy" src="https://arxiv.org/html/2408.15998v2/x2.png" alt="Refer to caption" class="img_ev3q"></p>
<hr>
<ul>
<li>序列维度拼接：SA sequence append</li>
<li>通道维度拼接：CC concat channel</li>
<li>LLAVA-HR式：LH 将高分辨率特征使用adapter注入低分辨率特征中，维持序列长度、通道维度不变</li>
<li>Mini-Gemini式：MG 将高分辨率特征使用local windows cross attention注入到低分辨率的queries中。</li>
<li>Deformable Attention式：DA 将MG的local windows变成了Deformable Attention</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="结果-1">结果：<a class="hash-link" aria-label="Direct link to 结果：" title="Direct link to 结果：" href="/blog/page/2#结果-1">​</a></h2>
<p><img decoding="async" loading="lazy" alt="image-20250601235208565" src="/assets/images/image-20250601235208565-66c205343b77aadcdafb4b386a8db120.png" width="947" height="415" class="img_ev3q"></p>
<ul>
<li>
<p>融合策略越复杂，性能的提升似乎越差，<strong>简单的SA/CC稳定涨点</strong></p>
</li>
<li>
<p>由于SA需要处理边长的序列长度，所以后面用CC</p>
</li>
</ul>
<hr>
<h1>Pre-Alignment</h1>
<p><img decoding="async" loading="lazy" src="https://arxiv.org/html/2408.15998v2/x3.png" alt="Refer to caption" class="img_ev3q"></p>
<p>考虑对其他的视觉专家进行预先的文本模态对齐，再学会去融合不同视觉专家的特征。因此在目前的两阶段MLLM训练框架之前，添加了一个vision-language pre-alignment training阶段，首先使用next-token prediction监督每个视觉专家的特征+各自<strong>单独的</strong>projector（与LLaVA原始预训练策略不同）训练，让其与一个冻结的较小语言模型对齐。</p>
<hr>
<ul>
<li><strong>进行一个额外的预先对齐，可以比较好提升MLLM性能。</strong></li>
<li>预对齐后，再合并所有的视觉专家，训练projector和encoder</li>
<li>虽然在 SFT 期间解冻视觉专家有助于通过更新视觉专家以适应语言模型来提高性能，但<em>预对齐</em>策略更有效地减轻了每位视觉专家的固有偏差，并稳定了训练过程，从而提高了整体性能 （<strong>unfreeze + pre-align效果加性</strong>）</li>
</ul>
<hr>
<h1>Fusion choice</h1>
<p><img decoding="async" loading="lazy" src="https://arxiv.org/html/2408.15998v2/x4.png" alt="w h:600" class="img_ev3q"></p>
<hr>
<p>采用上述的3阶段训练和最好最简单的Channel concat策略，就可以进一步研究哪种视觉编码器组合最好。组合的策略是依次增加模型视觉编码器的数量，每次的选择基于上一个数量下最好的组合进行进一步添加。<strong>四到五个编码器（X4, X5）目前看来就已经比较合适了。</strong></p>
<p>最佳组合是 <em>CLIP</em> 、 <em>ConvNeXt</em> 、 <em>SAM</em> 、 <em>Pix2Struct</em> 和 <em>EVA-02</em></p>
<hr>
<h1>最终和benchmark的比较</h1>
<p><img decoding="async" loading="lazy" src="https://arxiv.org/html/2408.15998v2/x1.png" alt="Refer to caption" class="img_ev3q"></p>
<hr>
<h1>高分辨率的文档任务的展示: 红色baseline失败，蓝色eagle成功</h1>
<p><img decoding="async" loading="lazy" src="https://arxiv.org/html/2408.15998v2/x5.png" alt="h:600" class="img_ev3q"></p>
<hr>
<h1>结论</h1>
<ol>
<li>MLLM训练期间<strong>解锁视觉编码器</strong>matters</li>
<li>设计<strong>先进的融合策略并不能较简单的通道级联显露优势</strong></li>
<li>更多的视觉专家<strong>MoE能带来持续增益</strong>，是增强MLLM能力的有效途径</li>
<li>视觉专家如果开始时候设计的任务和文本无关（没有对齐），用<strong>冻结的LLM进行预对齐</strong>（+解锁）后再整体训练能显著提升性能</li>
</ol></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/llm">llm</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/ai">ai</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/mllm">mllm</a></li></ul></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/blog/RAG的一些思考和细节">RAG的一些思考与细节</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-05-30T00:00:00.000Z">May 30, 2025</time> · <!-- -->13 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><div class="avatar__intro"><div class="avatar__name"><span>ayanami</span></div></div></div></div></div></header><div class="markdown"><h4 class="anchor anchorWithStickyNavbar_LWe7" id="langchain-needle-in-haystack-实验">Langchain needle in haystack 实验<a class="hash-link" aria-label="Direct link to Langchain needle in haystack 实验" title="Direct link to Langchain needle in haystack 实验" href="/blog/page/2#langchain-needle-in-haystack-实验">​</a></h4>
<p>长上下文之后，越后面的部分的事实性细节越容易找，尤其是多事实的情况下</p>
<p>引发的一个思考是 rerank 时是否需要将最关注的块放在 prompt 的最后面，也就是倒序？</p>
<ul>
<li>后补: 但其实又有attention sink相关的研究，可能还是需要具体任务具体测试分析</li>
</ul>
<p><img decoding="async" loading="lazy" alt="image-20250417222048515" src="/assets/images/image-20250417222048515-4b8dca1267f9a233e00e99645f943160.png" width="836" height="411" class="img_ev3q"></p>
<p>Maybe <strong>recency bias</strong> in LLMs：只记得最近的了</p>
<p>No retrieval guarantees</p>
<p><img decoding="async" loading="lazy" alt="image-20250417222613216" src="/assets/images/image-20250417222613216-ca6cf44bb533faa8009e9abf8e801437.png" width="1221" height="568" class="img_ev3q"></p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="query-analysis将-question-联系到正确的文档">query analysis：将 question 联系到正确的文档<a class="hash-link" aria-label="Direct link to query analysis：将 question 联系到正确的文档" title="Direct link to query analysis：将 question 联系到正确的文档" href="/blog/page/2#query-analysis将-question-联系到正确的文档">​</a></h4>
<p>routing (to right DB)</p>
<p><code>full doc -&gt; summary -&gt; embedding</code>： doc 中噪声非常大, summary 是必要的，语义层次的保留 level 通过 prompt 保证</p>
<p>self-reflection 听起来很美好，但实际常常用不到，太慢了，并且搜不出来更多是前期处理没做好，再换着花样也很难搜出来</p>
<p>HyDE 对于高度 Domain Knowledge 和抽象性理解的任务基本没用:</p>
<p>一些自己的解释</p>
<ol>
<li>能否生成正确的假设文档， 难</li>
<li>即使通过先行的小批量搜索教导 LLM 根据这些 example 生成假设文档，也很难让 LLM 从这些文档中抽取某个泛化的问题，经常会 <strong>过度 specific 而导致后续漏掉文档</strong></li>
<li>目前实验下来垂域脏文档类型最好的解决方案还是 reranker，embedder 如果不微调分布太接近了，例如全部的 chunk 都在 0.5~0.6 之间，意义寥寥</li>
</ol>
<p>和数据分析的结合:</p>
<p><code>分析波动-&gt;(数据分析)找出波动的阶段-&gt; 对每个波动的阶段做查询</code></p>
<p>GraphRag 这种 KG-based 的方法经常强调“对整个数据集信息的整合”</p>
<p>但这个要分领域，例如，个人知识库之中，这是好的</p>
<p>但垂域的知识文档常常是相似的格式，固定的路由，同时信息的整合关键不在“多实体”的关系上，而是在于“单个实体随时间的变化”上。</p>
<p>又或者说实体关系 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mo stretchy="false">(</mo><msub><mi>e</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>e</mi><mn>2</mn></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">R(e_1, e_2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.00773em">R</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> 本身应该建模成一个包含时间的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mo stretchy="false">(</mo><msub><mi>e</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>e</mi><mn>2</mn></msub><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">R(e_1, e_2, t)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.00773em">R</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">t</span><span class="mclose">)</span></span></span></span></p>
<p>如果仅仅是靠新加入的文档来动态更新 KG 的话，滞后性会很强</p>
<p>在这种半结构化的模板式文档中，LLM 实际上在干一个 Fuzzy DB manager, 提取信息，充当一个搜索引擎</p>
<p>利用 KG 进行某种意义上的多跳推理本质上也只是对文档的多次检索，推理跳数越多，关系越复杂，离线生成 KG 就越难，不是所有领域都像是法律一样有一个明确的 A 判例引用 BCD 法条的连接关系的，这样复杂的 KG 在要想随时间变化也更不可能</p>
<p>从某种意义上来说，KG 是在横向生成，而类似金融这种领域的 RAG 做的是纵向的 Timeline, 这部分对于关键实体是有数据的，并且可能数据都不需要自己做（例如各种行情的图），而离线准备好这些 timeline 之后，如何在 timeline 上进行一个跳跃和查询分析才是关键的。</p>
<p>如果从 DB 的角度上分析的话，金融领域这种关注点快速变化的 RAG 系统（with cache）也就相当于 lazy generated timeseries DB 了，例如问了一个 A 的价格变化，就像是生成了一个 <code>time, delta_price, event(detail)</code> 的 timeseries DB 表，把生成 reason 这样的 LLM 工作 lazy 化了而已</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="chunk-的前总结和后总结离线在线">chunk 的前总结和后总结(离线在线)<a class="hash-link" aria-label="Direct link to chunk 的前总结和后总结(离  线在线)" title="Direct link to chunk 的前总结和后总结(离线在线)" href="/blog/page/2#chunk-的前总结和后总结离线在线">​</a></h4>
<p>离线总结最大的问题在于总结哪些方面，实际上是文档预处理的一个部分</p>
<p>最简单的方法就是整个提示模板每个 chunk 问一次 LLM，有 langchain 的 map reduce 等稍微 high level 一点的工具可以支持这个事情</p>
<p>对长文档总结更有效一些的做法是利用好 embedding，先对 chunk embedding 做聚类，再每个聚类里面抽几个 chunk, 从而保证多样性和 chunk 数量的平衡</p>
<p>后总结，或者说 query-based 总结大体上是用 LLM 做比较多，但对于时延和开销的增加太高了，一个比较新的方法是 paragraph sentence-level mask bert（自己造的词），在段落中根据 q, d 的交叉编码得到句子级别的二进制掩码，从而删除无关部分。有一篇 ICLR2025 基于 bge 训了个，<a href="https://huggingface.co/blog/nadiinchi/provence" target="_blank" rel="noopener noreferrer">https://huggingface.co/blog/nadiinchi/provence</a></p>
<p>provence效果非常好，又快又几乎对齐例如GPT4.1这种顶级模型的效果</p>
<p>另一个思路就是绕过这个问题，切小块，依赖 rerank 和重新合并乃至知识图谱检索之类的策略保证相关性，也就是在查询完之后是合并还是切分的思路差距</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="半结构化数据">半结构化数据<a class="hash-link" aria-label="Direct link to 半结构化数据" title="Direct link to 半结构化数据" href="/blog/page/2#半结构化数据">​</a></h4>
<p><a href="https://docs.superlinked.com/getting-started/installation" target="_blank" rel="noopener noreferrer">https://docs.superlinked.com/getting-started/installation</a> 聚焦半结构化的异构数据，例如朴素 embedding 方案对数字的理解不足，无法建模 1-99 的相似度分数与 higher/lower 这种文本的关系</p>
<p><a href="https://github.com/microsoft/multifield-adaptive-retrieval" target="_blank" rel="noopener noreferrer">https://github.com/microsoft/multifield-adaptive-retrieval</a> 做多字段的权重学习(自适应选择查询应该着重的权重)</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="embedding-相关的调优">embedding 相关的调优<a class="hash-link" aria-label="Direct link to embedding 相关的调优" title="Direct link to embedding 相关的调优" href="/blog/page/2#embedding-相关的调优">​</a></h4>
<p>colbert架构是一个better embedding的方向，其核心在于将文档的token level embedding保存下来，对于每一个query token，计算maxsim算子得到单token的score，再求和</p>
<p><img decoding="async" loading="lazy" src="https://github.com/stanford-futuredata/ColBERT/raw/main/docs/images/ColBERT-Framework-MaxSim-W370px.png" alt="img" class="img_ev3q"></p>
<p>对比朴素embedding方案，它在token level进行计算可以很好的带来类似关键词匹配的效果，有效避免长文档下，embedding过于平均化余弦相似太不敏感的问题</p>
<p>对比rerank方案，它的优点又在嵌入矩阵可以离线计算，不需要完全在线的交叉编码器</p>
<p>引入方案： <a href="https://python.langchain.com/docs/integrations/providers/ragatouille/" target="_blank" rel="noopener noreferrer">https://python.langchain.com/docs/integrations/providers/ragatouille/</a></p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="prompt">Prompt<a class="hash-link" aria-label="Direct link to Prompt" title="Direct link to Prompt" href="/blog/page/2#prompt">​</a></h4>
<p>基本没有什么特别通用的工作，但值得一提的是将prompt作为一个优化变量，使用LLM在Trajatory上进行采样和跑各种论文的“prompt优化算法”的解耦框架dsPy <a href="https://dspy.ai/" target="_blank" rel="noopener noreferrer">https://dspy.ai/</a> 用户以类似类型/对象系统的简短注释提供给dspy作为“初始意图”，而后续复杂的提 示由dspy生成，核心思想是让用户专注于编程</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">class</span><span class="token plain"> </span><span class="token class-name">CheckCitationFaithfulness</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">dspy</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">Signature</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token triple-quoted-string string" style="color:#e3116c">&quot;&quot;&quot;Verify that the text is based on the provided context.&quot;&quot;&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    context</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token builtin">str</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> dspy</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">InputField</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">desc</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">&quot;facts here are assumed to be true&quot;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    text</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token builtin">str</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> dspy</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">InputField</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    faithfulness</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token builtin">bool</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> dspy</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">OutputField</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    evidence</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token builtin">dict</span><span class="token punctuation" style="color:#393A34">[</span><span class="token builtin">str</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token builtin">list</span><span class="token punctuation" style="color:#393A34">[</span><span class="token builtin">str</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> dspy</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">OutputField</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">desc</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">&quot;Supporting evidence for claims&quot;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">context </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;The 21-year-old made seven appearances for the Hammers and netted his only goal for them in a Europa League qualification round match against Andorran side FC Lustrains last season. Lee had two loan spells in League One last term, with Blackpool and then Colchester United. He scored twice for the U&#x27;s but was unable to save them from relegation. The length of Lee&#x27;s contract with the promoted Tykes has not been revealed. Find all the latest football transfers on our dedicated page.&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">text </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;Lee scored 3 goals for Colchester United.&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">faithfulness </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> dspy</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">ChainOfThought</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">CheckCitationFaithfulness</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">faithfulness</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">context</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">context</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> text</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">text</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<blockquote>
<p>DSPy 中的不同优化器将通过为每个模块<strong>合成良好的小样本示例</strong> （如 <code>dspy.BootstrapRS</code> <a href="https://arxiv.org/abs/2310.03714" target="_blank" rel="noopener noreferrer">1 ）</a> 来调  整程序的质量；为每个提示<strong>提出并智能地探索更好的自然语言指令</strong> （如 <code>dspy.MIPROv2</code> <a href="https://arxiv.org/abs/2406.11695" target="_blank" rel="noopener noreferrer">2 ）</a> ，以及<strong>为您的模块构建数据集并使用它们来微调系统中的 LM 权重</strong> （如 <code>dspy.BootstrapFinetune</code> <a href="https://arxiv.org/abs/2407.10930" target="_blank" rel="noopener noreferrer">3 ）</a></p>
</blockquote>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="llm评估">LLM评估<a class="hash-link" aria-label="Direct link to LLM评估" title="Direct link to LLM评估" href="/blog/page/2#llm评估">​</a></h4>
<p>测试不可靠：有多少答案是被记忆出来的？</p>
<p>有多篇相关的paper在讨论这个问题，然后采用了一些方法来衡量这个事情，例如，在数学问题题集中，替换无关的描述、修改数字等等，看看模型性能变差多少</p>
<p>类似数学问题集这种在网络上数据中很难过滤干净，还需要考虑多语言影响</p>
<p>另一些评估指标如ARC-AGI通过抽象图像智力问题集来评估模型的推理能力，相对来说泄题风险小一些(并且有隐藏test set)</p>
<ul>
<li>丢给LLM的时候不是图像，而是矩阵，用数字表示不同颜色</li>
</ul>
<p><img decoding="async" loading="lazy" alt="image-20250505134411046" src="/assets/images/image-20250505134411046-5ad9c901ec94d830bb33f096ec492f2a.png" width="1224" height="849" class="img_ev3q"></p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="chatbot-arena-让全世界的人都来进行判断哪个模型好">Chatbot Arena: 让全世界的人都来进行判断哪个模型好<a class="hash-link" aria-label="Direct link to Chatbot Arena: 让全世界的人都来进行判断哪个模型好" title="Direct link to Chatbot Arena: 让全世界的人都来进行判断哪个模型好" href="/blog/page/2#chatbot-arena-让全世界的人都来进行判断哪个模型好">​</a></h4>
<p>但还是有办法hack: 更fit人的倾向（粗体字、分点、emoji.....）</p>
<p>Elo Score 考虑除了人的直接倾向之外其他因素的影响，在BF模型计算时加上一项<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>β</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">\beta_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0528em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>,  <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy="false">(</mo><msub><mi>β</mi><mi>i</mi></msub><mo>−</mo><msub><mi>β</mi><mi>j</mi></msub><mo>+</mo><msub><mi>β</mi><mn>0</mn></msub><mo stretchy="false">)</mo></mrow></mfrac><mo>=</mo><msub><mi>E</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\frac{1}{1 + exp(\beta_i - \beta_j + \beta_0)} = E_{ij}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.3874em;vertical-align:-0.5423em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8451em"><span style="top:-2.655em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">x</span><span class="mord mathnormal mtight">p</span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05278em">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3281em"><span style="top:-2.357em;margin-left:-0.0528em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em"><span></span></span></span></span></span></span><span class="mbin mtight">−</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05278em">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3281em"><span style="top:-2.357em;margin-left:-0.0528em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2819em"><span></span></span></span></span></span></span><span class="mbin mtight">+</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05278em">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em"><span style="top:-2.357em;margin-left:-0.0528em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em"><span></span></span></span></span></span></span><span class="mclose mtight">)</span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.5423em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">ij</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span></span></span></span>, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>E</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">E_{ij}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">ij</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span></span></span></span> 是模型i和j的胜率，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>β</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\beta_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0528em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> 是模型i的真实评分，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>β</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">\beta_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0528em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> 是一个全局偏差项，表示人类评估者的偏好。通过最大化似然函数来估计参数<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>β</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\beta_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0528em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>β</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">\beta_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0528em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>，从而得到模型的真实评分。</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>β</mi><mn>0</mn></msub><mo>=</mo><msub><mi>γ</mi><mn>1</mn></msub><mo>∗</mo><mtext>长度差</mtext><mo>+</mo><msub><mi>γ</mi><mn>2</mn></msub><mo>∗</mo><mi>e</mi><mi>m</mi><mi>o</mi><mi>j</mi><mi>i</mi><mtext>个数差</mtext><mo>+</mo><msub><mi>γ</mi><mn>3</mn></msub><mo>∗</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\beta_0 = \gamma_1 * 长度差 + \gamma_2 * emoji个数差 + \gamma_3 * ...</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0528em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6597em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05556em">γ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0556em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em"></span><span class="mord cjk_fallback">长度差</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6597em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05556em">γ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0556em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em"></span><span class="mord mathnormal">e</span><span class="mord mathnormal">m</span><span class="mord mathnormal">o</span><span class="mord mathnormal">ji</span><span class="mord cjk_fallback">个数差</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6597em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05556em">γ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0556em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.1056em"></span><span class="mord">...</span></span></span></span></p>
<p><img decoding="async" loading="lazy" alt="image-20250505135351256" src="/assets/images/image-20250505135351256-3bf80497f080b17544c064176591200e.png" width="975" height="825" class="img_ev3q"></p>
<p>可以看到，考不考虑这个<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>β</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">\beta_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0528em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>，模型的排名差别很大</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="goodharts-law">Goodhart&#x27;s Law<a class="hash-link" aria-label="Direct link to Goodhart&#x27;s Law" title="Direct link to Goodhart&#x27;s Law" href="/blog/page/2#goodharts-law">​</a></h4>
<p><strong>一旦一项指标被用作目标，它就不再是一个好的指标</strong></p>
<p><a href="http://becomingahacker.org/integrating-agentic-rag-with-mcp-servers-technical-implementation-guide-1aba8fd4e442" target="_blank" rel="noopener noreferrer">http://becomingahacker.org/integrating-agentic-rag-with-mcp-servers-technical-implementation-guide-1aba8fd4e442</a></p>
<p>However, traditional RAG has limitations: it usually queries a single data source and only performs one retrieval pass, so if the initial results are poor or the query is phrased oddly, the answer will suffer
但是，传统的 RAG 存在局限性：它通常查询单个数据源，并且只执行一次检索传递，因此如果初始结果不佳或查询措辞奇怪，答案将受到影响</p>
<p>There’s no built-in mechanism for the system to reason about <em>how</em> to retrieve better information or to use additional tools if needed.
系统没有内置机制来推  理<em>如何</em>检索更好的信息或在需要时使用其他工具。</p>
<p>关于结构化输出的另一篇特别好的文章: <a href="https://www.boundaryml.com/blog/schema-aligned-parsing" target="_blank" rel="noopener noreferrer">https://www.boundaryml.com/blog/schema-aligned-parsing</a></p>
<p>推理加速：是对的，例如huggingface-text-embedding项目，将各种转trt/onnx 可以让吞吐提升5x</p>
<p>H100 bge-reranker-v2-m3  1024 * 512char sentence， 13s -&gt; 2.3s</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="关键词抽取">关键词抽取<a class="hash-link" aria-label="Direct link to 关键词抽取" title="Direct link to 关键词抽取" href="/blog/page/2#关键词抽取">​</a></h4>
<p>基于主题LDA，词典等</p>
<p>小模型方法：先用spaCy、hanLP等得到语法树，再从语法树中拿到名词性关键词等</p>
<p>无监督，经典如YAKE！综合考虑词频，词位，共现等。可以考虑<a href="https://github.com/JackHCC/Chinese-Keyphrase-Extraction" target="_blank" rel="noopener noreferrer">https://github.com/JackHCC/Chinese-Keyphrase-Extraction</a></p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="一篇非常有insight的blog上下文相关上下文充足定量充足性和它的应用">一篇非常有insight的blog：上下文相关!=上下文充足，定量充足性和它的应用<a class="hash-link" aria-label="Direct link to 一篇非常有insight的blog：上下文相关!=上下文充足，定量充足性和它的应用" title="Direct link to 一篇非常有insight的blog：上下文相关!=上下文充足，定量充足性和它的应用" href="/blog/page/2#一篇非常有insight的blog上下文相关上下文充足定量充足性和它的应用">​</a></h4>
<p><a href="https://research.google/blog/deeper-insights-into-retrieval-augmented-generation-the-role-of-sufficient-context/" target="_blank" rel="noopener noreferrer">https://research.google/blog/deeper-insights-into-retrieval-augmented-generation-the-role-of-sufficient-context/</a></p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/rag">rag</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/ai">ai</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/llm">llm</a></li></ul></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/blog/精读  Interleaved Scene Graph for Interleaved Text-and-Image Generation Assessment">Paper reading - Interleaved Scene Graph for Interleaved Text-and-Image Generation Assessment</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-05-30T00:00:00.000Z">May 30, 2025</time> · <!-- -->5 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><div class="avatar__intro"><div class="avatar__name"><span>ayanami</span></div></div></div></div></div></header><div class="markdown"><p>开发了一个<strong>交错文本和图像生成综合评估框架</strong>ISG</p>
<p>使用<code>scene graph</code>捕获文本和图像的关系，提供四个级别的评估：整体的、结构性的、块级别和特定于图像的，并引入了一个新benchmark，ISG-BENCH</p>
<p>作者实验认为现有模型在端到端生成文本图像交错内容时，效果不好，于是做了一个Agent来完成这个任务</p>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="motivation">motivation<a class="hash-link" aria-label="Direct link to motivation" title="Direct link to motivation" href="/blog/page/2#motivation">​</a></h3>
<p><img decoding="async" loading="lazy" alt="image-20250530152606364" src="/assets/images/image-20250530152606364-d9605f8fed7f263b463ed75398a8cf8a.png" width="990" height="433" class="img_ev3q"></p>
<p>如图，现有MLLM<strong>不能直接生成交错文本和图像内容</strong>，需要将生成图像部分交给SD等外部模型再组合，带来了更大的开销与不一致性</p>
<hr>
<p>为了专注这一任务，作者的Benchmark优先考虑视觉为中心的任务，例如风格迁移等图像输出的特定要求。</p>
<ul>
<li>作者的数据集和人工标注比较有<strong>较高Pearson相似度，以此说明准确性</strong></li>
<li>作者表示先前没什么<strong>benchmark主要以视觉为中心，以此说明新颖度</strong></li>
<li>但有一说一，作者的表还是有点不公平的，例如它自己的<strong>sample很少</strong>(一千多)，同时评估级别是自己提出的这个四级别评估</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="作者的表">作者的表<a class="hash-link" aria-label="Direct link to 作者的表" title="Direct link to 作者的表" href="/blog/page/2#作者的表">​</a></h2>
<p><img decoding="async" loading="lazy" alt="image-20250530160048840" src="/assets/images/image-20250530160048840-646ee93968c80f9867ea0f316414a5a5.png" width="1052" height="362" class="img_ev3q"></p>
<hr>
<h1>方法</h1>
<p><img decoding="async" loading="lazy" alt="image-20250530153213139 h:500" src="/assets/images/image-20250530153213139-4ceb43c19d4bcc07cdc1baeb75d2408b.png" width="1151" height="576" class="img_ev3q"></p>
<p>注意点: <strong>中间看起来很复杂, 实际上是很多组prompt完成的</strong></p>
<hr>
<p>评估框架将query拆成scene-graph-like structure，<strong>其中图文作为节点，而它们的关系作为边</strong></p>
<p>在整体，结构，块和图四级别的评估中，每个级别都会生成一些用于评估的QA对。作者的意图是，<strong>让整体和结构评估连贯性和整体质量，块和图像评估指令完成的细节</strong></p>
<hr>
<p>结构性：用一个LLM预估图文交替内容的结构，然后与实际生成的内容进行比较</p>
<p><img decoding="async" loading="lazy" alt="image-20250530163448151" src="/assets/images/image-20250530163448151-71150c0f60bd09aee45e9107edd3eaa7.png" width="1060" height="523" class="img_ev3q"></p>
<hr>
<p>整体：MLLM-as-a-Judge和CoT，用1-10打分配合Yes/No判断</p>
<p>块： 将prompt P用LLM表示成三元组 （subj, obj, rel）,再用LLM生成问题，并用VQA评估</p>
<p><img decoding="async" loading="lazy" alt="image-20250530163519317" src="/assets/images/image-20250530163519317-1ba26e8e2cd40c56119403af2ba7b9b9.png" width="1047" height="770" class="img_ev3q"></p>
<hr>
<p>图像：从prompt 给的图像中用LLM抽出三元组关系和实体，判断query类别，根据类别不同使用不同的prompt产生判断的VQA，例如如果是&quot;How to&quot;，则需要包含特定实体，如果是“Painting”，则需要图像的准确生成</p>
<p><img decoding="async" loading="lazy" alt="image-20250530163331400 h:600" src="/assets/images/image-20250530163331400-1b1c3b6298aa1e9aad449957c2e03fd3.png" width="1039" height="679" class="img_ev3q"></p>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="实验结果">实验结果<a class="hash-link" aria-label="Direct link to 实验结果" title="Direct link to 实验结果" href="/blog/page/2#实验结果">​</a></h3>
<blockquote>
<p>所有统一模型在按照说明生成交错文本和图像内容方面都存在重大缺陷。许多模型只生成 1 到 3 张图像，而有些模型根本无法生成任何图像。</p>
</blockquote>
<blockquote>
<p>整体评估结果与三个细粒度级别的评估结果之间的不一致表明，即使同时提供用户指示和正确的黄金答案，MLLM-as-a-Judge 在全面评估回答方面也存在显着局限性。具体来说，Judge MLLM 努力根据细粒度的标准评估响应，例如输出结构（包括图像数量）和提示中规定的详细文本-图像关系。此外，我们对结果的分析揭示了 <strong>MLLM-as-a-Judge 中固有的偏见</strong>，即“图像质量偏见”，即<strong>具有更高质量图像内容的回答始终获得更高的分数，尽管这些回答可能违反用户的指导 要求和评判指南</strong>。这种偏见表明，即使获得了黄金答案，MLLM-as-a-Judge 仍然无法正确地对符合指定要求的交错回答进行准确评估。</p>
</blockquote>
<hr>
<p><img decoding="async" loading="lazy" alt="image-20250530160948640" src="/assets/images/image-20250530160948640-8a5505eaeadfe2ed274a09937abf96eb.png" width="1200" height="519" class="img_ev3q"></p>
<hr>
<h1>效果展示: 跑一次它这个Benchmark要60美刀</h1>
<p><img decoding="async" loading="lazy" alt="image-20250530163815015 h:600" src="/assets/images/image-20250530163815015-79e642937f498c3c9f5bde75cd8d23e3.png" width="1051" height="1016" class="img_ev3q"></p>
<hr>
<h1>结论</h1>
<ol>
<li>MLLM-as-a-judge存在图像质量bias</li>
<li>现有端到端MLLM生成图文内容效果不佳, 可能需要在工程性上的agent做补救</li>
</ol></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/mllm">mllm</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/ai">ai</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/llm">llm</a></li></ul></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/blog/ColBERT">ColBERT-后期交互方法</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-05-29T00:00:00.000Z">May 29, 2025</time> · <!-- -->10 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><div class="avatar__intro"><div class="avatar__name"><span>ayanami</span></div></div></div></div></div></header><div class="markdown"><p>如果简单引入语义搜索，那么第一时间想到的肯定是向量搜索的方法</p>
<p>先不论小的优化，向量方法现在大体上就是两种架构，单塔和双塔，对应Cross-Encoder和普通的Encoder模型  。</p>
<p>双塔模型如下，查询<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">q</span></span></span></span>和文档<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">d</span></span></span></span>分别通过两个独立的编码器，得到向量表示<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>q</mi><mi>v</mi></msub></mrow><annotation encoding="application/x-tex">q_v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mi>v</mi></msub></mrow><annotation encoding="application/x-tex">d_v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>，然后计算相似度（内积，余弦，等等）。</p>
<p><img decoding="async" loading="lazy" src="https://blog.vespa.ai/assets/2024-02-14-announcing-colbert-embedder-in-vespa/image5.png" alt="overview traditional text embedding models" class="img_ev3q"></p>
<p>而单塔模型则是将查询和文档拼接在一起，输入到一个交叉编码器中，这个交叉编码器很多时候就直接输出相关性得分score了，即为我们所说的reranker</p>
<p>单塔虽然精度远高于双塔，但有无法离线计算的缺点</p>
<p>而双塔的一大精度困境在于，当编码的文档变长时，文档的大部分内容可能都和查询没什么关系，这会导致查询向量和文档向量的相似度计算不准确。实际上，在楼主之前的一些实验之中，一整个很大的文档集合内，和某个查询最无关和最相关的文档的余弦相似度相差也就0.2左右，这就是长文档带来的问题。</p>
<p>但客观地讲，长文档是无法避免的，如果把文档切成更细粒度的句子，在上下文补齐语义，后续合并等麻烦可能更多，并且会出现&quot;长文档实际上是在让相似度检索考虑上下文&quot;这样的情况，一个例子是，问题是&quot;上海交大的用户论坛中，....&quot;，而文档可能是&quot;...水源社区是上海交大的用户论坛。水源社区.....&quot; 如果仅在句子等短文本上面匹配，那缺少了上下文的情况下，&quot;水源社区&quot;当然和&quot;上海交大&quot;没什么关系。</p>
<p>那么，如何保证精度的同时又能离线计算呢？</p>
<p>ColBERT的思路是，使用双塔模型来计算相似度，但在编码文档时，<strong>使用了一个更细粒度的向量表示</strong>。</p>
<p>ColBERT<strong>给每个token一个向量表示</strong>，而不是给每个文档一个向量表示。这样，查询和文档的相似度计算就可以在token级别进行。</p>
<p>如下图，ColBERT在拿到最后一层的输出之后（这一层有非常多的语义信息！），将每一个token对应的vector都存下来，这一部分是离线的。</p>
<p>而在计算相似度的时候，将query的tensor和文档的tensor进行一个<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mi>a</mi><mi>x</mi><mi>S</mi><mi>i</mi><mi>m</mi></mrow><annotation encoding="application/x-tex">MaxSim</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">M</span><span class="mord mathnormal">a</span><span class="mord mathnormal">x</span><span class="mord mathnormal" style="margin-right:0.05764em">S</span><span class="mord mathnormal">im</span></span></span></span>算子</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mi>a</mi><mi>x</mi><mi>S</mi><mi>i</mi><mi>m</mi></mrow><annotation encoding="application/x-tex">MaxSim</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">M</span><span class="mord mathnormal">a</span><span class="mord mathnormal">x</span><span class="mord mathnormal" style="margin-right:0.05764em">S</span><span class="mord mathnormal">im</span></span></span></span>是一个最大池化操作，取出<strong>每个token的向量中与查询向量最相似的那个向量</strong>，然后计算相似度。</p>
<p><img decoding="async" loading="lazy" src="https://blog.vespa.ai/assets/2024-02-14-announcing-colbert-embedder-in-vespa/image1.png" alt="overview colbert" class="img_ev3q"></p>
<p>ColBERT的性能是逼近reranker的，这个也很好理解，毕竟交叉编码器的优势就是可以考虑<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi><mo separator="true">,</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">q,d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">d</span></span></span></span>之间的交互，而ColBERT除了保留语义嵌入之外，比起更暴力的加大embedding维度，更重要的是它<strong>保存了上下文次序的信息</strong></p>
<p>而ColBERT的最后一层MaxSim，而没有采用神经网络的方案，让他带来了良好的可解释性</p>
<p><img decoding="async" loading="lazy" src="https://blog.vespa.ai/assets/2024-02-14-announcing-colbert-embedder-in-vespa/image3.png" alt="colbert snippet" class="img_ev3q"></p>
<p>那看了上面立刻就会想到，这每一个token保存一个<code>768/1024/...</code>维的向量，存储开销不会很大吗？</p>
<p>ColBERT也考虑到了这个问题，因此在ColBERTv2中，采用了这样质心编码的方法来降低存储开销，能降低8倍</p>
<ol>
<li>
<p>对每个token的向量进行聚类，得到<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span></span></span></span>个质心（k是一个预定义的数字）</p>
</li>
<li>
<p>对每个token的向量，找到距离最近的质心，并将其索引存储下来，也就是从 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>v</mi><mi>d</mi></msub><mo separator="true">,</mo><mo stretchy="false">)</mo><mo>−</mo><mo>&gt;</mo><mo stretchy="false">(</mo><mn>1</mn><mo separator="true">,</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(v_d, ) -&gt;(1,)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mclose">)</span><span class="mord">−</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mclose">)</span></span></span></span></p>
</li>
<li>
<p>将质心向量库构建ANN索引，例如FAISS, ScaNN</p>
</li>
<li>
<p>在计算相似度时，查询向量也进行同样的处理，找到距离查询最近的质心索引，然后从质心向量库中取出对应的质心向量进行相似度计算</p>
</li>
</ol>
<p>在实际使用的时候，商业rag公司甚至对大规模检索做更狠的二值化向量压缩（说实话这也能检索出来真的有点现代模型神力了），让ColBERT的开销可以和单独的embedding媲美</p>
<p><img decoding="async" loading="lazy" src="https://blog.vespa.ai/assets/2024-02-14-announcing-colbert-embedder-in-vespa/image2.png" alt="colbert token" class="img_ev3q"></p>
<p>二值化的说法是这样的:</p>
<blockquote>
<p>压缩方法通过将正维度表示为 1、负维度表示为 0 来简化文档标记向量。<strong>这种二进制表示有效地指示了文档标记向量中重要语义特征的存在与否</strong>。
<strong>正维度有助于增加点积，表明相关的语义相似性，而负维度则被忽略。</strong></p>
</blockquote>
<p>ColBERT的使用上，很多公司都有了支持，例如vespa, jina等等，开源方案则有早期的ragatouile和后来的上下游如milvus，llamaindex的支持</p>
<p>但是，文档ColBERT还不是它发挥全部潜能的时候，据说SPLADE算法就比他效果好不少（这个我没有实测过），它在图像又活出了第二世，即所谓的ColPali架构</p>
<p>ColPali是MRAG、MLLM那边的新论文和解决方案，几个月的时间砍了1.9k star，ColPali的想法是这样的</p>
<ul>
<li>OCR的多个组件和分块带来误差传播，且预处理流程耗时也长，能不能直接端到端一次使用文档截图解决</li>
<li>但是如果将整页的文档编码成一个向量，肯定精度不够</li>
<li>我的ViT等视觉编码器会将整页文档变成一系列的patch（可以理解为子图），进而变成一系列视觉token，那我重用ColBERT，不就又有了多向量吗？并且这个存储和交互上比每个token存一个向量更合理! 子图本身就有很多的空间位置信息</li>
</ul>
<p><img decoding="async" loading="lazy" alt="image-20250529232012895" src="/assets/images/image-20250529232012895-e3798d5e7863883d9a90645e57d47f43.png" width="1506" height="916" class="img_ev3q"></p>
<p>并且，你会发现ColBERT的强可解释性在图像上有更关键的作用！模型在文本中关注了什么可能是某个词，还需要人进行一点逻辑推理来判断关系是否合理，而图像中关注了什么，直接看图就知道了！</p>
<p><img decoding="async" loading="lazy" alt="image-20250529232211333" src="/assets/images/image-20250529232211333-f16577089d8023fe748d53489301703c.png" width="1402" height="1150" class="img_ev3q"></p>
<p>作为一种新的RAG范式，ColPali从源头上解决了复杂的OCR和切块的问题</p>
<p>虽然其在重文字领域上的泛化性还留待验证，精度的提升也依赖于未来VLM的发展，但无疑社区已经认同了这个想法的价值</p>
<blockquote>
<p>基于 OCR 的文本提取，以及随后的布局和边界框分析，仍然是重要文档 AI 模型（例如 LayoutLM）的核心。例如， <a href="https://huggingface.co/microsoft/layoutlmv3-base" target="_blank" rel="noopener noreferrer">LayoutLMv3</a> 对文档文本进行编码，包括文本标记序列的顺序、标记或线段的 OCR 边界框坐标以及文档本身。这在关键的文档 AI 任务中取得了最佳成果，但前提是第一步——OCR 文本提取——能够顺利完成。</p>
<p><strong>但通常情况并  非如此。</strong></p>
<p>根据我最近的经验，<strong>OCR 瓶颈导致现实世界生产文档档案中的命名实体识别 (NER) 任务的性能下降近 50%。</strong></p>
</blockquote>
<p>目前例如ColQwen2这种ColBERT + Qwen2.5-VL-3B-Instruct的方案也很火，很多榜上都刷到了SOTA，感兴趣的同学也可以自己试试</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/col-bert">ColBERT</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/embedding">embedding</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/rag">rag</a></li></ul></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/blog/2025/05/26/技术博客阅读">美团技术博客阅读</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-05-26T00:00:00.000Z">May 26, 2025</time> · <!-- -->19 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><div class="avatar__intro"><div class="avatar__name"><span>ayanami</span></div></div></div></div></div></header><div class="markdown"><h3 class="anchor anchorWithStickyNavbar_LWe7" id="美团外卖基于gpu的向量检索系统实践">美团外卖基于GPU的向量检索系统实践<a class="hash-link" aria-label="Direct link to 美团外卖基于GPU的向量检索系统实践" title="Direct link to 美团外卖基于GPU的向量检索系统实践" href="/blog/page/2#美团外卖基于gpu的向量检索系统实践">​</a></h3>
<blockquote>
<p>美团外卖的向量检索系统使用了GPU来加速向量检索过程。该系统主要包括以下几个方面： 美团外卖业务特点具有较强的Location Based Service（LBS）依赖，即商家的配送范围，决定了用户所能点餐的商家列表。以商品向量  检索场景为例：向量检索结果集需要经过“可配送商家列表”过滤。</p>
</blockquote>
<blockquote>
<p>美团外卖向量检索基于Elasticsearch+FAISS进行搭建，实现了10亿级别+高维向量集的标量+向量混合检索的能力。为了在保证业务高召回率的同时进一步减少检索时间，我们探索基于GPU的向量检索，并实现了一套通用的检索系统。</p>
</blockquote>
<p><strong>相继使用了HNSW（Hierarchical Navigable Small World），IVF（Inverted File），IVF-PQ（Inverted File with Product Quantization）以及IVF-PQ+Refine等算法，基于CPU实现了向量检索能力</strong></p>
<p>在HNSW算法中，这种导航小世界图的层次结构使得搜索过程可以从图的高层开始，快速定位到目标点的大致位置，然后逐层向下精细化搜索，最终在底层找到最近邻，在通用检索场景上有显著的优势。然而<strong>该算法在高过滤比下性能会有折损</strong>，从而导致在到家搜推这种强LBS过滤场景下会暴露其性能的劣势。业界有较多相关的benchmark可以参考，以Yahoo的向量检索系统Vespa相关博客为例</p>
<p><img decoding="async" loading="lazy" alt="图片" src="/assets/images/640-6ecdab7cf574caf7866ef8076970b00a.webp" width="1080" height="678" class="img_ev3q"></p>
<blockquote>
<p>索引吞吐</p>
<p>Observations: 观察结果：</p>
<ul>
<li>Indexing throughput depends on corpus size for Annoy and HNSW, where throughput is halved when corpus size is increased by 10x.
对于 Annoy 和 HNSW，<strong>索引吞吐量</strong>取决于语料库大小，当语料库大小增加 10 倍时，吞吐量就会减半。</li>
<li>Indexing throughput for RPLSH is independent of corpus size.
RPLSH 的索引吞吐量与语料库大小无关。</li>
<li>Annoy is <strong>4.5 to 5 times</strong> faster than HNSW.
Annoy 比 HNSW 快 <strong>4.5 到 5 倍</strong> 。</li>
<li>RPLSH is <strong>23 to 24 times faster</strong> than HNSW at 1M documents.
对于 1M 文档，RPLSH 的<strong>速度比 HNSW 快 23 到 24 倍</strong> 。</li>
</ul>
</blockquote>
<p><img decoding="async" loading="lazy" alt="img" src="/assets/images/indexing-throughput-sift-0d6b3d3501d6d01b8531d2fbf5f73052.png" width="1110" height="686" class="img_ev3q"></p>
<blockquote>
<p>查询吞吐</p>
<p>Observations: 观察结果：</p>
<ul>
<li>HNSW outperforms Annoy and RPLSH. At corpus size 1M the QPS is <strong>9 times as high</strong> as Annoy, and <strong>16 times as high</strong> as RPLSH at comparable quality. Similar observations between hnswlib and Annoy are found in <a href="https://ann-benchmarks.com/" target="_blank" rel="noopener noreferrer">ANN Benchmarks</a>, where the QPS of hnswlib is 5-10 times higher at the same quality on all tested datasets.
HNSW 的表现优于 Annoy 和 RPLSH。在 1M 语料库规模下，其每秒查询速度 (QPS) 是 Annoy 的 <strong>9 倍</strong> ，在同等质量下是 RPLSH 的 <strong>16 倍</strong> 。在 <a href="https://ann-benchmarks.com/" target="_blank" rel="noopener noreferrer">ANN 基准测试</a>中也发现了 hnswlib 与 Annoy 之间的类似现象：在所有测试数据集上，相同质量下 hnswlib 的每秒查询速度 (QPS) 比 Annoy 高 5-10 倍。</li>
<li><strong>HNSW 搜索算法很大程度上取决于节点之间的链接数量，而链接数量又取决于语料库的大小。当语料库规模增加 10 倍时，QPS 会减半</strong>。在索引过程中，我们也看到了同样的情况，因为它使用搜索算法来查找要连接的候选节点。</li>
</ul>
</blockquote>
<p><img decoding="async" loading="lazy" src="https://blog.vespa.ai/assets/2020-06-30-approximate-nearest-neighbor-search-in-vespa-part-1/search-throughput-sift.png" alt="img" class="img_ev3q"></p>
<blockquote>
<p>内存占用</p>
<p>Observations: 观察结果：</p>
<ul>
<li>The Annoy index is almost 3 times larger than the HNSW index, which results in ~40% more total memory usage in the 1M SIFT dataset.
<strong>Annoy 索引几乎比 HNSW 索引 大 3 倍</strong>，这导致 1M SIFT 数据集的总内存使用量增加约 40%。</li>
<li>Both indexes are independent of dimension size, but max points in a leaf node (Annoy) and max links per level (HNSW) might need adjustments with higher dimensionality to get decent quality.
这两个索引都与维度大小无关，但叶节点中的最大点数（Annoy）和每级的最大链接数（HNSW）可能需要使用更高的维度进行调整才能获得不错的质量。</li>
</ul>
</blockquote>
<p><a href="https://blog.vespa.ai/approximate-nearest-neighbor-search-in-vespa-part-1/" target="_blank" rel="noopener noreferrer">博客</a>给出了一个很重要的观察是：<strong>当超过 90% 到 95% 的文档被过滤掉时，过滤后计算精确最近邻比搜索 HNSW 索引（过滤器会丢弃候选匹配项）的成本更低</strong></p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="22-ivf-inverted-file">2.2 IVF （Inverted File）<a class="hash-link" aria-label="Direct link to 2.2 IVF （Inverted File）" title="Direct link to 2.2 IVF （Inverted File）" href="/blog/page/2#22-ivf-inverted-file">​</a></h4>
<p>IVF是一种基于倒排索引的方法，它将高维向量空间分为多个簇（Cluster），每个簇对应一个倒排列表，存储了属于该簇的向量索引。这种方法大大减少了搜索时需要比较的向量数量，从而<strong>提高了检索速度</strong>。它的缺点是需要存储原始的向量数据，<strong>同时为了保证检索性能需要将其全量加载到内存中，从而占用了大量的内存空间</strong>，容易造成内存资源瓶颈。</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="23-ivf-pqinverted-file-with-product-quantization">2.3 IVF-PQ（Inverted File with Product Quantization）<a class="hash-link" aria-label="Direct link to 2.3 IVF-PQ（Inverted File with Product Quantization）" title="Direct link to 2.3 IVF-PQ（Inverted File with Product Quantization）" href="/blog/page/2#23-ivf-pqinverted-file-with-product-quantization">​</a></h4>
<p>在候选集数量巨大的场景下，比如商品向量检索场景下，IVF带来的内存空间大的问题很快就显现出来，为了解决内存空间的问题，开始尝试使用了IVF-PQ方法。该方法在IVF的基础上，使用了<strong>乘积量化（Product Quantization，PQ）的方法来压缩向量数据。PQ将高维向量分为多个子向量，然后对每个子向量进行量化</strong>，从而大大减少了对内存空间的需求。</p>
<p>然而，由于量化过程会引入误差，因此<strong>IVF-PQ的检索精度会低于IVF，从而导致召回率无法满足线上要求，对召回率要求相对较低的场景可以使用IVF-PQ，对召回率有一定要求的场景需要其他解决方案。</strong></p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="24-ivf-pqrefine">2.4 IVF-PQ+Refine<a class="hash-link" aria-label="Direct link to 2.4 IVF-PQ+Refine" title="Direct link to 2.4 IVF-PQ+Refine" href="/blog/page/2#24-ivf-pqrefine">​</a></h4>
<p>为了提高IVF-PQ的检索精度，进一步采用了IVF-PQ+Refine的方案，在IVF-PQ的基础上，在SSD磁盘上保存了未经压缩的原始向量数据。<strong>检索时，通过IVF-PQ召回数量更大的候选向量集合，然后获取对应的原始向量数据进行精确计算，从而提高检索精度</strong>。这种方法既保留了IVF-PQ的存储优势，解决了内存资源瓶颈，又保证了召回率，因此在实际应用中得到了广泛的使用。</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="25-基于地理位置的向量检索">2.5 基于地理位置的向量检索<a class="hash-link" aria-label="Direct link to 2.5 基于地理位置的向量检索" title="Direct link to 2.5 基于地理位置的向量检索" href="/blog/page/2#25-基于地理位置的向量检索">​</a></h4>
<p><strong>通过将经纬度编码为向量，优化具体做法是将用户或商家的经纬度以加权的方式加入查询Query和候选向量中，在计算Query和候选向量的相 似度时，距离因素就可以在不同程度上影响最终的检索结果，从而达到让向量索引具备LBS属性的目标。</strong></p>
<p>这里没有细讲，但怎么具体怎么融入的LBS属性还是比较有意思的，最直接的方法是将经纬度信息直接拼接到现有的文本embedding向量上，也可以将经纬度用geohash，或者以用户为中心的极坐标系统表示?</p>
<p>我觉得最复杂的在于：</p>
<ul>
<li>如何确定经纬度特征的维度，这也算是一种权值</li>
<li>如何让经纬度特征和其他向量特征上对齐？美团是否有一个专用的embedding模型来嵌入地理信息特征，这个模型又是根据什么进行微调的？是类似推荐系统那种基于用户反馈的，还是内部有一个地理加权的人工设计公式，这个模型提供的地理特征使得整体效果向这个公式靠齐的？</li>
</ul>
<p><a href="https://docs.google.com/document/d/1R5nOiwFUn9ZJtuWywmos2yfB4aCWCGy1TUN5VAnMRaY/edit?usp=sharing" target="_blank" rel="noopener noreferrer">https://docs.google.com/document/d/1R5nOiwFUn9ZJtuWywmos2yfB4aCWCGy1TUN5VAnMRaY/edit?usp=sharing</a></p>
<blockquote>
<p>考虑到美团外卖的业务场景，目标方案应该满足以下要求：</p>
<ul>
<li><strong>支持向量+标量混合检索</strong>：在向量检索的基础上，支持复杂的标量过滤条件。</li>
<li><strong>高过滤比</strong>：标量作为过滤条件，有较高的过滤比（大于99%），过滤后候选集大（以外卖商品为例，符合LBS过滤的商品向量候选集仍然超过百万）。</li>
<li><strong>高召回率</strong>：召回率需要在95%+水平。</li>
<li><strong>高性能</strong>：在满足高召回率的前提下，检索耗时Tp99控制在20ms以内。</li>
<li><strong>数据量</strong>：需要支持上亿级别的候选集规模。</li>
</ul>
</blockquote>
<blockquote>
<p>实现向量+标量混合检索，一般有两种方式：前置过滤（pre-filter）和后置过滤（post-filter）  。<strong>前置过滤指先对全体数据进行标量过滤，得到候选结果集，然后在候选结果集中进行向量检索，得到TopK结果。后置过滤指先进行向量检索，得到TopK*N个检索结果，再对这些结果进行标量过滤，得到最终的TopK结果</strong>。其中N为扩召回倍数，主要是为了缓解向量检索结果被标量检索条件过滤，导致最终结果数不足K个的问题。</p>
<p>业界已有较多的成熟的全库检索的方案，后置过滤方案可以尽量复用现有框架，开发量小、风险低，<strong>因此我们优先考虑后置过滤方案</strong>。我们基于GPU的后置过滤方案快速实现了一版向量检索引擎，并验证其召回率与检索性能。GPU中成熟的检索算法有Flat、IVFFlat和IVFPQ等，在不做扩召回的情况下，召回率偏低，<strong>因此我们在benchmark上选择了较大的扩召回倍数以提高召回率。</strong></p>
</blockquote>
<p><img decoding="async" loading="lazy" src="https://mmbiz.qpic.cn/sz_mmbiz_png/hEx03cFgUsVShPhmBKFPvZmO6XEY3ficv2O6cewUo5kHicVklOU2wUzECA2q5d0lf8kBFtYI8Jj2HBbvNs0TuQ9w/640?wx_fmt=png&amp;from=appmsg&amp;tp=webp&amp;wxfrom=10005&amp;wx_lazy=1" alt="图片" class="img_ev3q"></p>
<blockquote>
<p><strong>测试结果表明，以上三种算法均无法同时满足我们对检索性能和召回率的需求。其中IVF与IVFPQ召回率较低，Flat算法虽然召回率较高，但是与全体候选集计算向量相似度导致其性能较差。</strong></p>
</blockquote>
<blockquote>
<p><strong>根据用户的地理位置信息计算其GeoHash值，并扩展至附近9个或25个GeoHash块，在这些GeoHash块内采用Flat算法进行向量检索，可以有效减少计算量。这种向量子空间划分方式有效地提高了检索性能，但是存在某些距离稍远的商家无法被召回的情况，最终测得的召回率只有80%左右，无法满足要求。</strong></p>
</blockquote>
<blockquote>
<p>综上，后置过滤方案无法同 时满足检索性能和召回率的需求，而GPU版本的Faiss无法实现前置过滤功能，考虑到美团外卖的业务场景，向量+标量混合检索能力是最基本的要求，因此我们决定自研GPU向量检索引擎。</p>
</blockquote>
<blockquote>
<p>基于GPU的向量检索，要想实现前置过滤，一般有三种实现方案：</p>
<ol>
<li>所有<strong>原始数据都保存在GPU显存中，由GPU完成前置过滤</strong>，再进行向量计算。</li>
<li>所有<strong>原始数据都保存在CPU内存中，在CPU内存中完成前置过滤，将过滤后的原始向量数据传给GPU进行向量计算</strong>。(能存更大的数据集)</li>
<li><strong>原始向量数据保存在GPU显存中，其他标量数据保存在CPU内存中，在CPU内存完成标量过滤后，将过滤结果的下标传给GPU，GPU根据下标从显存中获取向量数据进行计算。</strong>（省显存带宽）</li>
</ol>
<p>由于GPU与CPU结构与功能上的差异性，使用GPU完成前置过滤，显存资源占用量更大，过滤性能较差，且无法充分利用过滤比大的业务特点，<strong>因此不考虑方案1</strong>。</p>
<p><img decoding="async" loading="lazy" alt="图片" src="/assets/images/640-1748242298419-5-cc781b495cb0bc8b56c804a537b59a3d.webp" width="1080" height="278" class="img_ev3q"></p>
<p>实验结果表明，方案2在数据拷贝阶段耗时严重，时延无法达到要求。因为在美团外卖的场景下，过滤后的数据集仍然很大，这对CPU到GPU之间的数据传输带宽（A30显卡带宽数据如下 CPU-GPU：PCIe Gen4: 64GB/s；GPU-GPU：933GB/s）提出了很高的要求，<strong>因此我们最终选择了方案3。</strong></p>
</blockquote>
<blockquote>
<p>考虑到显存的价格远高于内存，因此我们在设计方案的过程中，尽可能将数据存储在内存当中，仅将需要GPU计算的数据存储在显存当中。</p>
<p>内存中保存了所有的标量数据，数据<strong>按列存储</strong>，通  过位置索引可以快速找到某条数据的所有字段信息，数据按列存储具备较高的灵活性和可扩展性，同时也更容易进行数据压缩和计算加速。针对需要用于过滤的标量字段，<strong>在内存中构造了倒排索引，倒排链中保存了对应的原始数据位置索引信息</strong>，内存数据结构如下图所示</p>
</blockquote>
<p><img decoding="async" loading="lazy" alt="图片" src="/assets/images/640-1748242561687-8-46646c8a46fe5a278b81322532de631c.webp" width="1080" height="1238" class="img_ev3q"></p>
<blockquote>
<p>显存中保存了所有的向量数据，数据位置索引与内存中的数据一一对应，可以通过位置索引快速获取某条数据的向量信息，如下图所示：</p>
<p><img decoding="async" loading="lazy" alt="图片" src="data:image/webp;base64,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" width="1050" height="186" class="img_ev3q"></p>
</blockquote>
<p>最后的流程图(Flat)</p>
<p><img decoding="async" loading="lazy" alt="图片" src="/assets/images/640-1748242653397-14-854b2dbbbffe8af6b1e6a08532e26649.webp" width="1080" height="816" class="img_ev3q"></p>
<p>最后的流程图(IVF)，放宽召回率，提升性能</p>
<p><img decoding="async" loading="lazy" alt="图片" src="/assets/images/640-1748242729349-17-2a9cdd350ddfde26a2e58ee500477319.webp" width="1080" height="856" class="img_ev3q"></p>
<p><img decoding="async" loading="lazy" alt="图片" src="/assets/images/640-1748242743071-20-64fd68119b0135d73db5463a38526ce9.webp" width="1080" height="375" class="img_ev3q"></p>
<p><strong>可见，无论是Flat还是IVF，在相同的召回率下，使用前置过滤的性能都要明显好于后置过滤。</strong></p>
<p>性能优化</p>
<ul>
<li>
<p>高并发支持，通过Cuda Stream，GPU可以并行处理多个查询请求，高并发压测下，GPU利用率可以达到100%。</p>
</li>
<li>
<p>通过GPU实现部分标量过滤功能，支持在GPU上实现部分标量过滤功能，向量计算与标量过滤同处一个Kernel，充分利用GPU并行计算能力</p>
</li>
<li>
<p><strong>资源管理优化，支持句柄机制，资源预先分配，重复利用</strong>。每个句柄持有一部分私有资源，包含保存向量检索中间计算结果的可读写内存、显存，以及单独的Cuda Stream执行流；<strong>共享一份全局只读公有资源。在初始化阶段，创建句柄对象池，可以通过控制句柄数量，来调整服务端并发能力，避免服务被打爆。在检索阶段，每次向量检索需从句柄对象池中申请一个空闲的句柄，然后进行后续的计算流程，并在执行完后释放响应的句柄，达到资源回收和重复利用的目的</strong></p>
</li>
</ul>
<p><img decoding="async" loading="lazy" alt="图片" src="/assets/images/640-1748243142566-23-725fd68c9ca6aff07b382e1e9408c08b.webp" width="1080" height="367" class="img_ev3q"></p>
<blockquote>
<p>我们最终选择了单机多卡的数据分片方案，单台服务器部署多张GPU，检索时并行从本地多张GPU中检索数据，在CPU内存中进行数据合并。</p>
<p>为了支持更大规模的向量数据检索，我们还在GPU检索引擎上支持了半精度计算，使用FP16替换原来的FP32进行计算，可以节省一半的GPU显存占用，经验证Flat召回率由100%下降到99.4%，依然满足需求。使用半精度之后，单机可以加载近10亿数据，足够支撑较长时间的业务数据增长。</p>
</blockquote>
<p>GPU 检索系统上线后实际性能数据如下（数据量1亿+）：</p>
<p><img decoding="async" loading="lazy" alt="图片" src="data:image/webp;base64,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" width="1080" height="185" class="img_ev3q"></p>
<hr>
<p>22年还有一篇早期的搜索基于elasticsearch的优化实践</p>
<p><a href="https://mp.weixin.qq.com/s?__biz=MjM5NjQ5MTI5OA==&amp;mid=2651772026&amp;idx=1&amp;sn=6ff4cb024bb416c46d5d2850a6ae77d1&amp;chksm=bd120d378a6584217f1838c0f951204023e5c32b0ad413a731078e2f11f8f0009b39c3dec4ea&amp;scene=21#wechat_redirect" target="_blank" rel="noopener noreferrer">https://mp.weixin.qq.com/s?__biz=MjM5NjQ5MTI5OA==&amp;mid=2651772026&amp;idx=1&amp;sn=6ff4cb024bb416c46d5d2850a6ae77d1&amp;chksm=bd120d378a6584217f1838c0f951204023e5c32b0ad413a731078e2f11f8f0009b39c3dec4ea&amp;scene=21#wechat_redirect</a></p>
<p>但这个就很工程很机架了</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/tech-blog">tech blog</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/美团">美团</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/system">system</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/ai">ai</a></li></ul></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/blog/Milvus">稀疏神经嵌入</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-05-25T00:00:00.000Z">May 25, 2025</time> · <!-- -->6 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><div class="avatar__intro"><div class="avatar__name"><span>ayanami</span></div></div></div></div></div></header><div class="markdown"><p>下午在看milvus文档的时候看到着重提了稀疏检索，注意到bge-m3是有神经稀疏检索的支持的，于是学习了一下，下面属于纯入门笔记。</p>
<p><a href="https://bge-model.com/bge/bge_m3.html" target="_blank" rel="noopener noreferrer">https://bge-model.com/bge/bge_m3.html</a></p>
<p><img decoding="async" loading="lazy" alt="image-20250525152523623" src="/assets/images/image-20250525152523623-379d98e15be92bb305e9460ea197f20f.png" width="1141" height="874" class="img_ev3q"></p>
<p>和传统的BM25等稀疏嵌入不同，bge-m3的稀疏嵌入是基于模型的，复用密集嵌入的前面层</p>
<blockquote>
<p>BGE-M3 实现的是一种**“learned sparse embedding”（神经稀疏语义嵌入**）。与 SPLADE、uniCOIL 这类模型类似，这些都是让模型自适应学习每个 token 某种“匹配权重”，在大规模预训练和下游 fine-tune 时引入了专门的稀疏激活目标，使输出稀疏且有用</p>
</blockquote>
<p><img decoding="async" loading="lazy" src="https://miro.medium.com/v2/resize:fit:1050/0*_IxiJuTn_LTcDlq2.png" alt="From tokens to BERT dense embeddings" class="img_ev3q"></p>
<p><img decoding="async" loading="lazy" src="https://miro.medium.com/v2/resize:fit:1050/0*lmMjfFrUs1-VikZ_.png" alt="From tokens to sparse embeddings.png" class="img_ev3q"></p>
<p>SPLADE 模型的全称为&quot;Sparse Lexical and Expansion Model&quot;（稀疏词法和扩展模型），结合了传统稀疏向量检索的优点和神经网络的语义理解能力。</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mi>j</mi></msub><mo>=</mo><mi>m</mi><mi>a</mi><msub><mi>x</mi><mrow><mi>i</mi><mo>∈</mo><mi>t</mi><mi>o</mi><mi>k</mi><mi>e</mi><mi>n</mi><mi>s</mi></mrow></msub><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><mn>1</mn><mo>+</mo><mi>R</mi><mi>e</mi><mi>L</mi><mi>U</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">w_j = max_{i\in tokens} log(1 + ReLU(w_{ij}))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">ma</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">∈</span><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.03148em">k</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1774em"><span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.01968em">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em">g</span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em"></span><span class="mord mathnormal" style="margin-right:0.00773em">R</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.10903em">LU</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">ij</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mclose">))</span></span></span></span></p>
<p><img decoding="async" loading="lazy" alt="image-20250525155231480" src="/assets/images/image-20250525155231480-44fe88ea18a1d74d6f97bc797f54a880.png" width="980" height="323" class="img_ev3q"></p>
<p>BERT 的 MLM 头部会为每个输入位置计算对词汇表中每个词元的贡献分数。这些分数反映了当前上下文下，特定词元与其他词元的关联强度。</p>
<p><img decoding="async" loading="lazy" src="https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F81ba987ca3713008eb0b9ecaf1a4680d03fcff5f-2185x743.png&amp;w=3840&amp;q=75" alt="Term expansion in the query can lead to much greater overlap between queries and relevant documents, helping us minimize the vocabulary mismatch problem." class="img_ev3q"></p>
<p>也就是说，这个方法实际处理了三个问题:</p>
<ol>
<li>词表不够大（或者分词精度不够）的问题，采用预训练BERT的词表和分词器，可以随着预训练模型的拓展而拓展；</li>
<li>针对传统稀疏编码需要精确词匹配、编码值实际上都是离散变化的问题，采用遍历整个词表，利用BERT的mask-预测概率，计算将原始句子的每一个词与词汇表中其他词的<strong>关联强度</strong>，对应logits即为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">w_{ij}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">ij</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span></span></span></span>，从而实现了词汇的拓展，允许相关词、近义词匹配等</li>
<li>针对传统BM25中没有上下文位置关系的问题，利用BERT的位置编码和捕获双向信息的预测，将传统手工设计特征的部分取代</li>
</ol>
<p>还有一些其他的操作比如稀疏化，用于提供较密集嵌入更强的筛选能力</p>
<p>SPLADE 使用 ReLU 激活和 MAX 池化操作来确保生成稀疏向量。ReLU 将负值置为零，增加稀疏性；MAX 池化则为每个词元选择所有位置中的最大贡 献值，进一步增强了稀疏性 <a href="https://www.pinecone.io/learn/splade/" target="_blank" rel="noopener noreferrer">1</a>。</p>
<p>此外，SPLADE 还使用正则化（如 FLOPS 正则化）来控制稀疏性程度：</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">L_FLOPS = λ * ||q_splade||_1 * ||d_splade||_1</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>这个正则化项通过惩罚向量的 L1 范数（非零元素的绝对值和）来鼓励模型生成更稀疏的向量</p>
<p>还有一个问题是：这个“关联强度&quot;是什么?</p>
<p>从模型的角度说，这个关联强度向量就是BERT的嵌入再过一个MLP得到<code>(vocab_size, )</code>的向量</p>
<p>但实际上，这里并没有直接使用BERT的mask预测权重，而是针对信息检索进行了微调（使用MS MARCO数据集，100万条搜索引擎搜索数据）</p>
<p><img decoding="async" loading="lazy" alt="image-20250525160740142" src="/assets/images/image-20250525160740142-b41c4821f26321a294df4b6abce5d0ca.png" width="1459" height="800" class="img_ev3q"></p>
<p>最后的损失函数是三者的加权组合</p>
<p>目前SPLADE稀疏向量的召回率已经显著由于BM25传统搜索引擎的召回率</p>
<p><img decoding="async" loading="lazy" src="https://miro.medium.com/v2/resize:fit:1155/1*nfUexIXcqoIaDUFLYcPtwg.png" alt="img" class="img_ev3q"></p>
<p>但是BM25等传统方法就完全不行了吗？也未必，有文章指出SPLADE这样的基于模型的方法始终会受到预训练语料的领域限制，并且在垂域少量数据上训练/微调的成本开销都比较大，此时未必有简单的BM25 + 领域定制词典权重好</p>
<p>而作为召回的一道路径来说，还有许多额外的召回规则，例如通配符和前缀，编辑距离和短语......</p>
<hr>
<p>很fashion的reranker <a href="https://www.mixedbread.com/blog/mxbai-rerank-v2" target="_blank" rel="noopener noreferrer">https://www.mixedbread.com/blog/mxbai-rerank-v2</a></p>
<p>双模搜索：<a href="https://www.mixedbread.com/blog/the-hidden-ceiling" target="_blank" rel="noopener noreferrer">https://www.mixedbread.com/blog/the-hidden-ceiling</a></p>
<p>做了一系列实验证明了OCR质量在RAG系统中的重要性和目前的OCR方法质量的局限性，多模态生成用于检索的编码，OCR生成嵌入：检索时用能理解文本布局、复杂图表的多模态嵌入，而进入LLM的时候用OCR生成的文本</p>
<ul>
<li>他们也实验了使用图片/嵌入直接进视觉LLM，但效果不佳。提出的观点是LLM能够容忍文本的噪声和解析错误（文字质量下降），但不太能容忍精致且无关的文本（搜索质量下降），在传统rag流程中，OCR质量下降会直接导致搜索质量下降</li>
</ul></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/milvus">milvus</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/sparse-embedding">sparse embedding</a></li></ul></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/blog/RocketMQ">RocketMQ学习</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-05-24T00:00:00.000Z">May 24, 2025</time> · <!-- -->19 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><div class="avatar__intro"><div class="avatar__name"><span>ayanami</span></div></div></div></div></div></header><div class="markdown"><p>mq: 异步，解耦，削峰填谷</p>
<p>传统项目架构下，对网络波动没有耐受性</p>
<p>mq多用于分布式系统间进行通信</p>
<p>请求方/响应方 <code>-&gt;</code> 生产者/消费者</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="优劣">优劣<a class="hash-link" aria-label="Direct link to 优劣" title="Direct link to 优劣" href="/blog/page/2#优劣">​</a></h3>
<p>优势：异步，解耦，削峰填谷</p>
<ul>
<li>解耦: 消费方存活与否不影响生产方</li>
<li>异步: 提速</li>
<li>削峰填谷（作为一个buffer/cache）, 提升系统稳定性，应对突发性高并发冲击</li>
</ul>
<p>例子-电子商务下单：</p>
<p>生产者: 订单系统 <code>-&gt;</code> MQ <code>-&gt;</code> 库存系统、支付系统、物流系统、大数据系统(用户数据收集)（复制与多分发）</p>
<p>生产者发完消息，可以继续下一步业务逻辑</p>
<p>订单系统不需要新增业务代码，达成解耦</p>
<p>同时，订单系统可以发MQ消息之后就返回。准确地说，MQ消息 + 订单入库(校验等)</p>
<p>劣势：</p>
<ul>
<li>可用性降低: MQ宕机就寄，需要保证MQ的高可用</li>
<li>系统复杂度提高：<strong>消息丢失? 消息保序？重复消费？</strong></li>
<li>一致性问题：多消费，部分成功，部分失败？下游失败怎么办？</li>
</ul>
<p>市面主流MQ产品：</p>
<ul>
<li>ActiveMQ: 万级吞吐，主从架构，ms延迟，现在不怎么用</li>
<li>RabbitMQ: erlang，us处理，万级吞  吐，主从架构，较难维护</li>
<li>RocketMQ: java，十万级吞吐，ms级，分布式</li>
<li>kafka: scala， 十万级，ms级，分布式，功能比较少</li>
</ul>
<p>rocketmq: 17年双十一，TPS 5600w</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="架构">架构<a class="hash-link" aria-label="Direct link to 架构" title="Direct link to 架构" href="/blog/page/2#架构">​</a></h3>
<p>生产者集群 Producer</p>
<p>消息服务器集群 Broker 接受消息，提供消息，消息持久化，过滤消息，高可用</p>
<p>消费者 Consumer</p>
<p>命名服务器集群 NameServer Cluster 存储元数据**（Broker IPs）**</p>
<p>producer，broker，consumer向nameserver注册，nameserver用心跳确认其他组件的存活</p>
<p><img decoding="async" loading="lazy" alt="image-20250524133745882" src="/assets/images/image-20250524133745882-2079dd6b627ccbcc592dcdba316130f8.png" width="1723" height="844" class="img_ev3q"></p>
<p>支持拉推两种模式，Consumer可以主动拉取，也可以用监听器模式</p>
<p>能推肯定是推省资源，免去轮询等</p>
<p>消息</p>
<ul>
<li>Message</li>
<li>Topic 一级标题</li>
<li>Tag 二级标题</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="基础流程">基础流程:<a class="hash-link" aria-label="Direct link to 基础流程:" title="Direct link to 基础流程:" href="/blog/page/2#基础流程">​</a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="producer">Producer：<a class="hash-link" aria-label="Direct link to Producer：" title="Direct link to Producer：" href="/blog/page/2#producer">​</a></h4>
<p>生产者创建 - 设置nameserver - 生产者启动 - 创建消息（指定topic，tag，内容）- 发送（获取结果）- 关闭生产者</p>
<p>发送结果是什么？主要是记录消息元数据例如消息ID的一个结构体，和<code>Future</code>那种设计不一样</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="consumer">Consumer<a class="hash-link" aria-label="Direct link to Consumer" title="Direct link to Consumer" href="/blog/page/2#consumer">​</a></h4>
<p>两类： <code>DefaultLitePullConsumer</code> 和 <code>DefaultMQPushConsumer</code></p>
<p>对应拉取(额外线程轮询)和推送(长连接)模式</p>
<p>消费者创建 - 设置nameserver - 设置监听(订阅)主题 - 注册监听器(消息处理函数类，返回消息处理结果) - 消费者启动</p>
<p>设置监听的api rocketmq是这样设计的</p>
<p><code>subscribe(&lt;topic&gt;, &lt;subExpression&gt;)</code></p>
<p>这个<code>subExpression</code>通过通配符等支持，让api 表达力变强不少</p>
<ul>
<li>支持tag过滤</li>
<li>支持sql过滤，在给<strong>消息追加属性</strong>的时候很有用<!-- -->
<ul>
<li><code>&gt;&lt;=</code> <code>BETWEEN</code> <code>IN</code> <code>IS NULL</code> <code>AND</code> <code>OR</code> <code>NOT</code></li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="onetomany-多消费">OneToMany 多消费<a class="hash-link" aria-label="Direct link to OneToMany 多消费" title="Direct link to OneToMany 多消费" href="/blog/page/2#onetomany-多消费">​</a></h3>
<p>多个消费者监听一个topic的默认行为：</p>
<ul>
<li><strong>一条消息只会被消费一次</strong></li>
<li><strong>多个消费者之间有默认的负载均衡</strong></li>
</ul>
<p>如果想要多个消费者都消费这条消息呢？例如上面的电商情况</p>
<ul>
<li><strong>消费者组</strong> consumer group概念</li>
</ul>
<p>相同组的消费者，有负载均衡，单消费</p>
<ul>
<li><strong>也可以修改消息模式</strong>，将默认的消费模式改掉<!-- -->
<ul>
<li><code>CLUSTERING -&gt; BROADCASTING</code></li>
</ul>
</li>
</ul>
<p>单条消息会被<strong>复制数份</strong>，发送到每一个消费者组</p>
<ul>
<li>“复制”是指HTTP传数次，不是存储文件复制</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="消息类型">消息类型<a class="hash-link" aria-label="Direct link to 消息 类型" title="Direct link to 消息类型" href="/blog/page/2#消息类型">​</a></h3>
<p>同步：即时性强、必须有回执，例如短信通知</p>
<p>异步：即时性弱，也需要有回执，如订单信息</p>
<p>单向：不需要有回执，如写日志</p>
<ul>
<li>eg 分布式日志系统，所有Producer只管发</li>
</ul>
<p>直接<code>send(msg)</code> 是发同步消息</p>
<p><code>send(msg, callback)</code>是异步，等有结果再做处理</p>
<p><code>sendOneway</code>是单向</p>
<p>延时消息</p>
<p>早期: v4.x</p>
<p>只支持不同的delayLevel，固定的1s 1m这样的时间</p>
<ul>
<li>固定级别的延时消息实现简单，<strong>为每个延时类别创建单独的队列来管理， 采用内部特定主题（SCHEDULE_TOPIC_XXXX）和队列来实现延时功能</strong></li>
<li>可能是简单的分队列定时扫描算法</li>
</ul>
<p>后来：v5+</p>
<p>任意毫秒级时间戳延时，需要高效时间轮算法, 每条消息单独计时器跟踪</p>
<ul>
<li>
<p><strong>时间轮算法</strong>：小时轮，分钟轮，秒钟轮。将消息“填入时间轮槽”（即每个槽是一个<code>TaskList</code>）</p>
<ul>
<li><strong>层级时间轮</strong>，如果一个任务是1分30s, 会先被放入1分的分钟轮，处理到时，减去1分，降级放入秒钟轮</li>
</ul>
</li>
<li>
<p>将时间线分割成多个区间，不同区间采用不同精度的扫描策略, 近期消息采用高精度扫描，远期消息采用低频率扫描</p>
</li>
<li>
<p>高效索引，分布式时钟对齐.....</p>
</li>
</ul>
<p>批量消息</p>
<p>底层支持直接传 <code>Collection&lt;Message&gt;</code></p>
<p>注意:</p>
<ul>
<li>相同的topic</li>
<li>不能是延时消息</li>
<li>总长度不超过<strong>4M</strong> (IBM默认最大消息长度设置，可以通过改环境变量修改，但4M算是一个实验值)</li>
<li>相同的waitStoreMsgOK</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="spring-ioc集成">Spring IoC集成<a class="hash-link" aria-label="Direct link to Spring IoC集成" title="Direct link to Spring IoC集成" href="/blog/page/2#spring-ioc集成">​</a></h3>
<p>直接在配置文件中指定name-server， producer group之类</p>
<p>类似Kafka/Redis, <code>RocketMQTemplate</code> 链接管理</p>
<p><code>convertAndSend</code> 方法</p>
<ul>
<li><code>convert</code>? Spring的Template send发送的是抽象的message，只有一个<code>byte[]</code>的<code>payload</code>，convert就是在处理上层java类与下层不同的template需要的通信格式的转换</li>
</ul>
<p>消费者也是和Kafka类似的</p>
<p><code>@Service</code>的类 <code>implements RocketMQListener&lt;MessageType&gt;</code> 就行,  <code>@RocketMQListener(topic=, tag=,consumerGroup=, selctorExpression=, selectorType=, messageModal=)</code></p>
<p>然后这个类就作为监听类了，调用的方法就是重载方法<code>onMessage(T t)</code></p>
<p>(这里Spring顺带还做了个返回值处理，只要没抛异常都是消费成功，抛异常消费失败回传)</p>
<p>其他的机制也整合了，例如同步异步单向延时批量之类对应<code>syncSend, asyncSend, ...</code></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="消息保序">消息保序<a class="hash-link" aria-label="Direct link to 消息保序" title="Direct link to 消息保序" href="/blog/page/2#消息保序">​</a></h3>
<p>消息错乱的原因，队列内有序，队列外无序</p>
<p>要做多队列的负载均衡，就不能无开销严格保证顺序</p>
<p>一连串的消息需要作为一个不能被拆分到多个负载均衡队列的整体</p>
<ul>
<li>一个实现<code>messagequeueSelector</code>的实体类，例如id hash + 取模</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="事务消息无丢">事务消息（无丢）<a class="hash-link" aria-label="Direct link to 事务消息（无丢）" title="Direct link to 事务消息（无丢）" href="/blog/page/2#事务消息无丢">​</a></h3>
<p><img decoding="async" loading="lazy" alt="image-20250524165007633" src="/assets/images/image-20250524165007633-44d4c669017b42796867ad8a572764af.png" width="1621" height="776" class="img_ev3q"></p>
<p>本地事务：如入本地数据库</p>
<p>事务状态：</p>
<ul>
<li>提交状态，允许进入队列</li>
<li>回滚状态，不允许进入队列，当作没发生过</li>
<li>中间状态，未对half做二次确认</li>
</ul>
<p>代码实现</p>
<p><code>TransactionMQProducer</code></p>
<p><code>setTransactionListener</code>: 正常事务过程, 补偿过程</p>
<ul>
<li><code>executeLocalTransaction</code>: 正常事务，入库等，根据本地事务状态返回消息状态</li>
<li><code>checkLocalTransaction</code>：在正常事务超时等情况（实际上是正常事务函数返回了<code>UNKNOW</code>状态）时被调用，本地再次查询事务状态的函数</li>
</ul>
<p>事务补偿还是<code>UNKNOW</code>?写个log或者其他人工介入方式</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="集群搭建">集群搭建<a class="hash-link" aria-label="Direct link to 集群搭建" title="Direct link to 集群搭建" href="/blog/page/2#集群搭建">​</a></h3>
<p>多broker：</p>
<p>多master多slave架构</p>
<p>master slave同步消息的方式可以是同步（生产者阻塞请求）也可以是异步（不阻塞）</p>
<p>常见：一主三从</p>
<ul>
<li>只有brokerId为0的是主节点</li>
<li>brokerName是集群名</li>
</ul>
<p>每一个broker会向<strong>所有的</strong>nameserver注册</p>
<p><img decoding="async" loading="lazy" alt="image-20250525132947941" src="/assets/images/image-20250525132947941-9a53ed1bdc0ac1caef4e61df5db97513.png" width="1576" height="521" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="高级特性">高级特性<a class="hash-link" aria-label="Direct link to 高级特性" title="Direct link to 高级特性" href="/blog/page/2#高级特性">​</a></h3>
<p>消息存储</p>
<p>一次完整的消费需要两个ACK</p>
<p>即Producer向Broker发消息，Broker返回ACK</p>
<p>Broker向Consumer发消息，Consumer返回ACK</p>
<p>但如果中间Broker宕机，就会出现重复消费</p>
<p>例如，在Broker返回ACK之前宕机，Producer就可能再发一次消息；在Consumer返回ACK之前宕机，Broker就可能再发一次消息。</p>
<p>解决方案是，<strong>在Broker返回Producer ACK之前，先将消息存储到磁盘上持久化（数据库中），在接收到Consumer ACK之后，Broker删除这一条消息</strong></p>
<ul>
<li>如果在返回Producer ACK之前宕机，能从磁盘读消息避免重发</li>
<li>如果在Consumer返回ACK之前宕机，也是同理</li>
</ul>
<p>消息的存储介质</p>
<p>使用<strong>数据库</strong>：</p>
<ul>
<li>ActiveMQ：缺点是<strong>数据库瓶颈成为MQ瓶颈</strong></li>
</ul>
<p><strong>文件系统</strong>：</p>
<ul>
<li><strong>RocketMQ/Kafka/RabbitMQ：采用消息刷盘机制，进行数据存储</strong></li>
</ul>
<p>zero copy：mmap，java MappedByteBuffer</p>
<p>预留了一块空间进行顺序读写，默认1G commitlog</p>
<p>本质上，利用<code>mmap</code>,<code>sendfile</code>等系统api减少了<strong>内核空间与用户空间</strong>的数据交换次数。<code>mmap</code>处理<code>文件-内存</code>在内核态直通，<code>sendfile</code>处理<code>内存-网络</code>在内核态直通。省去的是内核态内存页到用户态内存页的拷贝，zero copy的用户程序都是没有持有数据copy的buffer的。</p>
<p><img decoding="async" loading="lazy" alt="image-20250525135808051" src="/assets/images/image-20250525135808051-fd6ae5ce6cf4bf10575448fffe1f8747.png" width="1557" height="859" class="img_ev3q"></p>
<p>刷盘机制</p>
<p>同步刷盘：先<strong>入盘</strong>再返回ACK</p>
<ul>
<li>可靠性高，性能低</li>
</ul>
<p>异步刷盘：<strong>不挂起Producer线程</strong>，也先不写硬盘，<strong>将消息保留到内存之后就向Producer返回ACK</strong>，而是<strong>积累到一定batch的消息，再批量刷盘</strong></p>
<p>高可用：</p>
<ul>
<li>nameserver:<!-- -->
<ul>
<li>无状态+全服务器注册</li>
</ul>
</li>
<li>消息服务器<!-- -->
<ul>
<li>主从架构，2主2从</li>
</ul>
</li>
<li>消息生产<!-- -->
<ul>
<li>生产者将相同的topic绑定到多个group组，保障master挂掉之后，其他master仍然可以正常接受消息</li>
</ul>
</li>
<li>消息消费：RocketMQ会根据master压力确认是否由master承担数据读取功能，master繁忙的时候，自动切换slave做承担数据读取的工作（<strong>读写分离</strong>）</li>
</ul>
<p>负载均衡</p>
<p>Producer负载均衡</p>
<ul>
<li>RocketMQ内部实现了不同broker集群中对同一topic对应消费队列的负载均衡</li>
</ul>
<p>Consumer负载均衡</p>
<ul>
<li>平均分配(AABBCC)不好，循环平均分配(ABCABC)好<!-- -->
<ul>
<li>原因，broker部分挂掉时，生产者的流量会被均分到剩下的broker上，如果平均分配，则有些对应挂掉的broker的consumer就不干活了，其他consumer压力会变大；循环平均分配则是将所有的consumer都分到剩下的broker上，避免了单个consumer压力过大</li>
</ul>
</li>
</ul>
<p>消息重试：</p>
<p>顺序消息：</p>
<ul>
<li>当消费消息失败后，RocketMQ会以1s为间隔进行自动重试。</li>
<li>应用会出现消息消费被阻塞的情况，因此需要对顺序消息的消费情况进行监控（监控offset等），避免阻塞</li>
</ul>
<p>无序消息：</p>
<ul>
<li>
<p>仅适用于负载均衡（集群）模型下的消息消费，不适用于广播模式</p>
</li>
<li>
<p>MQ设定了合理的消息重试间隔时长，有一个指数的backoff</p>
</li>
<li>
<p>当重试到达指定次数（默认16次）后，MQ将无法被正常消费的消息称为死信消息。<strong>死信消息不会被直接抛弃，而是会被发送到一个死信队列中</strong>，供后续处理</p>
</li>
</ul>
<p>死信消息不会再被重复消费，有效期为3天，过时后会被删除</p>
<p>死信处理 ，业务逻辑处理，或者人工介入</p>
<p>RocketMQ不可能完全避免重复消费，还是存在可能出现重复消费的情况：</p>
<ul>
<li>生产者发送重复消息，例如，网络闪断没收到ACK，生产者宕机</li>
<li>Broker和消费者之间网络闪断，消费者/broker重启</li>
<li>客户端扩缩容</li>
<li>......</li>
</ul>
<p><strong>所以不能完全依赖RocketMQ的幂等性，还是要在业务逻辑上做幂等性处理</strong></p>
<ul>
<li>使用业务id作为消息key</li>
<li>在消费消息时，客户端对key做判定，未使用放行，使用过抛弃</li>
<li>注意：messageId由RocketMQ生成，不具有唯一性，不能做幂等判定条件</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="kafka-vs-rocketmq">Kafka VS RocketMQ<a class="hash-link" aria-label="Direct link to Kafka VS RocketMQ" title="Direct link to Kafka VS RocketMQ" href="/blog/page/2#kafka-vs-rocketmq">​</a></h3>
<p><strong>Kafka:</strong></p>
<ul>
<li><strong>专注简单与吞吐量</strong>: &quot;Do one thing and do it well&quot;的Unix哲学，专注于高吞吐的消息传递</li>
<li><strong>不可变数据流</strong>: 将消息视为不可变的数据流，适合事件溯源和流处理</li>
<li><strong>客户端复杂性</strong>: 将复杂性推向客户端，保持服务端简单高效</li>
</ul>
<p><strong>RocketMQ:</strong></p>
<ul>
<li><strong>丰富的消息功能</strong>: 目标是作为全功能的企业级消息系统</li>
<li><strong>服务端智能</strong>: 在服务端实现更多功能，减轻客户端负担</li>
<li><strong>电商场景驱动</strong>: 由阿里巴巴电商业务需求驱动设计，面向复杂业务场景</li>
</ul>
<p>那么古尔丹，高吞吐量的代价是什么呢？</p>
<ul>
<li>偏移量指针的设计只能顺序前进，无法原生支持延迟时间，通过时间戳索引查找偏移量、专用延时主题、定时扫描来达到延时队列</li>
<li>必须顺序处理消息，无法灵活跳过（异常消息）和回退（重放）</li>
<li>消息路由和分布式一致性绑定，路由灵活性受限</li>
<li>不支持消息优先级队列，因为都得按照offset指针顺序处理.....</li>
<li>无法设置可见性超时等，都需要上层应用做</li>
<li>消费失败的幂等性保证处理复杂，偏移量需要分布式维护增加网络开销.....</li>
</ul>
<table><thead><tr><th>特性</th><th>Kafka</th><th>RocketMQ</th></tr></thead><tbody><tr><td>定时/延时消息</td><td>需外部实现</td><td>原生支持 <a href="https://rocketmq.apache.org/docs/" target="_blank" rel="noopener noreferrer">1</a></td></tr><tr><td>消息回溯</td><td>支持(通过偏移量)</td><td>支持(更灵活) <a href="https://rocketmq.apache.org/docs/" target="_blank" rel="noopener noreferrer">1</a></td></tr><tr><td>消息过滤</td><td>有限支持</td><td>服务器端支持SQL92表达式过滤 <a href="https://rocketmq.apache.org/docs/" target="_blank" rel="noopener noreferrer">1</a></td></tr><tr><td>事务消息</td><td>有限支持</td><td>完整支持 <a href="https://rocketmq.apache.org/docs/" target="_blank" rel="noopener noreferrer">1</a></td></tr><tr><td>死信队列</td><td>不支持</td><td>支持</td></tr><tr><td>消息优先级</td><td>不支持</td><td>不直接支持，但可通过设计实现</td></tr><tr><td>多租户隔离</td><td>有限支持</td><td>更好支持</td></tr><tr><td>消息轨迹追踪</td><td>需外部工具</td><td>原生支持 <a href="https://rocketmq.apache.org/docs/" target="_blank" rel="noopener noreferrer">1</a></td></tr></tbody></table>
<p>核心设计的哪些不同带来了这样的差异？</p>
<p>存储模型</p>
<p><strong>Kafka:</strong></p>
<ul>
<li><strong>分散的文件存储</strong>: 每个主题的每个分区对应一个物理文件，消息按照写入顺序存储 <a href="https://rocketmq.apache.org/docs/" target="_blank" rel="noopener noreferrer">1</a></li>
<li><strong>顺序追加写入</strong>: 使用顺序追加的方式写入文件，不允许修改已写入的数据</li>
<li><strong>偏移量指针</strong>: 消费者通过偏移量指针确定读取位置，不复制消息</li>
</ul>
<p><strong>RocketMQ:</strong></p>
<ul>
<li><strong>统一的文件存储</strong>: 所有主题的消息存储在同一组物理文件中 <a href="https://alibaba-cloud.medium.com/kafka-vs-rocketmq-multiple-topic-stress-test-results-d27b8cbb360f" target="_blank" rel="noopener noreferrer">3</a></li>
<li><strong>逻辑分区</strong>: 主题和队列仅是逻辑概念，不与物理文件一一对应</li>
<li><strong>消息索引</strong>: 维护更复杂的索引结构，支持按多种方式查询消息</li>
</ul>
<p>消息投递模型</p>
<p><strong>Kafka:</strong></p>
<ul>
<li><strong>基于分区的消费模型</strong>: 消费者组内的消费者分配分区，消费者只能按顺序消费分区中的消息 <a href="https://rocketmq.apache.org/docs/" target="_blank" rel="noopener noreferrer">1</a></li>
<li><strong>仅支持拉模式</strong>: 消费者主动从Broker拉取消息</li>
</ul>
<p><strong>RocketMQ:</strong></p>
<ul>
<li><strong>更灵活的消费模型</strong>: 支持更多的消费模式，包括集群消费和广播消费 <a href="https://rocketmq.apache.org/docs/" target="_blank" rel="noopener noreferrer">1</a></li>
<li><strong>推拉结合</strong>: 同时支持推模式和拉模式，提供更灵活的消息投递方式</li>
<li><strong>消息过滤</strong>: 支持在服务器端进行消息过滤，减少不必要的网络传输</li>
</ul>
<p>消息处理机制</p>
<p><strong>Kafka:</strong> 消息就是字节数组</p>
<p>**RocketMQ: ** 消息包含更多元数据和属性</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/mq">mq</a></li></ul></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/blog/cs186-database-WIP">ucb cs186 课程笔记(更新中)</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-02-12T13:56:21.000Z">February 12, 2025</time> · <!-- -->8 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><div class="avatar__intro"><div class="avatar__name"><span>ayanami</span></div></div></div></div></div></header><div class="markdown"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="lec2">lec2<a class="hash-link" aria-label="Direct link to lec2" title="Direct link to lec2" href="/blog/page/2#lec2">​</a></h2>
<p>join: inner join, natural join, outer join</p>
<p>sql  实际执行模型 写起来是 SELECT - FROM - GROUP BY - HAVING - WHERE - DISTINCT - ORDER BY</p>
<p>实际是 FROM(table过滤) - GRUOP BY(行分组) - HAVING(组过滤) - WHERE(行过滤) - DISTINCT(行去重) - SELECT(行内列过滤)</p>
<p>inner join:叉积，对AB所有行组合</p>
<div class="language-sql codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-sql codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">SELECT</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">FROM</span><span class="token plain"> TABLE1 t1</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> TABLE2 t2 </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	</span><span class="token keyword" style="color:#00009f">WHERE</span><span class="token plain"> t1</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">id </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> t2</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">id</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	</span><span class="token operator" style="color:#393A34">AND</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">.</span><span class="token punctuation" style="color:#393A34">.</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic">-- 等效于</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">SELECT</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">FROM</span><span class="token plain"> </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	TABLE1 t1 </span><span class="token keyword" style="color:#00009f">INNER</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">JOIN</span><span class="token plain"> TABLE2 t2</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">ON</span><span class="token plain"> t1</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">id </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> t2</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">id</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">WHERE</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">.</span><span class="token punctuation" style="color:#393A34">.</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic">-- 下面这种更加清晰一点</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic">-- 等效于</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">SELECT</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">FROM</span><span class="token plain"> </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	TABLE1 t1 </span><span class="token keyword" style="color:#00009f">NATURAL</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">JOIN</span><span class="token plain"> TABLE2 t2</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">WHERE</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">.</span><span class="token punctuation" style="color:#393A34">.</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic">-- natural join就是在组合的基础上自动用了一个过滤，要求table所 有相同名字的列的值都相同</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>outer join:</p>
<p>Left Outer join:</p>
<p><code>A LEFT OUTER JOIN B ON cond</code> 如果cond满足的话，得到的是AB的组合（一行有A的列+B的列）;如果不满足，得到A的列+空</p>
<p>Right Outer Join 同理</p>
<p>Full Outer Join 同理 例如<code>ON A.id = B.id</code></p>
<p>如果有A没有对应的B, 那就是是 A + 空</p>
<p>如果有B没有对应的A, 那就是 空 + B</p>
<p>非常好的图</p>
<p><img decoding="async" loading="lazy" alt="db-join" src="/assets/images/image-f9fa91e00a44e27c8e39a9d1bf4c6d74.png" width="1419" height="322" class="img_ev3q"></p>
<p>alias</p>
<p>简化 + 看起来更清楚（尤其是self-join）</p>
<p><code>FROM TABLE1 AS x, TABLE1 AS y</code></p>
<p>String Comp</p>
<p><code>LIKE</code>或者正则<code>S.name ~ &#x27;^B.*&#x27;</code> (等效于<code>S.name LIKE &#x27;B_%&#x27;</code>)</p>
<p><code>AND</code> <code>OR</code> 做条件交并</p>
<p><code>EXCEPT</code> <code>UNION (ALL)</code> <code>INTERSECT</code>做子查询结果集合的交并差</p>
<p><code>IN</code> <code>EXISTS</code>用于子查询 (<code>NOT IN</code>, <code>NOT EXIST</code>) EXISTS是判空</p>
<div class="language-SQL language-sql codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-sql codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">SELECT</span><span class="token plain"> S</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">sname </span><span class="token keyword" style="color:#00009f">FROM</span><span class="token plain"> Sailors S </span><span class="token keyword" style="color:#00009f">WHERE</span><span class="token plain"> S</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">sid </span><span class="token operator" style="color:#393A34">IN</span><span class="token plain"> </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	</span><span class="token punctuation" style="color:#393A34">(</span><span class="token keyword" style="color:#00009f">SELECT</span><span class="token plain"> R</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">sid </span><span class="token keyword" style="color:#00009f">FROM</span><span class="token plain"> Reserves R </span><span class="token keyword" style="color:#00009f">WHERE</span><span class="token plain"> R</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">bid</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">102</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>还有<code>ANY</code> <code>ALL</code></p>
<p>ARGMAX?</p>
<div class="language-SQL language-sql codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-sql codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">SELECT</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">FROM</span><span class="token plain"> Sailors S </span><span class="token keyword" style="color:#00009f">WHERE</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	S</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">rating </span><span class="token operator" style="color:#393A34">&gt;=</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">ALL</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">		</span><span class="token punctuation" style="color:#393A34">(</span><span class="token keyword" style="color:#00009f">SELECT</span><span class="token plain"> S2</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">rating </span><span class="token keyword" style="color:#00009f">FROM</span><span class="token plain"> Sailors S2</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>View: Named Queries</p>
<div class="language-sql codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-sql codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">CREATE</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">VIEW</span><span class="token plain"> xxx</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token keyword" style="color:#00009f">AS</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">.</span><span class="token punctuation" style="color:#393A34">.</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">SELECT</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">FROM</span><span class="token plain"> xxx</span><span class="token punctuation" style="color:#393A34">;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>cache and reuse</p>
<p>或者</p>
<p><code>WITH [viewname] AS [statement]</code>创建一个临时view</p>
<p>NULL 参与的运算大多是NULL, 除了IS NULL，False AND NULL这种</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="lec3">lec3<a class="hash-link" aria-label="Direct link to lec3" title="Direct link to lec3" href="/blog/page/2#lec3">​</a></h2>
<p>Disk &amp; Buffer</p>
<p>整体架构</p>
<p>SQL client-&gt; Query Parsing &amp; Optimization-&gt;Relational Operators-&gt; Files and Index Management-&gt;Buffer Management-&gt;Disk Space Management</p>
<p>Concurrency Control &amp; Recovery</p>
<p>磁盘太慢，需要尽量减少读写，且寻道和旋转时间是大头</p>
<p>&quot;block&quot; &amp;&amp; &quot;page&quot;: 一个意思，磁盘上的块状读写最小单元 一般64KB-128KB</p>
<p>为了重用硬件驱动，经常会将磁盘空间管理器建立在文件系统API上，但带来了一些大数据库多文件系统的问题，也有直接建立在设备上的，更快但是移植性问题</p>
<p>给上层的抽象是一个巨大的文件</p>
<p>DB file: page的集合，每个page又包含了许多records</p>
<p>给上层提供：CRUD on records</p>
<p>record解构成一个&quot;指针&quot; <code>{pageID, location on page}</code></p>
<p>structures</p>
<ul>
<li>Unordered Heap Files(和数据结构heap没啥关系，无序records)</li>
<li>Clustered Heap Files</li>
<li>Sorted Files</li>
<li>Index Files</li>
</ul>
<p> 如何组织page呢？</p>
<p>链表？ 想想就知道效率很差</p>
<p>类似目录的形式？ 部分page只存到其他page的指针，并且始终放在缓存之中</p>
<p>page解构</p>
<p>Page Header:</p>
<ul>
<li>Number of records</li>
<li>Free space</li>
<li>Mayba a last/next pointer</li>
<li>Bitmaps, slot table</li>
</ul>
<p>record 中间留不留空？</p>
<p>不留空：Fixed Length Records, Packed</p>
<p>header后面跟紧密定长records, 因此可以有 record id = <code>{pageId, record number in page}</code>， 简单运算得到location</p>
<p>加很简单，直接append</p>
<p>删，全移一遍？-&gt;O(N),自然想到能不能lazy delete或者soft delete</p>
<p>方法是在header里面放一个delete bit的bitmap</p>
<p>变长？</p>
<p>slotted page</p>
<p>将信息存在footer（称为slot directory）, record从头部开始存</p>
<p>由record id得到dir中位置，位置里面是pointer + length，</p>
<p>删，将slot dir中的项置空</p>
<p>插入，插在空位上，更新slot dir</p>
<p>fragmentation?</p>
<p>什么时候reorganize?-&gt;设计取舍，大部分时候没有那么多删除（乐）</p>
<p>slot不够-&gt;从page尾部向前增长</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="lec4">lec4<a class="hash-link" aria-label="Direct link to lec4" title="Direct link to lec4" href="/blog/page/2#lec4">​</a></h2>
<p>cost model for ayalysis</p>
<p>B, D, R</p>
<ul>
<li>the number of data blocks</li>
<li>the number of records per clock</li>
<li>avg time to r/w disk block</li>
<li>opt: index</li>
</ul>
<p>indexes:</p>
<p>大幅度降低range操作耗时</p>
<p>An index is data structure that enables fast lookup and modification of data entries by search key</p>
<p>区间查找 &amp; 子集搜索, 可以复合, 不需要唯一</p>
<p>2-d box 2-d circle n-d indexes都有</p>
<p>kd树啊R树啊</p>
<p>postgres 的 GiST index</p>
<p>left key opt: 最小的key是不需要的,直接拿-inf当下界就行</p>
<p>处理相等:<code>&gt;=</code> 向右走就行</p>
<p>B+树</p>
<ul>
<li>
<p>叶子不一定是连续的-动态分配,指针连接以支持range scan</p>
</li>
<li>
<p>阶数d, fan-out 2d+1 典型的fan-out 为2144()</p>
</li>
<li>
<p>删除, 理论上来说, 可能涉及到重新平衡等操作 但实际的操作之中, 只需要删除即可, 原因是平衡太慢了,并且删了也能再插</p>
</li>
</ul>
<p>叶子放什么?</p>
<ol>
<li>数据</li>
</ol>
<p>pros:</p>
<ul>
<li>快</li>
</ul>
<p>cons:</p>
<ul>
<li>想要在另一列构建索引只能重新复制文件(文件只能按照一种方式实际排序存储)</li>
<li>即使真复制了,同步问题也很寄</li>
</ul>
<ol start="2">
<li>指向数据的指针 (key, page id+list of record id)</li>
</ol>
<p>在b+树里面有重复项</p>
<ol start="3">
<li>指向同一个键的所有records (key, list of (page id + list of record id))</li>
</ol>
<p>减少冗余,增加复杂性</p>
<p>clustered: index指向的数据块在磁盘上是按照这个index排序或者近似排序的</p>
<p>非常大影响性能 顺序比随机快100倍</p>
<p>对于一个有变化的数据,例如插入或者删除,需要一些成本进行磁盘数据的重新排序来维持clustered</p>
<p>B+树的平衡性:</p>
<p>使用字节数半满(占页面容量)就行, 甚至实际上更低, 按照实际性能来决定,不严格</p>
<p>变长key: 前缀压缩 trie</p>
<p>性能的常数:</p>
<p>由于顺序读写比随机读写快100倍</p>
<p>B+树比全表扫描差不多也是涉及到1%以下的表才有显著优势</p>
<p>所以例如对一个非聚簇索引进行一个跨越半个表的range的扫描, 那还不如直接把全表取出来</p>
<p>优化</p>
<p>由于B+树效率真的很低,所以有很多优化策略</p>
<ul>
<li>bulk loading 批量装载</li>
</ul>
<ol>
<li>Sort the data by a key.</li>
<li>Fill leaf pages up to size f (the <strong>fill factor</strong>).</li>
<li>If the leaf page overflows, then use the insertion split algorithm from a normal B+ tree.</li>
<li>Adjust pointers to reflect new nodes if needed.</li>
</ol></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/database">database</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/system">system</a></li></ul></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/blog/jyy-os-虚拟化">NJU操作系统(jyy OS)课程笔记-虚拟化部分</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-02-12T13:56:21.000Z">February 12, 2025</time> · <!-- -->18 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><div class="avatar__intro"><div class="avatar__name"><span>ayanami</span></div></div></div></div></div></header><div class="markdown"><h3 class="anchor anchorWithStickyNavbar_LWe7" id="lec14-操作系统上的进程">lec14 操作系统上的进程<a class="hash-link" aria-label="Direct link to lec14 操作系统上的进程" title="Direct link to lec14 操作系统上的进程" href="/blog/page/2#lec14-操作系统上的进程">​</a></h3>
<p>cpu有初始pc地址<code>-&gt;</code>放置固件上的初始程序(固件状态机)<code>-&gt;</code>启动OS(os状态机)<code>-&gt;</code>load init程序(程序状态机), 之后OS完全把行为转交给init(进程树的root)</p>
<p>llm <code>知道存在</code>与<code>知道</code>的界限正在模糊: 知道存在且合理 逐渐趋同于 能做</p>
<p>例如 qemu 相关的一些东西</p>
<p>问llm发散出的概念<code>-&gt;</code>知识体系的快速建立</p>
<p>fork? 以状态机的视角理解</p>
<p>经典的for fork + printf</p>
<p>写了个示例</p>
<div class="language-cpp codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-cpp codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token macro property directive-hash" style="color:#36acaa">#</span><span class="token macro property directive keyword" style="color:#00009f">include</span><span class="token macro property" style="color:#36acaa"> </span><span class="token macro property string" style="color:#e3116c">&lt;cstddef&gt;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token macro property directive-hash" style="color:#36acaa">#</span><span class="token macro property directive keyword" style="color:#00009f">include</span><span class="token macro property" style="color:#36acaa"> </span><span class="token macro property string" style="color:#e3116c">&lt;cstdio&gt;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token macro property directive-hash" style="color:#36acaa">#</span><span class="token macro property directive keyword" style="color:#00009f">include</span><span class="token macro property" style="color:#36acaa"> </span><span class="token macro property string" style="color:#e3116c">&lt;cstdlib&gt;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token macro property directive-hash" style="color:#36acaa">#</span><span class="token macro property directive keyword" style="color:#00009f">include</span><span class="token macro property" style="color:#36acaa"> </span><span class="token macro property string" style="color:#e3116c">&lt;stdio.h&gt;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token macro property directive-hash" style="color:#36acaa">#</span><span class="token macro property directive keyword" style="color:#00009f">include</span><span class="token macro property" style="color:#36acaa"> </span><span class="token macro property string" style="color:#e3116c">&lt;unistd.h&gt;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token macro property directive-hash" style="color:#36acaa">#</span><span class="token macro property directive keyword" style="color:#00009f">include</span><span class="token macro property" style="color:#36acaa"> </span><span class="token macro property string" style="color:#e3116c">&lt;vector&gt;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token macro property directive-hash" style="color:#36acaa">#</span><span class="token macro property directive keyword" style="color:#00009f">include</span><span class="token macro property" style="color:#36acaa"> </span><span class="token macro property string" style="color:#e3116c">&lt;mutex&gt;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token macro property directive-hash" style="color:#36acaa">#</span><span class="token macro property directive keyword" style="color:#00009f">include</span><span class="token macro property" style="color:#36acaa"> </span><span class="token macro property string" style="color:#e3116c">&lt;sys/wait.h&gt;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token macro property directive-hash" style="color:#36acaa">#</span><span class="token macro property directive keyword" style="color:#00009f">include</span><span class="token macro property" style="color:#36acaa"> </span><span class="token macro property string" style="color:#e3116c">&lt;map&gt;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token macro property directive-hash" style="color:#36acaa">#</span><span class="token macro property directive keyword" style="color:#00009f">include</span><span class="token macro property" style="color:#36acaa"> </span><span class="token macro property string" style="color:#e3116c">&lt;string&gt;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">using</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">namespace</span><span class="token plain"> std</span><span class="token punctuation" style="color:#393A34">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">const</span><span class="token plain"> size_t buf_size </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1024</span><span class="token punctuation" style="color:#393A34">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">const</span><span class="token plain"> std</span><span class="token double-colon punctuation" style="color:#393A34">::</span><span class="token plain">map</span><span class="token operator" style="color:#393A34">&lt;</span><span class="token keyword" style="color:#00009f">int</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> std</span><span class="token double-colon punctuation" style="color:#393A34">::</span><span class="token plain">string</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> mode_map </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain">_IONBF</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;no buffer&quot;</span><span class="token punctuation" style="color:#393A34">}</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain">_IOLBF</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;line buffer&quot;</span><span class="token punctuation" style="color:#393A34">}</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain">_IOFBF</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;full buffer&quot;</span><span class="token punctuation" style="color:#393A34">}</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">}</span><span class="token punctuation" style="color:#393A34">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">void</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">test</span><span class="token punctuation" style="color:#393A34">(</span><span class="token keyword" style="color:#00009f">int</span><span class="token plain"> __modes</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token function" style="color:#d73a49">printf</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&quot;test in mode %s\n&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> mode_map</span><span class="token punctuation" style="color:#393A34">.</span><span class="token function" style="color:#d73a49">at</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">__modes</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token function" style="color:#d73a49">c_str</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token function" style="color:#d73a49">fflush</span><span class="token punctuation" style="color:#393A34">(</span><span class="token constant" style="color:#36acaa">stdout</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    vector</span><span class="token operator" style="color:#393A34">&lt;</span><span class="token keyword" style="color:#00009f">int</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> childs</span><span class="token punctuation" style="color:#393A34">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    std</span><span class="token double-colon punctuation" style="color:#393A34">::</span><span class="token plain">mutex mtx</span><span class="token punctuation" style="color:#393A34">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token function" style="color:#d73a49">setvbuf</span><span class="token punctuation" style="color:#393A34">(</span><span class="token constant" style="color:#36acaa">stdout</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">nullptr</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> __modes</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token keyword" style="color:#00009f">int</span><span class="token plain"> i </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0</span><span class="token punctuation" style="color:#393A34">;</span><span class="token plain"> i </span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">2</span><span class="token punctuation" style="color:#393A34">;</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">++</span><span class="token plain">i</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token keyword" style="color:#00009f">int</span><span class="token plain"> pid </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">fork</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token function" style="color:#d73a49">printf</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&quot;hello from pid %d\n&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> pid</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token keyword" style="color:#00009f">if</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">pid </span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            std</span><span class="token double-colon punctuation" style="color:#393A34">::</span><span class="token plain">lock_guard</span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain">mutex</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">lock</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">mtx</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            childs</span><span class="token punctuation" style="color:#393A34">.</span><span class="token function" style="color:#d73a49">push_back</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">pid</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token punctuation" style="color:#393A34">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">int</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">main</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic">// _IOLBF, _IOFBF, _IONBF</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token function" style="color:#d73a49">test</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">_IOFBF</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token function" style="color:#d73a49">printf</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&quot;\n&quot;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token function" style="color:#d73a49">fflush</span><span class="token punctuation" style="color:#393A34">(</span><span class="token constant" style="color:#36acaa">stdout</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">return</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0</span><span class="token punctuation" style="color:#393A34">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>在<code>_IOLBF</code>和<code>_IONBF</code>的情况下会出来6个hello</p>
<p>每次printf都直接刷新/检测到换行符刷新缓冲, fork的时候没有IO状态
而<code>_IOFBF</code>会有8个hello, 在fork第二次的时候会带着缓冲区(就是一段内存空间)进行fork,所以最后的4个进程每个都带着2个hello</p>
<p><strong>系统里面没有魔法</strong></p>
<p>fork: 把所有的知道的不知道的都复制了</p>
<p>“是不是这样?” <code>-&gt;</code> 不知道的底层状态被复制了</p>
<p><code>execve</code>: <strong>重置状态机</strong> <code>argc, argv, envp -&gt; main()</code></p>
<p>execve是唯一一个可以新建一个状 态机的系统调用</p>
<p><code>exit</code>?</p>
<ul>
<li>main return</li>
<li><code>exit</code> libc提供的</li>
<li><code>_exit</code> 系统调用退出（<code>== asm volatile(&quot;mov ..., %rax; syscall&quot;)</code>）</li>
<li>直接<code>SYSCALL</code></li>
</ul>
<p>前两个在c语言的空间, 是“normal exit”</p>
<p>后两个不是normal的, <code>_exit</code> <code>exit_group</code> , <code>__exit</code> exit self</p>
<p>行为区别? <code>strace</code></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="lec15-进程的地址空间">lec15 进程的地址空间<a class="hash-link" aria-label="Direct link to lec15 进程的地址空间" title="Direct link to lec15 进程的地址空间" href="/blog/page/2#lec15-进程的地址空间">​</a></h3>
<p>pmap</p>
<p><code>/proc/[pid]/maps</code></p>
<p>vvar(r), vdso(rx), vsyscall</p>
<p>os内只读的syscall <code>-&gt;</code> 可以以内存的形式共享</p>
<p>其实只需要进程能和OS交互一些数据就行 —— why not进程写page, OS轮询?</p>
<ul>
<li>在极端的时候能提高一些高优先级的进程的性能, 某篇OSDI</li>
</ul>
<p>地址空间应该是在运行时可变的</p>
<p>所以我们需要一个不存在于c世界的操作(syscall)去操作地址空间 <code>-&gt;</code> mmap, munmap</p>
<p>入侵进程的地址空间: gdb, perf</p>
<p>Game Genie 物理入侵地址空间</p>
<ul>
<li>外接电路: 当cpu读地址a的时候读到x, 则替换为y</li>
</ul>
<p>jyy现场演示mini CE(雾)</p>
<p>gdb attach到虚拟机,查找满足某个模式的内存值, 修改之</p>
<p><code>/proc/[pid]/mem</code> 修改器 = 调试器</p>
<p>xdotool: cmd X11 automation tool</p>
<p>ydotool: better xdotool <code>-&gt;</code> 按键精灵</p>
<p>evdev 按键显示脚本</p>
<p>xdotool测试vsc插件, crazy</p>
<p>或许不需要那么多的“魔法工具”</p>
<p>OS: 解放编程能力, 什么事情在OS上可以做</p>
<p>变速齿轮: syscall是感知时间的唯一方法</p>
<p>gdb 脚本之中, 在gettimeofday打断点,   然后修改寄存器, amazing!!!</p>
<p>hook</p>
<p>patching: 整活, kpatch, 不停机更新(软件动态链接)</p>
<p><code>old func, rx -&gt; 修改为rwx -&gt; 修改old func为, jmp到new func</code></p>
<p>在chcore里面看看? 或许有必要研究一下gdb(attach with qemu)</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="lec16-syscall--unix-shell">lec16 syscall &amp; unix shell<a class="hash-link" aria-label="Direct link to lec16 syscall &amp; unix shell" title="Direct link to lec16 syscall &amp; unix shell" href="/blog/page/2#lec16-syscall--unix-shell">​</a></h3>
<p>everything is a file</p>
<p>thing: 操作系统里面的对象</p>
<p>gpt时代的“编程”——自然语言?</p>
<div class="language-c codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-c codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic">//OS: API:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic">//	get_object_by_name(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic">//		&quot;the address space file of pid=1234&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic">//	)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p><strong>文件描述符: 指向OS对象的“指针”</strong></p>
<p>windows: handle(句柄)</p>
<p>IPC endpoints: 例子, 管道</p>
<p>管道是同步的</p>
<p>fork + pipe? 本质是&quot;指针&quot;的拷贝</p>
<p>现在两个进程都有读口和写口啦</p>
<p>shell, kernel 的外壳</p>
<p>cli: 高效简洁的编程语言</p>
<p>算力的提升: <code>cli -&gt; gui -&gt; 自然语言</code></p>
<p>shell as pl: <strong>基于文本替换的快速工作流搭建</strong></p>
<p>job control: 类比窗口管理器的&quot;x&quot;, 最小化</p>
<p>或许不需要tmux, shell就是最简单的tmux</p>
<p>手册: complete ref</p>
<p>AI是“被动的”, 读一读shell manual</p>
<p>复刻unix shell</p>
<p>“抛开系统库”</p>
<p><code>-ffreestanding -nostdlib -static</code></p>
<p>gdb init已经很常见了, 但gdb init到python再在python里面转回<code>/proc/[pid]/fd</code>打印, 最后结合gdb的内置hook,在stop时候打印, fancy!</p>
<p>这打印的不是我们go的channel语法吗, 更有趣了</p>
<p>sh manual</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="lec-17-syscall的封装-libc">lec 17 syscall的封装: libc<a class="hash-link" aria-label="Direct link to lec 17 syscall的封装: libc" title="Direct link to lec 17 syscall的封装: libc" href="/blog/page/2#lec-17-syscall的封装-libc">​</a></h3>
<p>pipe write如果小于PIPE_BUF, 是原子的</p>
<p>pipe 7</p>
<p>读者关闭: Broken pipe</p>
<p>libc 标准化, 稳定可靠, 移植性极好</p>
<p>C runtime library: -Wl, --verbose看到链接列表</p>
<p>调试glibc? 历史包袱重, 大量内联汇编, musl</p>
<p>只  要实现了C ABI指定的堆栈排布的系统调用, 就可以轻松移植musl等到自己的OS上, 底层的计算由硬件指令集给出</p>
<p>System V ABI</p>
<p>脱开workload 做优化就是耍流氓</p>
<ul>
<li>在开始考虑性能之前, 理解需要考虑什么样的性能</li>
</ul>
<p><strong>workload哪里找? 当然是paper了(顺便白得方案)</strong></p>
<ul>
<li>看wkld调性能</li>
</ul>
<p>mm alloctor: 根基</p>
<ul>
<li>大对象应该有长生存期, 否则是performance bug</li>
<li>越小的对象创建/分配越频繁</li>
<li>小对象, 中对象, 大对象</li>
</ul>
<p>瓶颈几乎是小对象</p>
<p>链表/区间树不是一个好想法: 上锁, 不能很好的并行化</p>
<p>设置两套系统：</p>
<ul>
<li>Fast path 性能极好，并行度极高，覆盖大部分情况</li>
<li>Slow path 不在乎速度，但把困难的事情做好</li>
<li>例如cache</li>
</ul>
<p>init ram fs</p>
<p>ISA -&gt; OS 对象/syscall -&gt; libc -&gt; 系统工具 coreutils, busybox -&gt; 应用程序</p>
<p>initramfs</p>
<ul>
<li>
<p>加载剩余必要的驱动程序, 例如磁盘/网卡</p>
</li>
<li>
<p>挂载必要的fs</p>
</li>
<li>
<p><strong>将根文件系统和控制权移交给另一个程序, 例如systemd</strong></p>
</li>
</ul>
<p>initramfs作为一个非常小的启动fs, 再把磁盘这个OS Object mount进来, 最后switch root把控制权给到磁盘的的根系统</p>
<p>启动的第二级阶段 /sbin/init</p>
<p>疯狂的事情不断有人在做, 但疯狂的事情的起点其实经常很小</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="lec-19-可执行文件">lec 19 可执行文件<a class="hash-link" aria-label="Direct link to lec 19 可执行文件" title="Direct link to lec 19 可执行文件" href="/blog/page/2#lec-19-可执行文件">​</a></h3>
<p>elf不是一个人类友好的“状态机数据结构描述”</p>
<p>为了性能, 彻底违背了可读(“信息局部性”)原则</p>
<p>可执行文件=OS的<strong>数据结构</strong>(core.dump), 描  述了程序应该的初始状态</p>
<p>支持的特性越多, 人类越不能理解</p>
<p>人类友好: <strong>平坦的</strong></p>
<p>回归连接和加载的核心概念: 代码、符号、重定位</p>
<p>my_execve</p>
<p>elf file -&gt; parse as struct</p>
<p>-&gt; 将各个section load到指定的地址(mmap)-&gt;asm volatile布置好ABI调用栈(根据手册)-&gt;jmp!</p>
<p>如何释放旧进程的内存资源？proc里面需要有记录</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="lec-21-syscall--ctx-switch">lec 21 syscall &amp; ctx switch<a class="hash-link" aria-label="Direct link to lec 21 syscall &amp; ctx switch" title="Direct link to lec 21 syscall &amp; ctx switch" href="/blog/page/2#lec-21-syscall--ctx-switch">​</a></h3>
<p>dynamic linker</p>
<p>se给的os基础还是很扎实的
很难想象ics2里面讲了GOT和PLT</p>
<p>SEE ALSO是一个宝藏 man ld.so</p>
<p>hacking: <code>LD_PRELOAD</code>不需要修改libc, 动态加载的全局符号, <strong>先到先得</strong></p>
<p><strong>劫持大法</strong>！</p>
<p>kernel memory mapping</p>
<p>低配版Linux 1.X 分段, 内核在低位, 只是分个段</p>
<p>低配版Linux 2.X 内核还是在物理低位, 但程序看到虚拟地址已经是高位了</p>
<p>today: complete memory map</p>
<p>qemu is a state machine simulator: 调试syscall(gdb并不能si从用户态进kernel)</p>
<p>另一种理解中断的方式：&quot;被&quot;插入一条syscall</p>
<p>中断, 把状态机的整个寄存器状态存到内存里面</p>
<p>在汇编之中小心排布内存和搬运寄存器, 返回到c之中就是结构体的context</p>
<p>schedule的核心: 调用一个“不会返回的函数”</p>
<p>这个(汇编)函数以context为参数, 并且根据context, 返回到另一处控制流...</p>
<p><code>-&gt;</code> coroutine 也是如此! OS作为一个“状态机管理器”就在做一个&quot;coroutine event handler&quot;的作用</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="lec-22-process">lec 22 process<a class="hash-link" aria-label="Direct link to lec 22 process" title="Direct link to lec 22 process" href="/blog/page/2#lec-22-process">​</a></h3>
<p>进程: “戴上VR”的thread</p>
<p>有自己的地址转换, 对一切的load/store会应用一个f，作用在addr上</p>
<p>硬件提供了“戴上VR”的指令</p>
<p>这个f从ds的视角来说就是<code>int-&gt;int</code>的映射</p>
<p>查页表(<code>int-&gt;int</code>的映射)这件事, 如何加速? --自然想到radix tree</p>
<p>普通实现是radix tree(x86, riscv, ...收敛到的最终方案)</p>
<p>每一次访存都要查这么几次的话不可接受</p>
<p>因此有了TLB, 但立刻带来的一个设计问题是, 谁来管TLB(以及对应的miss处理？)</p>
<p>x86选择放到硬件, 但丧失灵活性的后果是即使有些进程只想要f(x)=x, 也必须要老实查表, TLB在和cpu cache抢带宽</p>
<p>MIPS选择放到软件, miss了直接丢出来异常, 让软件来决定怎么处理TLB</p>
<p>疯狂的想法: inverted page table</p>
<p>把key从VPN换成 (VPN, pid), 然后从一一映射改成hashtable, 支持每个进程有自己的页表</p>
<p>缺点在例如hashtable带来的冲突时(TLB miss, etc)时间不可控(O(1) ~ O(n))</p>
<p>每个进程都有自己的“VR眼镜”这件事情还带来了更多的优化空间, 例如多个进程, 不同的虚拟地址块映射到同一个物理地址, 以及cow</p>
<p>KSM(kernel samepage merging/mermory deduplication), demand paging</p>
<p>fork: 进程快照, redis</p>
<p>cow fork的缺点: 让系统实现变复杂</p>
<p>改革: 砍掉所有的内核部分, 剩下的全部交给xv6</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="lec-23-处理器调度">lec 23 处理器调度<a class="hash-link" aria-label="Direct link to lec 23 处理器调度" title="Direct link to lec 23 处理器调度" href="/blog/page/2#lec-23-处理器调度">​</a></h3>
<p>trampoline code</p>
<p>跳板代码, 例子</p>
<ul>
<li>call printf -&gt; call *GOT(printf)</li>
<li>JIT编译器</li>
<li>软件热更新(patch 函数头)</li>
</ul>
<p>资源调度(分配)是一个非常复杂的问题</p>
<p>建模, 预测, 决策 <code>-&gt;</code> 调度策略的设计空间</p>
<p>调度策略</p>
<p>再加一层机制 &quot;niceness&quot;, 管理员控制nice, 越nice越能得到cpu</p>
<p>10 nice ~ 10倍性能差异</p>
<p>taskset 绑定一个process到一个cpu上</p>
<p>round-robin时代: MLFQ, 动态优先级</p>
<ul>
<li>
<p>让出CPU(I/O) -&gt; “好”</p>
</li>
<li>
<p>用完时间片 -&gt; “坏”!</p>
</li>
</ul>
<p>1960s: breakthrough!</p>
<p>2020s: 对很多负载都欠考虑</p>
<p>今天的调度: CFS(complete fair scheduling)</p>
<p>但有vruntime, &quot;好人&quot;的钟快一些</p>
<p>真实的处理器调度: 不要高兴得太早...</p>
<ul>
<li><strong>低优先级的在持有mutex的时候被中间优先级的赶下处理器</strong>, 可以<strong>导致高优先级的任务等待mutex退化到低优先级</strong> <code>-&gt;</code> 火星车</li>
</ul>
<p><strong>Linux: 没法解决, CFS凑合用</strong></p>
<p>实时系统: 火星车在CPU Reset, 不能摆烂</p>
<ul>
<li>
<p>优先级继承, 条件变量唤醒?</p>
</li>
<li>
<p>lockdep预警</p>
</li>
<li>
<p>...</p>
</li>
</ul>
<p><strong>然而不止有锁, 还有多处理器...</strong></p>
<p><strong>今天的计算机系统: SMP</strong></p>
<p><strong>多处理器的矛盾困境</strong></p>
<ul>
<li><strong>绑定一个线程:&quot;一核有难, 八方围观&quot;</strong></li>
<li><strong>谁空丢给谁: cache, TLB白干</strong></li>
</ul>
<p><strong>更多的实际情况: NUMA, 异构, 多用户</strong></p>
<ul>
<li>
<p><strong>numa: 远近cpu性能差达到数倍</strong></p>
</li>
<li>
<p><strong>多用户的cpu共享? namespaces, cgroups, 例如一个程序开并行, 另一个程序是串行的, 是否需要给串行的保留一个核, 而不是开得越多抢得越多</strong></p>
</li>
<li>
<p><strong>异构, 大小核超小核, GPUNPU, 每个核的独有缓存和共享缓存...</strong></p>
</li>
<li>
<p><strong>更少的处理  器可能更快...(反直觉, 同步cacheline带来的开销)</strong></p>
</li>
</ul>
<p>复杂的系统无人掌控</p>
<p>ghOSt: Fast &amp; Flexible User-Space Delegation of Linux</p>
<p>开始下放给应用程序做调度</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="others">Others<a class="hash-link" aria-label="Direct link to Others" title="Direct link to Others" href="/blog/page/2#others">​</a></h3>
<p>早期优雅的设计可能会成为后续发展的包袱: fork+exec带来的膨胀, <strong>所有涉及到OS内部状态的api都需要考虑fork行为</strong>, 例如文件偏移量...</p>
<p>总线, 中断控制器, DMA</p>
<p>总线: 提供设备的“虚拟化”, 注册和转发, 把收到的地址(总线地址)和数据转发到对应的设备上</p>
<p><strong>这样cpu只需要直连一根总线就行了!</strong></p>
<p>PCI总线</p>
<ul>
<li>总线可以桥接其他总线, 例如<code>pci -&gt; usb</code></li>
</ul>
<p><code>lspci -tv</code>可视化</p>
<p>&quot;即插即用&quot;的实现——非常复杂!</p>
<p>cpu: 只有一根中断线</p>
<p>启动多个cpu: cpu给其他cpu发中断!</p>
<p>中断仲裁: 收集各个设备中断, 选一个发给cpu</p>
<p>APIC(Advanced PIC):</p>
<ul>
<li>local APIC: 中断向量表, IPI, 时钟, ...</li>
<li>IO APIC: IO设备</li>
</ul>
<p>DMA: 很早期就有了, 解放cpu, 设计专用的电路只做memcpy</p>
<p>今天: PCI总线直接支持</p>
<p><strong>文件 = 实现了文件操作的“Anything”</strong></p>
<p><strong>设备驱动程序: 一个 struct file_operations的实现, 就是一段普通的内核, “翻译”read/write等系统调用</strong></p>
<p><code>/dev/null</code>的驱动: read永远什么都不做返回0, write永远什么都不做返回count</p>
<p>一种&quot;duck type&quot;</p>
<p>设备不仅仅是数据, 还有配置</p>
<p>配置设备:</p>
<ul>
<li>控制作为数据流的一部分(自定义一套write的指令编码)</li>
<li>提供一个新的接口</li>
</ul>
<p>ioctl: 非数据的设备功能几乎完全依赖ioctl, 完全由驱动决定</p>
<p>数量最庞大,质量最低的shit</p>
<p>unix的负担: 复杂的hidden spec</p>
<p>/dev/kvm 硬件虚拟化, 支撑了几乎所有的云产商虚拟化方案</p>
<p>unix的设计: 目录树的拼接</p>
<p>将一棵目录树拼到另一棵上</p>
<p>回想最小linux系统, 只有/dev/console和几个文件</p>
<p>/proc, /sys, /tmp都是mount系统调用创建的</p>
<p>&quot;看到的fs!=磁盘的fs&quot;, is just a <strong>view</strong></p>
<p>像是procfs这种并非实际的fs更是, 可以挂载到任意的地方, 以任意的数量(因为他只是fake了read/write的“file Object”)</p>
<p>根本设计哲学: 灵活</p>
<p>灵活性带来的</p>
<ul>
<li><code>/</code>, <code>/home</code>, <code>/var</code>都可以是独立的设备, 把有些快的放在一个目录存可执行文件, 另一些存数据...</li>
</ul>
<p>mount一个文件? loopback device</p>
<p>设备驱动把设备的read/write翻译成文件的rw</p>
<p>FHS: Filesystem Hierarchy Standard</p>
<p>ln -s 图结构 as 状态机</p>
<p>fs: 一个”数据结构题“, 但读写的单元是一个block</p>
<p>FAT: 集中保存所有&quot;next&quot;指针, 可靠性? 存n份!</p>
<p>fat manual</p>
<p>fat 小文件ok, 大文件不行</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/os">os</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/system">system</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/virtualize">virtualize</a></li></ul></div></footer></article><nav class="pagination-nav" aria-label="Blog list page navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/blog"><div class="pagination-nav__label">Newer Entries</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/blog/page/3"><div class="pagination-nav__label">Older Entries</div></a></nav></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">notes</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/RL">课程笔记</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a class="footer__link-item" href="/personal-essays">Personal Essays</a></li><li class="footer__item"><a href="https://github.com/facebook/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Ayanami, Inc. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>