<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-tags-post-list-page plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.4.0">
<title data-rh="true">One post tagged with &quot;code&quot; | Ayanami&#x27;s Cave</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://ayanami1314.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://ayanami1314.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://ayanami1314.github.io/blog/tags/code"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" property="og:title" content="One post tagged with &quot;code&quot; | Ayanami&#x27;s Cave"><meta data-rh="true" name="docusaurus_tag" content="blog_tags_posts"><meta data-rh="true" name="docsearch:docusaurus_tag" content="blog_tags_posts"><link data-rh="true" rel="icon" href="/img/ayanami.jpg"><link data-rh="true" rel="canonical" href="https://ayanami1314.github.io/blog/tags/code"><link data-rh="true" rel="alternate" href="https://ayanami1314.github.io/blog/tags/code" hreflang="en"><link data-rh="true" rel="alternate" href="https://ayanami1314.github.io/blog/tags/code" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Ayanami&#39;s Cave RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Ayanami&#39;s Cave Atom Feed">



<link rel="alternate" type="application/rss+xml" href="/personal-essays/rss.xml" title="Ayanami&#39;s Cave RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/personal-essays/atom.xml" title="Ayanami&#39;s Cave Atom Feed">


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.00dd3480.css">
<script src="/assets/js/runtime~main.5a5b8d46.js" defer="defer"></script>
<script src="/assets/js/main.cc42b366.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/logo.svg" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Ayanami&#x27;s Cave</b></a><a class="navbar__item navbar__link" href="/docs/Chcore源码阅读">课程笔记</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">技术博客</a><a class="navbar__item navbar__link" href="/personal-essays">个人随笔</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/facebook/docusaurus" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><div class="navbar__search searchBarContainer_NW3z" dir="ltr"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">All our posts</div><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/code-search&amp;code-embedding">从现代Coding Agent视角回看代码搜索与嵌入</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/AI limu">李沐dl笔记</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/llm for code paper notes">paper-reading, code&amp;rl方向</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/speculative-decode-overview">投机解码简述</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/Paper reading Context Pruning and beyond hard pruning">Paper reading - Context Pruning and beyond hard pruning</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/结构化输出">结构化输出与AI工具与Agent</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/context-engineering">context-engineering</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/从微调reranker到搜推">从微调reranker到搜推工程实践</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/llm-tech-report">部分llm技术报告的阅读</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/Ask in Any Modality A Comprehensive Survey on Multimodal Retrieval-Augmented Generation">Paper reading-Ask in Any Modality A Comprehensive Survey on Multimodal Retrieval-Augmented Generation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/精读 AAAI 2025 Fit and Prune Fast and Training-free Visual Token Pruning for Multi-modal Large Language Models">Paper reading - Fit and Prune Fast and Training-free Visual Token Pruning for Multi-modal Large Language Models</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/精读：Eagle Exploring The Design Space for Multi- modal LLMs with Mixture of Encoders">Paper reading-Eagle Exploring The Design Space for Multi- modal LLMs with Mixture of Encoders</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/RAG的一些思考和细节">RAG的一些思考与细节</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/精读  Interleaved Scene Graph for Interleaved Text-and-Image Generation Assessment">Paper reading - Interleaved Scene Graph for Interleaved Text-and-Image Generation Assessment</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/ColBERT">ColBERT-后期交互方法</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/2025/05/26/技术博客阅读">美团技术博客阅读</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/Milvus">稀疏神经嵌入</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/RocketMQ">RocketMQ学习</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/cs186-database-WIP">ucb cs186 课程笔记(更新中)</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/jyy-os-虚拟化">NJU操作系统(jyy OS)课程笔记-虚拟化部分</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/local-llm">来本地部署大模型!</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/ostep-chapter42-44">ostep阅读笔记：单机fs的崩溃一致性(chapter42-44)</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/系统架构设计笔记">system-design-interview笔记</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/JUC">JUC</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/cs144 labs">cs144 labs(Winter 2024)</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/jyy-os：并发">NJU操作系统(jyy OS)课程笔记-并发部分</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/nginx">nginx基础</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/cs144/cs144 lec notes">CS144 Lecture Notes</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/django-mosh">Django_mosh</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/splay-tree">splay tree</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/xv6book-notes">xv6book Notes(C1-4)</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/Go-Gin学习">Go,Gin学习</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/godis源码阅读">godis源码阅读</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/hibernate-jpa">hibernate&amp;jpa</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/linking-复习">linking 复习</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/ts基础">ts基础</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/实战2-mosh-gamehub">react practice:mosh gamehub</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/浅入理解断点和调试器">浅入理解断点和调试器</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/黑马点评">黑马点评(速通版)</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/js基础">js基础</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/11-14-11-26学习双周记">11-14-11-26学习双周记</a></li></ul></nav></aside><main class="col col--7"><header class="margin-bottom--xl"><h1>One post tagged with &quot;code&quot;</h1><a href="/blog/tags">View All Tags</a></header><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/blog/llm for code paper notes">paper-reading, code&amp;rl方向</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-09-01T00:00:00.000Z">September 1, 2025</time> · <!-- -->27 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><div class="avatar__intro"><div class="avatar__name"><span>ayanami</span></div></div></div></div></div></header><div class="markdown"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="effi-code-unleashing-code-efficiency-in-language-modelsswiftcoder-enhancing-code-generation-in-large-language-models-through-efficiency-aware-fine-tuning">Effi-code: Unleashing code efficiency in language modelsSWIFTCODER: Enhancing Code Generation in Large Language Models through Efficiency-Aware Fine-tuning<a class="hash-link" aria-label="Direct link to Effi-code: Unleashing code efficiency in language modelsSWIFTCODER: Enhancing Code Generation in Large Language Models through Efficiency-Aware Fine-tuning" title="Direct link to Effi-code: Unleashing code efficiency in language modelsSWIFTCODER: Enhancing Code Generation in Large Language Models through Efficiency-Aware Fine-tuning" href="/blog/tags/code#effi-code-unleashing-code-efficiency-in-language-modelsswiftcoder-enhancing-code-generation-in-large-language-models-through-efficiency-aware-fine-tuning">​</a></h2>
<p>问题：以前的方法主要关注正确性忽略效率(effibench，gpt4 代码执行时间是标准解决方案的1.69与45.49倍(avg, worst))</p>
<p>衡量效率：本地测量执行时间和内存</p>
<p>具体来说是三个指标：</p>
<ul>
<li>执行时间ET</li>
<li>最大内存使用量MU</li>
<li>总内存使用量TMU</li>
</ul>
<p>这篇论文并没有用RL的方法去激发LLM生成更高效率代码的能力，而是”优化“训练数据集的代码效率，并证明了训练集代码效率高也会让LLM生成更高效率的代码（ET 的相关性为 0.972，MU 的相关性为 0.950，TMU 的相关性为 0.986）</p>
<p><img decoding="async" loading="lazy" src="https://s2.loli.net/2025/08/19/8ODW2a1x7r9bFUC.png" alt="Refer to caption" class="img_ev3q"></p>
<p>效果：<code>qwen2.5-coder-7b-instruct pass@1 44.8 -&gt; 57.7</code>，正确任务的执行时间减少48.4%</p>
<p>方法：构建代码生成数据集，进行微调</p>
<p><img decoding="async" loading="lazy" src="https://s2.loli.net/2025/08/19/ecP8JdFNAkaCwVr.png" alt="Refer to caption" class="img_ev3q"></p>
<p>具体来说，先拉下来开源数据集，过滤一遍后，直接让更强的LLM生成更好的解决方案，然后本地跑一遍得到效率</p>
<p>不同效率的代码示例 <img decoding="async" loading="lazy" src="https://s2.loli.net/2025/08/19/njUJ3lsdbSM2v9e.png" alt="Refer to caption" class="img_ev3q"></p>
<p>文章认为的效果提升来源：</p>
<ol>
<li>多语言数据集</li>
<li>数据准备阶段过滤充分</li>
<li>数据量（有超过 70,000 个训练示例，比先前的mercury 1.8k要大很多）</li>
</ol>
<p>简评：大体上感觉是工作量密集型的工作，比如finetune多个llm和在多个数据集上进行相关评测，但方法上并没有创新之感，洗的数据是高效率代码的自然输出的结果效率也会变高，并不令人感觉新奇，只是讲好了我们需要同时看重代码质量（这里是效率）这一指标的故事</p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="effibench-benchmarking-the-efficiency-of-automatically-generated-code">EffiBench: Benchmarking the Efficiency of Automatically Generated Code<a class="hash-link" aria-label="Direct link to EffiBench: Benchmarking the Efficiency of Automatically Generated Code" title="Direct link to EffiBench: Benchmarking the Efficiency of Automatically Generated Code" href="/blog/tags/code#effibench-benchmarking-the-efficiency-of-automatically-generated-code">​</a></h2>
<p>问题：现在衡量代码正确性的文章已经有很多了，但是兼顾正确和效率的相对少</p>
<p>如果要考虑效率的话，一个问题是原先的代码数据集的任务太简单了，很难区分效率；同时很多任务也不是效率密集型的，并且效率相关的测试也不够</p>
<p>数据集构建：leetcode</p>
<p>“标准解决方案”: stackoverflow最多star/leetcode top answer</p>
<p>测试用例：LLM生成</p>
<p>简评：典型的benchmark工作</p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="diversity-aware-policy-optimization-for-large-language-model-reasoning">Diversity-Aware Policy Optimization for Large Language Model Reasoning<a class="hash-link" aria-label="Direct link to Diversity-Aware Policy Optimization for Large Language Model Reasoning" title="Direct link to Diversity-Aware Policy Optimization for Large Language Model Reasoning" href="/blog/tags/code#diversity-aware-policy-optimization-for-large-language-model-reasoning">​</a></h2>
<p>问题：diversity在reasoning能力中扮演重要角色，但缺乏定量的研究</p>
<p>动机：传统RL认为，多样性有助于策略探索（例如，SAC等算法），帮助跳出局部最优、加速训练收敛，但对于LLM呢？</p>
<p>方法：</p>
<p>直接增加熵，长度bias, 较长的响应 <code>-&gt;</code> 引入token level diversity</p>
<p>tradeoff 质量和多样性 <code>-&gt;</code> 仅对正样本用多样性增强，确保以性能标准为主导</p>
<p>贡献：</p>
<ul>
<li>多样性的Potential@k指标和LLM reasoning存在正相关关系</li>
<li>token-level diversity objective, selectively applied to positive samples</li>
</ul>
<p>奖励：和R1一致，acc reward和format reward，前者和ground truth 比较，后者让答案以 <code>\boxed{}</code>格式呈现</p>
<p>多样性度量：response中不同方程的比例</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi><mi>i</mi><mi>v</mi><mi>E</mi><mi>q</mi><mi>u</mi><mo>:</mo><mo>=</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><msub><mo>∑</mo><mi>k</mi></msub><mfrac><mi>U</mi><mi>A</mi></mfrac></mrow><annotation encoding="application/x-tex">DivEqu :=  \frac{1}{N} \sum_k \frac{U}{A}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.02778em">D</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="mord mathnormal" style="margin-right:0.03588em">Eq</span><span class="mord mathnormal">u</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">:=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.2173em;vertical-align:-0.345em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8451em"><span style="top:-2.655em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">N</span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1864em"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2997em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8723em"><span style="top:-2.655em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">A</span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">U</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></p>
<p>其中，U是k个采样中的独立方程数量，A是总方程数量</p>
<p>Potential@k： 衡量模型在第一次失败后，在k次（k=16 in paper）内纠正答案的能力</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mi>o</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>i</mi><mi>a</mi><mi>l</mi><mi mathvariant="normal">@</mi><mi>k</mi><mo>:</mo><mo>=</mo><mfrac><mrow><mo>∑</mo><mi>P</mi><mi>a</mi><mi>s</mi><mi>s</mi><mi mathvariant="normal">@</mi><mi>k</mi><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>P</mi><mi>a</mi><mi>s</mi><mi>s</mi><mi mathvariant="normal">@</mi><mn>1</mn><mo stretchy="false">)</mo></mrow><mrow><mo>∑</mo><mn>1</mn><mo>−</mo><mi>P</mi><mi>a</mi><mi>s</mi><mi>s</mi><mi mathvariant="normal">@</mi><mn>1</mn></mrow></mfrac></mrow><annotation encoding="application/x-tex">Potential@k := \frac{\sum Pass@k (1 - Pass@1)}{ \sum 1 - Pass@1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mord mathnormal">o</span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mord mathnormal">ia</span><span class="mord mathnormal" style="margin-right:0.01968em">l</span><span class="mord">@</span><span class="mord mathnormal" style="margin-right:0.03148em">k</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">:=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.53em;vertical-align:-0.52em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.01em"><span style="top:-2.655em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mop op-symbol small-op mtight" style="position:relative;top:0em">∑</span><span class="mspace mtight" style="margin-right:0.1952em"></span><span class="mord mtight">1</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight" style="margin-right:0.13889em">P</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight">ss</span><span class="mord mtight">@1</span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.485em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mop op-symbol small-op mtight" style="position:relative;top:0em">∑</span><span class="mspace mtight" style="margin-right:0.1952em"></span><span class="mord mathnormal mtight" style="margin-right:0.13889em">P</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight">ss</span><span class="mord mtight">@</span><span class="mord mathnormal mtight" style="margin-right:0.03148em">k</span><span class="mopen mtight">(</span><span class="mord mtight">1</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight" style="margin-right:0.13889em">P</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight">ss</span><span class="mord mtight">@1</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.52em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></p>
<p>（为啥定义这样一个指标？这个分子分母分别求和挺怪异的，也不是类似条件概率的算法）</p>
<p>结果：对于推理能力有限的LLM(<code>Pass@1&lt;0.4</code>) 多样性和Potential@k没什么关系，但对于更好的LLM，就有明显的正相关关系</p>
<p><img decoding="async" loading="lazy" src="https://s2.loli.net/2025/08/19/KN2ZTl3WHbAfotv.png" alt="Refer to caption" class="img_ev3q"></p>
<p>定义的token-level熵</p>
<p><img decoding="async" loading="lazy" src="https://s2.loli.net/2025/08/19/9zOCUMnTHIdcsA5.png" alt="image-20250817161107663" class="img_ev3q"></p>
<p>在实测中，作者发现直接把这个熵带入训练会增强错误样本的多样性，相当于对错误样本做增强，因此打了只对正样本做的补丁</p>
<p><img decoding="async" loading="lazy" src="https://s2.loli.net/2025/08/19/EiW5Io6xJjOgLyp.png" alt="image-20250817161240377" class="img_ev3q"></p>
<p>而对这个式子求导能直观感受到多样性的部分</p>
<p><img decoding="async" loading="lazy" src="https://s2.loli.net/2025/08/19/B4GSM6VPi3c2Cml.png" alt="image-20250817161427155" class="img_ev3q"></p>
<p>对于大多数token, 采样概率<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi></mrow><annotation encoding="application/x-tex">\pi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">π</span></span></span></span>的值都是小于<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>e</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">e^{-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></span>的，则前一个乘项小于0 ，熵的梯度和采样概率的梯度成正相关，熵增有利于对稀有Token的采样</p>
<p><img decoding="async" loading="lazy" src="https://s2.loli.net/2025/08/19/tuR3Ey7BxJ4sFfi.png" alt="image-20250817161718666" class="img_ev3q"></p>
<p>实际取 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi><mo>=</mo><mn>0.01</mn></mrow><annotation encoding="application/x-tex">\lambda=0.01</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">λ</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">0.01</span></span></span></span></p>
<p>简评：</p>
<p>动机非常清晰，实验也比较充分，展示了虽然是 well-known 的需要基模能力达标多样性才有意义的结论。</p>
<p>对于LLM优化目标的改造的说明是合理的。理论说法是GRPO带来的更新依赖于组内样本的 差异，（因为是用std/mean来计算优势函数A），所以增强多样性能避免优势消失带来的一些问题，本质上是在避免 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi>r</mi><mo>−</mo><mi>m</mi><mi>e</mi><mi>a</mi><mi>n</mi></mrow><mrow><mi>s</mi><mi>t</mi><mi>d</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{r - mean}{std}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1473em;vertical-align:-0.345em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8023em"><span style="top:-2.655em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">d</span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">r</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">an</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span> 里面的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mi>t</mi><mi>d</mi></mrow><annotation encoding="application/x-tex">std</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">t</span><span class="mord mathnormal">d</span></span></span></span> 过小导致不稳定的问题</p>
<p>只对正样本应用多样性损失的trick也是有意义的。</p>
<p>但衡量多样性的时候比较草率，首先是局限在数学范围，其次感觉 <code>方程多样性 != 解法多样性</code>，所以多样性指标总感觉欠说服力。</p>
<p>他们自己也在文章中说：&quot;许多现实世界的应用需要用户意图的多样性（例如，需要数学问题的代数和算术解，或者生成具有不同算法方法的代码）, 这样的多样性不等于token level的多样性&quot;</p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="beyond-the-8020-rule-high-entropy-minority-tokens-drive-effective-reinforcement-learning-for-llm-reasoning">Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning<a class="hash-link" aria-label="Direct link to Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning" title="Direct link to Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning" href="/blog/tags/code#beyond-the-8020-rule-high-entropy-minority-tokens-drive-effective-reinforcement-learning-for-llm-reasoning">​</a></h2>
<p><img decoding="async" loading="lazy" src="https://s2.loli.net/2025/08/19/EXbKjUpG9tBPO4R.png" alt="Refer to caption" class="img_ev3q"></p>
<p>问题： 现有RL for LLM算法对不同的token一视同仁，没有考虑token自身的异构性</p>
<p>观察： 低熵token主要决定语言结构，高熵token则作为关键的决策点。手动调整forking token的熵，适度增加这些token的熵可以显著提升推理性 能，降低熵会导致性能下降。</p>
<p>仅保留20% token的策略梯度更新，剩下的mask掉，性能上能和全量媲美甚至超越，在RL过程中，只有一小部分高熵 token 对探索有实际贡献，而其他 token 则可能中性甚至有害</p>
<p>20%是实验得到的最优比例</p>
<p>另一个发现是32B的性能提升大于14B大于8B</p>
<p>熵定义：</p>
<p><img decoding="async" loading="lazy" src="https://s2.loli.net/2025/08/19/dvBLzo681VuCRsa.png" alt="image-20250817165117549" class="img_ev3q"></p>
<p>一个很直观的分布图，高熵Token基本是重要的转折词，而低熵token是一些前后缀等</p>
<p><img decoding="async" loading="lazy" src="https://ar5iv.labs.arxiv.org/html/2506.01939/assets/x2.png" alt="Refer to caption" class="img_ev3q"></p>
<p>另一个稍微不直观的图</p>
<p>可以得到的结论是高熵token配合高温度能够得到好性能，而低熵token在不同温度下都不太影响最终效果</p>
<p><img decoding="async" loading="lazy" src="https://s2.loli.net/2025/08/19/xJQaW8wsCp6OT2H.png" alt="Refer to caption" class="img_ev3q"></p>
<p>作者还发现，RLVR的过程基本就是这些高熵token的熵改变的过程，低熵Token的熵相对稳定，初始熵较高的 token 在 RLVR 后往往会经历更大的熵增。</p>
<p>而只对高熵Token进行RLVR能得到更好的效果</p>
<p><img decoding="async" loading="lazy" src="https://s2.loli.net/2025/08/19/QsO7VlpMdqhmPKb.png" alt="Refer to caption" class="img_ev3q"></p>
<p>甚至作者在OOD数据（代码数据）上进行评测，发现高熵的token选择后，泛化性也更好</p>
<p><img decoding="async" loading="lazy" src="https://s2.loli.net/2025/08/19/8JzUWnoklK5xdS1.png" alt="Refer to caption" class="img_ev3q"></p>
<p>讨论：</p>
<ol>
<li>RL倾向于保持高熵令牌的熵，而SFT倾向于将输出推向one-hot，降低熵，作者表示这可能是RL更能泛化而SFT容易记忆、难以泛化的原因</li>
<li>传统RL假设一整条轨迹上的熵是接近均匀分布的，这对LLM不成立</li>
<li>LLM RL中，之前常用的熵损失鼓励探索可能未必适用，因为可以是低熵token的熵增也可能是高熵token的，而DAPO的clip higher能筛选出高熵token，即重要性比率ratio（<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi><mi mathvariant="normal">/</mi><msub><mi>π</mi><mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\pi/\pi_{old}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.03588em">π</span><span class="mord">/</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.01968em">l</span><span class="mord mathnormal mtight">d</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>）更高的token通常对应高熵token，这呼应了前文中，RLVF对高熵token的熵值有较大改变</li>
</ol>
<p>简评：开始的手动把熵调高感觉等价于把奖励调高等价于数据增强，作为一种RL trick细想并不惊奇，是稀有样本情况下的常用技术</p>
<p>从这个角度继续往下想，如何理解这个多数token对训练甚至有害呢，感觉也是能合上现有对LLM RL的state定义不太合理，或者说探索空间太大  ，采样样本太少，不可能得到正确的Q函数，导致对于一些本来就很无所谓的token，更新的方向也是比较盲目</p>
<p>总之，这篇文章实验非常充足，分析也比较到位，效果也十分亮眼，确实是在当前的SOTA上往前推进的好文章</p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="passk-training-for-adaptively-balancing-exploration-and-exploitation-of-large-reasoning-models">Pass@k Training for Adaptively Balancing Exploration and Exploitation of Large Reasoning Models<a class="hash-link" aria-label="Direct link to Pass@k Training for Adaptively Balancing Exploration and Exploitation of Large Reasoning Models" title="Direct link to Pass@k Training for Adaptively Balancing Exploration and Exploitation of Large Reasoning Models" href="/blog/tags/code#passk-training-for-adaptively-balancing-exploration-and-exploitation-of-large-reasoning-models">​</a></h2>
<p>问题：RLVR 通常采用 Pass@1作为奖励，但容易收敛到局部最优；Pass@k则常用于验证，本文用Pass@k直接作为训练，并设计了对应的优势函数，发现效果比Pass@1更好（更高的Pass@k分数，保持的Pass@1分数）</p>
<p><img decoding="async" loading="lazy" src="https://s2.loli.net/2025/08/19/U3Ay7amXxpnqcub.png" alt="Refer to caption" class="img_ev3q"></p>
<p>Pass@1 收敛到局部最优的问题在于正向奖励的探索可能路径太长，模型会倾向于利用而不是探索</p>
<p><img decoding="async" loading="lazy" src="https://s2.loli.net/2025/08/19/i85Q2WRxPvmazh9.png" alt="Refer to caption" class="img_ev3q"></p>
<p>方法：</p>
<ul>
<li>
<p>Full Sampling: 每组采样的k个rollout计算奖励，之后整组的奖励由每个rollout的<strong>最大值</strong>给出<img decoding="async" loading="lazy" src="https://s2.loli.net/2025/08/19/asnfy7dcjm4UeK3.jpg" alt="Refer to caption" class="img_ev3q"></p>
</li>
<li>
<p>Bootstrap Sampling: Full Sampling虽然提高了性能，但计算量太大。为了减少推理次数，同时  保持组数不变，采用bootstrap采样，先生成一个候选池，再从这个池子里面抽取k个答案，就形成了一组（这样允许某个答案被分到多个组里面重用），论文中候选池大小就和正常top1大小相同，也就是平均而言每个样本被重用k次</p>
</li>
</ul>
<p><img decoding="async" loading="lazy" src="https://s2.loli.net/2025/08/19/mLOqDY2PvnCokEN.jpg" alt="Refer to caption" class="img_ev3q"></p>
<ul>
<li>既然Bootstrap Sampling只不过是对样本的采样重用，那其实可以直接计算对应的优势值的期望，所以可以省去采样这一步，直接计算候选池中正负样本的个数，通过解析解得到期望带入计算</li>
</ul>
<p>其他实验：</p>
<p>Pass@k的熵在训练中是上升的，而Pass@1后期会收敛，支持了前面的探索-利用论</p>
<p>k的值的影响？k的值不是跳出局部解的重要因素，但k值越大，优势越小（因为只有抽样全负才会是负奖励，k值越大正奖励概率越高），步长越小，训练效率降低，这个结论和也可以在改变学习率中得到验证</p>
<p>无论是小规模还是大规模的 LLM，都可以从 Pass@k 训练中受益。此外，模型架构和模型系列不会影响持续 Pass@1 训练的提升，下游任务的领域和形式也不会影响 LLM Pass@k 性能向 Pass@1 性能的迁移</p>
<p>分析：</p>
<p>同样是奖励从0到1，Pass@k的梯度出现在比Pass@1更早的地方（一次做对和K次做对），会使得Pass@k更倾向于解决更难的问题而不是中等难度的问题（由于Pass@k有一个argmax, 所以提高已经会做的题的正确率的效果是不断减小的）</p>
<p>Pass@1</p>
<p><img decoding="async" loading="lazy" src="https://s2.loli.net/2025/08/19/kwHnEW4mdQfrZqc.jpg" alt="Refer to caption" class="img_ev3q"></p>
<p>Pass@k</p>
<p><img decoding="async" loading="lazy" src="https://s2.loli.net/2025/08/19/d1Rg98LKeMtoNhm.jpg" alt="Refer to caption" class="img_ev3q"></p>
<p>还做了一个对比试验是仅将简单问题的奖励设置为0，不能防止模型过度优化</p>
<p><img decoding="async" loading="lazy" src="https://s2.loli.net/2025/08/19/Urpd158SLRyWvaM.jpg" alt="Refer to caption" class="img_ev3q"></p>
<p>为了分析是否全是梯度曲线峰值带来的影响，手动调整奖励曲线，设计了一个这样的曲线</p>
<p><img decoding="async" loading="lazy" src="https://s2.loli.net/2025/08/19/pWVjfl1t8JSgyo6.jpg" alt="Refer to caption" class="img_ev3q"></p>
<p>发现太注重困难的样本也不好，模型后期乏力</p>
<p>既然Pass@k 更注重困难样本，Pass@1 更注重一般样本，能否动态结合？</p>
<p>一个样本池中，正样本越多，越需要注重困难样本；反之需要先学会一般样本，因此设计了这样的优势函数</p>
<p><img decoding="async" loading="lazy" src="https://s2.loli.net/2025/08/19/gwcpM1DWYLRAonm.png" alt="image-20250818003914103" class="img_ev3q"></p>
<p>发现效果非常好</p>
<p><img decoding="async" loading="lazy" src="https://s2.loli.net/2025/08/19/rLXHtQpnTy2Phqc.jpg" alt="Refer to caption" class="img_ev3q"></p>
<p>文章还做了另一个实验是，用熵而不是正样本数量来判断一个问题是否是困难的，熵高的50%认为是困难问题，使用Pass@1, 低的使用Pass@k，也得到了不错的效果</p>
<p>简评：感觉没太多好说的了，他们做的相当好，从最开始的发现topk training可以提升效果，再到用bootstrap sample提高效率，再到公式的推出，自然发现topk就是本质上对应的难度-奖励曲线的不同，再到设计相关的实验验证，最后提出简单的自适应机制来结合Pass@1和Pass@k，也取得了相当好的实验效果，感觉挺一气呵成的，感觉是一个会成为范式的trick</p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="structure-aware-fill-in-the-middle-pretraining-for-code">Structure-Aware Fill-in-the-Middle Pretraining for Code<a class="hash-link" aria-label="Direct link to Structure-Aware Fill-in-the-Middle Pretraining for Code" title="Direct link to Structure-Aware Fill-in-the-Middle Pretraining for Code" href="/blog/tags/code#structure-aware-fill-in-the-middle-pretraining-for-code">​</a></h2>
<p>问题：现有的FIM将代码视为字符序列，而忽视句法结构</p>
<p><img decoding="async" loading="lazy" src="https://s2.loli.net/2025/08/19/qrSZeCaH1uMOfsI.png" alt="Refer to caption" class="img_ev3q"></p>
<p>方法： 结合AST和FIM， 在训练时，被mask的部分始终是AST的一个或多个完整子树</p>
<p>代码解析：Tree-sitter</p>
<p>mask算法：涵盖不同的AST节点，提高泛化能力，且与具体语言无关</p>
<ul>
<li>单节点mask: 按照对应文本的数量成比例抽样</li>
<li>多节点mask: 先进行一次字符区间的采样，再找到包含这个字符区间的最小3节点的AST子树，子树中再取和原始字符区间有最大交并比的部分</li>
</ul>
<p>评估：字符级别困惑度，文章给出不用实际benchmark的原因是大规模单测太难。</p>
<p>简评：非常直接的想法，就类似word-level BERT对原始BERT的改进，不过它怎么构建AST的倒是可以参考。文章最后的评估用困惑度说服力不高，但考虑到它的数据量确实大（256*H100训练），也可以理解</p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-entropy-mechanism-of-reinforcement-learning-for-reasoning-language-models">The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models<a class="hash-link" aria-label="Direct link to The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models" title="Direct link to The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models" href="/blog/tags/code#the-entropy-mechanism-of-reinforcement-learning-for-reasoning-language-models">​</a></h2>
<p>问题：现有RL后训练存在策略熵减小导致模型快速收敛，后期探索较少难以提升的问题</p>
<p><img decoding="async" loading="lazy" src="https://s2.loli.net/2025/08/19/G7D8E1Tij9VUvor.png" alt="Refer to caption" class="img_ev3q"></p>
<p>作者发现，在前1/3的epoch中，基本就已经达到了大部分的性能，而熵也进入低值，作者称之为熵崩溃&quot;entropy collapse&quot;</p>
<p>且对于不同的模型大小，对于不同的RL方法，都能拟合近似的定律  <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mo>=</mo><mo>−</mo><mi>a</mi><msup><mi>e</mi><mi>H</mi></msup><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">R=-a e^{H} + b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.00773em">R</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.9247em;vertical-align:-0.0833em"></span><span class="mord">−</span><span class="mord mathnormal">a</span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.08125em">H</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">b</span></span></span></span></p>
<p>有了这样的拟合公式，可以在训练早期估计后期的性能，且ab与算法几乎无关，极限就是 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mi>a</mi><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">-a+b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em"></span><span class="mord">−</span><span class="mord mathnormal">a</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">b</span></span></span></span></p>
<p><img decoding="async" loading="lazy" src="https://s2.loli.net/2025/08/19/U8MvB1IJD5izsQf.png" alt="Refer to caption" class="img_ev3q"></p>
<p>另一个有趣的发现是，ab和模型大小呈对数线性关系</p>
<p><img decoding="async" loading="lazy" src="https://s2.loli.net/2025/08/19/Vqo3QmfCA54is7x.png" alt="Refer to caption" class="img_ev3q"></p>
<p>也就是说，不仅可以在训练前期拟合后期，还可以用小模型预测大模型的RL效果</p>
<p><img decoding="async" loading="lazy" src="https://s2.loli.net/2025/08/19/DnwJNZRAXuvSlEO.png" alt="image-20250818222517778" class="img_ev3q"></p>
<p>熵变公式如上，直观地讲，如果动作 a 同时获得高/低概率和高/低优势，则熵会降低，反之亦然</p>
<p><img decoding="async" loading="lazy" src="https://s2.loli.net/2025/08/19/DRXsvrGAMuIBFOU.png" alt="Refer to caption" class="img_ev3q"></p>
<p>作者还提出，直接使用熵损失的方法，如<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo>=</mo><msub><mi>L</mi><mn>0</mn></msub><mo>−</mo><mi>α</mi><mi>H</mi></mrow><annotation encoding="application/x-tex">L = L_0 - \alpha H</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">L</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span><span class="mord mathnormal" style="margin-right:0.08125em">H</span></span></span></span>， 不仅对超参数敏感，实验效果也并不优于基线</p>
<p>所以作者提出的方法是，针对高协方差的一小部分token做Clip或者KL，就能防止熵崩溃，下面的实验结果也表现很好，熵后期不下降，回答长度增长，正确率大幅度提高</p>
<p><img decoding="async" loading="lazy" src="https://arxiv.org/html/2505.22617v1/x25.png" alt="Refer to caption" class="img_ev3q"></p>
<p><img decoding="async" loading="lazy" src="https://s2.loli.net/2025/08/19/4iR9K8EvMwGD1no.png" alt="Refer to caption" class="img_ev3q"></p>
<p>作者发现策略熵对超参数设置非常敏感。具体来说，我们的方法仅干预一小部分 token（ 10−4 到 10−3 ），却完全改变了熵曲线。这意味着几个“关键” token 对 LLM 的熵至关重要</p>
<p>简评：搬运作者对clip-higher的讨论，作者认为，clip-higher也有类似的功能，提高重要性采样的上限会带来更多低概率的token，上限阈值仅影响具有正优势的 token，这意味着 clip-higher 实际上在梯度计算中添加了更多低协方差（低概率、高优势）的 token，所以结论殊途同归。而作者直接提出协方差是更胜一筹。</p>
<p>不过作者也说了现在还不清楚熵和模型性能的完整关系，也不清楚最优的熵值</p>
<p>另一个就是这些熵的论文主要还是在math任务上做的，code任务能否有相同的结论还是一个问题 (我认为这两个任务关键在于中间过程是不是重要的，math只有结果可能会得到一些错误的结论)</p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="improving-llm-generated-code-quality-with-grpo">Improving LLM-Generated Code Quality with GRPO<a class="hash-link" aria-label="Direct link to Improving LLM-Generated Code Quality with GRPO" title="Direct link to Improving LLM-Generated Code Quality with GRPO" href="/blog/tags/code#improving-llm-generated-code-quality-with-grpo">​</a></h2>
<p>问题：take code quality into consideration，不多赘述</p>
<p>方法：维护了一个库，把现有的一些评估代码质量的方案整合了起来（code complexity, dead code, code structure(linter等)， style&amp;doc, safety, performance...）, 然后质量奖励和正确性奖励一起放到奖励里面丢给GRPO</p>
<p>简评：只是占坑的，很草率的方法（对于奖励参数的设定），定量结果也不足。</p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="enhancing-high-quality-code-generation-in-large-language-models-with-comparative-prefix-tuning">Enhancing High-Quality Code Generation in Large Language Models with Comparative Prefix-Tuning<a class="hash-link" aria-label="Direct link to Enhancing High-Quality Code Generation in Large Language Models with Comparative Prefix-Tuning" title="Direct link to Enhancing High-Quality Code Generation in Large Language Models with Comparative Prefix-Tuning" href="/blog/tags/code#enhancing-high-quality-code-generation-in-large-language-models-with-comparative-prefix-tuning">​</a></h2>
<p>问题：take code quality into consideration</p>
<p>方法：比较有新意，将Dynamic Prefix和代码质量结合起来了，并且是使用对比学习的方法做这个前缀</p>
<p>基于Pylint打分，构建了一套数据处理流水线标注大量高、低质量的代码对（相似度高，质量差距大，且都至少通过一项基本测试）</p>
<p>然后在微调Dynamic Prefix的的时候，加上一个排名Loss，希望模型倾向于生成高质量的样本。</p>
<p>里面的掩码是用difflib做的，目标是聚焦于差异的导致质量出现区别的token，而不要考虑重复的token</p>
<p><img decoding="async" loading="lazy" src="https://s2.loli.net/2025/08/19/trwUWyP8RMniE5H.png" alt="image-20250818231615372" class="img_ev3q"></p>
<p>然后进行PEFT微调，再加上KL散度来保证不要丢失原始模型的代码生成能力</p>
<p>简评：这篇文章写得特别冗长，实验做的重点不突出，但思想是有意思的，并且明显可以继续挖，例如他们的代码相似度是简单的词频向量，自然挖掘出来的是细微处的代码风格问题，如是用index还是for each的形式遍历循环（只有这样的才会词频上高度相似）。但实际上是否可以用例如bge-code这样的代码语义嵌入呢？值得探究。</p>
<p>还有就是，这个数据收集的方法依赖于大语料库，也只能挖掘常见的代码模式，如果用自生成的方法，例如假设我们已经有一些高质量的代码库作为ground truth，</p>
<p>用llm得到的补全片段当负项，也能构造正负样本对啊，既然都是训练得到一个通用的“code style prefix”，这样数据丰富程度能高很多。他们明显的数据少训练小（2*A6000*3h）。</p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="augmenting-large-language-models-with-static-code-analysis-for-automated-code-quality-improvements">Augmenting Large Language Models with Static Code Analysis for Automated Code Quality Improvements<a class="hash-link" aria-label="Direct link to Augmenting Large Language Models with Static Code Analysis for Automated Code Quality Improvements" title="Direct link to Augmenting Large Language Models with Static Code Analysis for Automated Code Quality Improvements" href="/blog/tags/code#augmenting-large-language-models-with-static-code-analysis-for-automated-code-quality-improvements">​</a></h2>
<p>问题：LLM refactor code没结合静态分析</p>
<p>方法：RAG + 静态分析软件 + Prompt 工程</p>
<p>简评：垃圾文章，真要做也是做一个能排序Code Quality的专用BERT，或者对现有的code embedder/reranker做adapter研究怎么把code quality调进去</p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="a-hierarchical-and-evolvable-benchmark-for-fine-grained-code-instruction-following-with-multi-turn-feedback">A Hierarchical and Evolvable Benchmark for Fine-Grained Code Instruction Following with Multi-Turn Feedback<a class="hash-link" aria-label="Direct link to A Hierarchical and Evolvable Benchmark for Fine-Grained Code Instruction Following with Multi-Turn Feedback" title="Direct link to A Hierarchical and Evolvable Benchmark for Fine-Grained Code Instruction Following with Multi-Turn Feedback" href="/blog/tags/code#a-hierarchical-and-evolvable-benchmark-for-fine-grained-code-instruction-following-with-multi-turn-feedback">​</a></h2>
<p>只需要看一张图就行了</p>
<p><img decoding="async" loading="lazy" src="https://s2.loli.net/2025/08/19/ZOrG26Leygztn5N.png" alt="image-20250818234727444" class="img_ev3q"></p>
<p>现有模型在约束生成时，quality这种抽象的约束是满足最差的</p>
<p>而对于多种约束的组合，现有LLM都很差</p>
<p>而对于有反馈的情况（例如linter之类），在3轮迭代左右就能有很大的提升，但后续再增加  轮数也难以获得更高收益</p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="training-language-models-on-synthetic-edit-sequences-improves-code-synthesis">Training Language Models on Synthetic Edit Sequences Improves Code Synthesis<a class="hash-link" aria-label="Direct link to Training Language Models on Synthetic Edit Sequences Improves Code Synthesis" title="Direct link to Training Language Models on Synthetic Edit Sequences Improves Code Synthesis" href="/blog/tags/code#training-language-models-on-synthetic-edit-sequences-improves-code-synthesis">​</a></h2>
<p>问题：LLM这样“一口气生成所有代码”和先前的软件工程实践（增量式开发）是相悖的，而现在的code agent又需要增量开发的能力，有绕远路之感，于是研究能不能从预训练的数据侧上解决这个问题，即大规模合成 增量编辑数据</p>
<p>方法: 文章提出了一个LintSeq的方法，对于一段已有的代码，从里面修建某些部分回退，让回退后的代码不会触发linter错误，则构建了一个edit stage</p>
<p><img decoding="async" loading="lazy" src="https://s2.loli.net/2025/08/19/kGrRXUvC1Sbs6Pe.png" alt="Refer to caption" class="img_ev3q"></p>
<p>然后这样构建了数据集后自己SFT codellm，发现确有提升</p>
<p>简评：简单有效，或许可以想想这个怎么和RL结合？</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="focused-dpo-enhancing-code-generation-through-focused-preference-optimization-on-error-prone-points">Focused-DPO: Enhancing Code Generation Through Focused Preference Optimization on Error-Prone Points<a class="hash-link" aria-label="Direct link to Focused-DPO: Enhancing Code Generation Through Focused Preference Optimization on Error-Prone Points" title="Direct link to Focused-DPO: Enhancing Code Generation Through Focused Preference Optimization on Error-Prone Points" href="/blog/tags/code#focused-dpo-enhancing-code-generation-through-focused-preference-optimization-on-error-prone-points">​</a></h2>
<p>问题：代码错误很多是中间的“易错点”出错，对于所有token一视同仁的奖励函数在代码任务上可能未必高效</p>
<p><img decoding="async" loading="lazy" src="https://s2.loli.net/2025/08/19/1PfxEt2h8gaKnXl.png" alt="image-20250819001722237" class="img_ev3q"></p>
<p>方法：</p>
<ol>
<li>该方法从真实代码库中提取概念，生成问题、代码和测试</li>
<li>由于有了测试，所以可以比较不同的生成代码的相对性能</li>
<li>通过共同前缀和共同后缀，得到中间不一样的中缀，就认为是“易错点”</li>
</ol>
<p>然后DPO专注这一块的优化</p>
<p><img decoding="async" loading="lazy" src="https://s2.loli.net/2025/08/19/BKAtb5MYlhkGoW3.png" alt="image-20250819002037550" class="img_ev3q"></p>
<p>简评：感觉上是更软件工程的熵方法的简化，感觉这个易错点是能从LLM自身状态或者其他软件工程分析技巧中得到的，从前面几篇也可以看出，现在这种广义上的RL ”attention“ mask类工作越来越多了</p>
<p>这个方法要求生成测试，实际生产中感觉并不可用；抛开这个不谈，感觉就单纯对一个大型代码数据集，去分析里面的编码模式，找到相对少的n-gram，或者AST level迅速变化的地方作为易错点重点训都或许可行</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/llm">llm</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/ai">ai</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/code">code</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/rl">RL</a></li></ul></div></footer></article><nav class="pagination-nav" aria-label="Blog list page navigation"></nav></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">notes</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/RL">课程笔记</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a class="footer__link-item" href="/personal-essays">Personal Essays</a></li><li class="footer__item"><a href="https://github.com/facebook/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Ayanami, Inc. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>