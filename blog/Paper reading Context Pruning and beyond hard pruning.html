<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.4.0">
<title data-rh="true">Paper reading - Context Pruning and beyond hard pruning | Ayanami&#x27;s Cave</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://ayanami1314.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://ayanami1314.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://ayanami1314.github.io/blog/Paper reading Context Pruning and beyond hard pruning"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="Paper reading - Context Pruning and beyond hard pruning | Ayanami&#x27;s Cave"><meta data-rh="true" name="description" content="引子"><meta data-rh="true" property="og:description" content="引子"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2025-07-29T00:00:00.000Z"><meta data-rh="true" property="article:tag" content="rag,ai,llm,agent,embedding"><link data-rh="true" rel="icon" href="/img/ayanami.jpg"><link data-rh="true" rel="canonical" href="https://ayanami1314.github.io/blog/Paper reading Context Pruning and beyond hard pruning"><link data-rh="true" rel="alternate" href="https://ayanami1314.github.io/blog/Paper reading Context Pruning and beyond hard pruning" hreflang="en"><link data-rh="true" rel="alternate" href="https://ayanami1314.github.io/blog/Paper reading Context Pruning and beyond hard pruning" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","@id":"https://ayanami1314.github.io/blog/Paper reading Context Pruning and beyond hard pruning","mainEntityOfPage":"https://ayanami1314.github.io/blog/Paper reading Context Pruning and beyond hard pruning","url":"https://ayanami1314.github.io/blog/Paper reading Context Pruning and beyond hard pruning","headline":"Paper reading - Context Pruning and beyond hard pruning","name":"Paper reading - Context Pruning and beyond hard pruning","description":"引子","datePublished":"2025-07-29T00:00:00.000Z","author":{"@type":"Person","name":"ayanami"},"keywords":[],"isPartOf":{"@type":"Blog","@id":"https://ayanami1314.github.io/blog","name":"Blog"}}</script><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Ayanami&#39;s Cave RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Ayanami&#39;s Cave Atom Feed">



<link rel="alternate" type="application/rss+xml" href="/personal-essays/rss.xml" title="Ayanami&#39;s Cave RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/personal-essays/atom.xml" title="Ayanami&#39;s Cave Atom Feed">


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.00dd3480.css">
<script src="/assets/js/runtime~main.aa217668.js" defer="defer"></script>
<script src="/assets/js/main.0da58ad9.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/logo.svg" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Ayanami&#x27;s Cave</b></a><a class="navbar__item navbar__link" href="/docs/Chcore源码阅读">课程笔记</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">技术博客</a><a class="navbar__item navbar__link" href="/personal-essays">个人随笔</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/facebook/docusaurus" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><div class="navbar__search searchBarContainer_NW3z" dir="ltr"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">All our posts</div><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/llm for code paper notes">paper-reading, code&amp;rl方向</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/speculative-decode-overview">投机解码简述</a></li><li class="sidebarItem__DBe"><a aria-current="page" class="sidebarItemLink_mo7H sidebarItemLinkActive_I1ZP" href="/blog/Paper reading Context Pruning and beyond hard pruning">Paper reading - Context Pruning and beyond hard pruning</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/结构化输出">结构化输出与AI工具与Agent</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/context-engineering">context-engineering</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/从微调reranker到搜推">从微调reranker到搜推工程实践</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/llm-tech-report">部分llm技术报告的阅读</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/Ask in Any Modality A Comprehensive Survey on Multimodal Retrieval-Augmented Generation">Paper reading-Ask in Any Modality A Comprehensive Survey on Multimodal Retrieval-Augmented Generation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/精读 AAAI 2025 Fit and Prune Fast and Training-free Visual Token Pruning for Multi-modal Large Language Models">Paper reading - Fit and Prune Fast and Training-free Visual Token Pruning for Multi-modal Large Language Models</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/精读：Eagle Exploring The Design Space for Multi- modal LLMs with Mixture of Encoders">Paper reading-Eagle Exploring The Design Space for Multi- modal LLMs with Mixture of Encoders</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/RAG的一些思考和细节">RAG的一些思考与细节</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/精读  Interleaved Scene Graph for Interleaved Text-and-Image Generation Assessment">Paper reading - Interleaved Scene Graph for Interleaved Text-and-Image Generation Assessment</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/ColBERT">ColBERT-后期交互方法</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/2025/05/26/技术博客阅读">美团技术博客阅读</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/Milvus">稀疏神经嵌入</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/RocketMQ">RocketMQ学习</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/AI limu">李沐dl笔记</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/cs186-database-WIP">ucb cs186 课程笔记(更新中)</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/jyy-os-虚拟化">NJU操作系统(jyy OS)课程笔记-虚拟化部分</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/local-llm">来本地部署大模型!</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/ostep-chapter42-44">ostep阅读笔记：单机fs的崩溃一致性(chapter42-44)</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/系统架构设计笔记">system-design-interview笔记</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/JUC">JUC</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/cs144 labs">cs144 labs(Winter 2024)</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/jyy-os：并发">NJU操作系统(jyy OS)课程笔记-并发部分</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/nginx">nginx基础</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/cs144/cs144 lec notes">CS144 Lecture Notes</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/django-mosh">Django_mosh</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/splay-tree">splay tree</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/xv6book-notes">xv6book Notes(C1-4)</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/Go-Gin学习">Go,Gin学习</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/godis源码阅读">godis源码阅读</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/hibernate-jpa">hibernate&amp;jpa</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/linking-复习">linking 复习</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/ts基础">ts基础</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/实战2-mosh-gamehub">react practice:mosh gamehub</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/浅入理解断点和调试器">浅入理解断点和调试器</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/黑马点评">黑马点评(速通版)</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/js基础">js基础</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/11-14-11-26学习双周记">11-14-11-26学习双周记</a></li></ul></nav></aside><main class="col col--7"><article><header><h1 class="title_f1Hy">Paper reading - Context Pruning and beyond hard pruning</h1><div class="container_mt6G margin-vert--md"><time datetime="2025-07-29T00:00:00.000Z">July 29, 2025</time> · <!-- -->17 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><div class="avatar__intro"><div class="avatar__name"><span>ayanami</span></div></div></div></div></div></header><div id="__blog-post-container" class="markdown"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="引子">引子<a class="hash-link" aria-label="Direct link to 引子" title="Direct link to 引子" href="/blog/Paper reading Context Pruning and beyond hard pruning#引子">​</a></h2>
<p>我们知道，在现在Agent需要处理的一大问题是长上下文下性能的开销问题，对此infra团队有非常多的优化，从attention架构的优化如各种windowed attention到kv的压缩重用如cacheblend和megicdec等都提出了一系列的解决方案，但有一个最本质的方法是：有没有可能直接减少上下文的长度(去掉不必要的上下文?) ，这就是Context Pruning的出发点。</p>
<p>而截止2025年8月，相关的方  法已经发展了两三年了，大体上可以分成几个类别，本文会对此做一些简单的介绍和总结。</p>
<p>借用naver lab最新的相关论文里面的<a href="https://europe.naverlabs.com/blog/efficient-online-text-compression-for-rag/" target="_blank" rel="noopener noreferrer">说法</a>，现在的方法可以被一个四方格归纳：</p>
<p><img decoding="async" loading="lazy" src="https://europe.naverlabs.com/wp-content/uploads/2025/05/5-OnlineOfflineHardSoft.png" alt="" class="img_ev3q"></p>
<p>其中，Hard和Soft代表裁剪方法是直接作用于token上（hard，相当于裁剪结束后，输入的是一个新的prompt），还是作用于token的embedding上（soft，相当于裁剪结束后，输入的是一个新的qkv和其他东西，无法还原出“原始”的token输入）</p>
<p>在线和离线一般代表着这个裁剪方案是否依赖于用户查询q，依赖q的方案是在线的，不依赖q的方案是离线的，可以提前做好。但传统上，如果你的裁剪方法也需要用到和原始模型一样大的LLM，也一般称之为离线，或许“是否会对在线推理造成明显时延影响”做划分更好一些</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="离线硬裁剪">离线硬裁剪<a class="hash-link" aria-label="Direct link to 离线硬裁剪" title="Direct link to 离线硬裁剪" href="/blog/Paper reading Context Pruning and beyond hard pruning#离线硬裁剪">​</a></h3>
<p>在最早期的时候，就有相关的一些朴素方法，例如直接对查询文本段做一次总结摘要，再用总结后的文本段去做后续的任务，这种方法是离线硬裁剪的典型代表。如果用的是llm就是离线的，如果用轻量级模型做摘要或者总结就是在线的</p>
<p>而在后面的时候，出现了例如微软的llmlingua这样的工作，直接用一个小模型(gpt2 small，llama-7b, etc)去预测哪些token是重要的，哪些token是不重要的，然后把不重要的token直接裁剪掉，这种  方法也是离线硬裁剪的典型代表。(llmlingua2 换成了微调的 BERT 来做这个事情，所以可以说在线的)， 其出发点和常规的硬裁剪可能有部分地方不同，例如llmlingua认为，裁剪本身是可以得到一些人类不可读但是大模型可以理解的token序列的，所以可解释性上可能并没有想象的那么强。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="离线软裁剪">离线软裁剪<a class="hash-link" aria-label="Direct link to 离线软裁剪" title="Direct link to 离线软裁剪" href="/blog/Paper reading Context Pruning and beyond hard pruning#离线软裁剪">​</a></h3>
<p>和硬裁剪同时推进的是软裁剪相关的工作，其想法很简单: 如果我牺牲解释性，直接调整prompt的embedding这类，即使产生的是不对应任何token的&quot;fake embedding&quot;，其在高维空间中也应该融合了多个token的语义，理应得到更高的压缩率(可以理解为，在训练过程中为llm 扩充为无限词表，然后定义了一些高效的&quot;额外语言&quot;)</p>
<p>比较早期的工作是 xRAG， 其裁剪策略非常极端，将整个段落都压缩成1个embedding X，怎么训练呢？一个在此类论文中经常出现的是重建loss，即</p>
<p>压缩前<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi><mi>o</mi><mi>c</mi><mo>+</mo><mi>q</mi><mi>u</mi><mi>e</mi><mi>r</mi><mi>y</mi><mo>→</mo><mi>x</mi></mrow><annotation encoding="application/x-tex">Doc + query \to x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em"></span><span class="mord mathnormal">Doc</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">q</span><span class="mord mathnormal">u</span><span class="mord mathnormal" style="margin-right:0.03588em">ery</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">x</span></span></span></span></p>
<p>压缩后<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi><mi>o</mi><msup><mi>c</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>+</mo><mi>q</mi><mi>u</mi><mi>e</mi><mi>r</mi><mi>y</mi><mo>→</mo><msup><mi>x</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><annotation encoding="application/x-tex">Doc&#x27; + query \to x&#x27;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8352em;vertical-align:-0.0833em"></span><span class="mord mathnormal">Do</span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">q</span><span class="mord mathnormal">u</span><span class="mord mathnormal" style="margin-right:0.03588em">ery</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.7519em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span>, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo>=</mo><mi>L</mi><mo stretchy="false">(</mo><msup><mi>x</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mi>D</mi><mrow><mi>K</mi><mi>L</mi></mrow></msub><mo stretchy="false">(</mo><msup><mi>x</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">L=L(x&#x27;,x)=D_{KL}(x&#x27;,x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">L</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.0019em;vertical-align:-0.25em"></span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.0019em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em">K</span><span class="mord mathnormal mtight">L</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span>，  即自蒸馏，希望压缩后依然能重建原始的输出，论文实际中可能会用变体版本来实现指令遵循等</p>
<p>xRAG的做法是，使用一个通用编码器E，把这  个编码器E视作一个新的模态，仿照CLIP的方法直接用MLP projector做通用编码器和实际使用的LLM token embedding的模态对齐</p>
<p>但[大家实测下来](笔记：RAG 的相关优化方法之六（xRAG/PISCO） - 刀刀宁的文章 - 知乎
<a href="https://zhuanlan.zhihu.com/p/29292925032)%EF%BC%8CxRAG%E7%9A%84%E6%95%88%E6%9E%9C%E5%B9%B6%E4%B8%8D%E5%A5%BD%EF%BC%8C%E8%80%8C%E7%9B%B8%E5%AF%B9%E8%BE%83%E5%A5%BD%E7%9A%84%E6%98%AF%E6%9B%B4%E6%96%B0%E7%9A%84Pisco%E6%96%B9%E6%B3%95" target="_blank" rel="noopener noreferrer">https://zhuanlan.zhihu.com/p/29292925032)，xRAG的效果并不好，而相对较好的是更新的Pisco方法</a></p>
<p><img decoding="async" loading="lazy" src="https://s2.loli.net/2025/09/07/qYgLrSRHVJj4P5x.png" alt="Refer to caption" class="img_ev3q"></p>
<p>Pisco将检索到的文档D和memory tokens一起送到LLM中，产生embeddings</p>
<p>再将embeddings +query送到相同的LLM中，产生输出，这个 q+E 和原始的 q+D 比较， 计算交叉熵损失</p>
<p>这里有一些复杂的地方:</p>
<ul>
<li>
<p>虽然叫解码和编码，但是Student LLM都是同一个LLM, 只是训练不同LoRA模块</p>
</li>
<li>
<p>交叉熵是怎么得出的? teacher模型和student模型都是采用的最大长度128的贪婪解码，就可以直接令 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo>=</mo><mo>−</mo><mo>∑</mo><mn>1</mn><mi>l</mi><mi>o</mi><mi>g</mi><mi>p</mi><mo>+</mo><mn>0</mn><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>p</mi><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><msub><mo>∑</mo><mi>i</mi></msub><mi>l</mi><mi>o</mi><mi>g</mi><mi>P</mi><mo stretchy="false">(</mo><msub><mi>a</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi><mi>q</mi><mo separator="true">,</mo><mi>e</mi><mo separator="true">,</mo><msub><mi>a</mi><mrow><mo>&lt;</mo><mi>i</mi></mrow></msub><mo separator="true">,</mo><msub><mi>θ</mi><mi>c</mi></msub><mo separator="true">,</mo><msub><mi>θ</mi><mi>d</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">L=-\sum 1logp + 0log(1-p) = - \sum_i log P(a_i|q,e,a_{&lt;i},\theta_c,\theta_d) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">L</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">−</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop op-symbol small-op" style="position:relative;top:0em">∑</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">1</span><span class="mord mathnormal" style="margin-right:0.01968em">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em">g</span><span class="mord mathnormal">p</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">0</span><span class="mord mathnormal" style="margin-right:0.01968em">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em">g</span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">p</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.0497em;vertical-align:-0.2997em"></span><span class="mord">−</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.162em"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2997em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.01968em">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em">g</span><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.03588em">q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">e</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mrel mtight">&lt;</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1774em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>， 优化目标是 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>θ</mi><mi>c</mi></msub></mrow><annotation encoding="application/x-tex">\theta_c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>θ</mi><mi>d</mi></msub></mrow><annotation encoding="application/x-tex">\theta_d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> 还有 memory_tokens</p>
</li>
<li>
<p>如何理解memory token? 我觉得文章是借用了之前的一些研究比如ICAE, 在这些 文章之中，训练的压缩机制是，将上下文压缩成一个定长的memory slot, 这里的memory token实际上只是多个embedding向量而已，而更关键的是LoRA微调的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>θ</mi><mi>c</mi></msub></mrow><annotation encoding="application/x-tex">\theta_c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>，我的理解是，memory tokens只是一个后置的、可以看到Documents的所有信息（假设它没有魔改注意力）的语义位置，叫tokens也可以理解为直接扩了词表加入了l个特殊token，类似BERT里面的<code>[BOS]</code> ，只是decoder llm需要后置。</p>
<ul>
<li>文章并没有细说这里的注意力是怎么设置的，但从后文中发现的memory tokens具有明显的位置特性（例如1位mem token主要注意最开头一段），感觉应该是没改过</li>
</ul>
</li>
<li>
<p>文章的另一个重要的实验结论是，微调student llm(<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>θ</mi><mi>d</mi></msub></mrow><annotation encoding="application/x-tex">\theta_d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>)是必要的，之前的研究中没有相关模块，会导致性能的大幅度下降。这细想其实是一个很有趣的事情，可以注意到，压缩的时候是没有接触到query信息的（这也是为什么称为离线的原因），可以理解为某种意义上的LLM as an embedder，而加入了query和embedding再训练的时候，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>θ</mi><mi>d</mi></msub></mrow><annotation encoding="application/x-tex">\theta_d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>一边学会了如何理解自己产生的embedding，另一方面学会了如何根据query去选择embedding，整体上类似于ColBERT架构的Reranker（前面是multi-vec embed, 后面是maxsim）</p>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="在线硬裁剪">在线硬裁剪<a class="hash-link" aria-label="Direct link to 在线硬裁剪" title="Direct link to 在线硬裁剪" href="/blog/Paper reading Context Pruning and beyond hard pruning#在线硬裁剪">​</a></h3>
<p>Provence</p>
<p>之前的裁剪方案只注重于“自然语言是有冗余的”，所以主要做的都是token-level的pruning，而provence则更注重实际一些，它发掘了一个问题是，其实现在RAG里面的 “Chunk” 是一个特别微妙的概念</p>
<p>如果chunk切得大了，那上下文自然就长了，甚至效果也会明显下降（详见ground truth在chunk中的不同位置的position bias相关的研究，现有embedder对这个bias耐受性不佳，会狠狠掉点）；但如果chunk切得小了，语义信息的丢失、检索的困难又是很恼人的事情（先不论检索，检索到了多个小块之后信息不够怎么办？一种是合并，但策略怎么定？另一种是Anthropic的Contextual Retrieval，把上下文放进来，本质上还是变成大块（我说这个a一串真是炒作勾啊.jpg））。</p>
<p>而Provence给了一个折中的方案，既然我们有句子级别的语义，为什么不用呢？分几步走</p>
<ol>
<li>训练一个接受q,d的BERT，给每一个token打0~1分，并根据用户指定的阈值进行二值化变为0/1, 表示删除/留下</li>
<li>进行句子级别的聚类，裁剪掉0的token数量大于1的token数量的句子</li>
</ol>
<p>如何训练呢？选取有5~10个句子的段（可以多次选取来拓展到更长的上下文），标上句子序号，让LLM选择相关句子来产生label，从而训练模型</p>
<p>这里其实做了很有意思的工程设计，</p>
<ul>
<li>
<p>如果让LLM来打token-level的 标，肯定是收集不到足够的样本的，并且真的无所谓多出来的几个token，更在意句意的完整性</p>
</li>
<li>
<p>BERT带来了相当多的好处:</p>
<ul>
<li>
<p>这样进行的句子裁剪，每个句子都可以和整个chunk里面的所有上下文交互，使得一个句子的保留与否不仅取决于这个句子和查询的相关性，还取决于其于其他（和查询相关性高的）句子的相关性，这就使得这个方法必然会优于按句子切分的朴素方法</p>
</li>
<li>
<p>我们的 reranker 也就是个BERT啊，完全可以训裁剪和训rerank一起进行，推的时候也一样，相当于和rerank overlap了</p>
<p><img decoding="async" loading="lazy" src="https://s2.loli.net/2025/09/07/yxnpqQOfW6lv8zg.png" alt="image/png" class="img_ev3q"></p>
</li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="在线软裁剪">在线软裁剪<a class="hash-link" aria-label="Direct link to 在线软裁剪" title="Direct link to 在线软裁剪" href="/blog/Paper reading Context Pruning and beyond hard pruning#在线软裁剪">​</a></h3>
<p>Oscar</p>
<p>Pisco为代表的离线软裁剪有一个问题是，它的压缩需要微调，并且受限于难以对齐encoder-only架构的预训练编码器模态和实际推理使用的decoder LLM的模态，难以把压缩这一步在线做</p>
<p>Oscar就提出了一种方法是，我的对齐既然难做，我直接不对齐了，使用LLM的前L层 + memory token(他们也做了用Llama硬对齐的版本)，足以得到够好的embedding，文章最大的贡献其实是实验证明了这样表达能力已经足够，能训出来（太神奇了LLM）。当然，L越大效果越好</p>
<p>而还是复用Provence的工程技巧，把裁剪和rerank overlap起来，OSCAR的compressor留了一个RR头，在这个头和Teacher Reranker对齐，整体的Loss就是rerank loss + generation loss</p>
<p>而令LLM理解embedding这件事情还是通过LoRA adapter来做，这篇文章其实像是序列工作  的延申，综合了PISCO的训练方法，把PISCO的压缩部分从LLM + LoRA换成目标模型的前N层transformer，然后压缩器全参微调、生成器LoRA微调，再使用和Provence相同的技巧进行rerank的overlap</p>
<p><img decoding="async" loading="lazy" src="https://s2.loli.net/2025/09/07/pDRSe2sCwFabN8X.png" alt="image-20250907213839337" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="异曲同工">异曲同工<a class="hash-link" aria-label="Direct link to 异曲同工" title="Direct link to 异曲同工" href="/blog/Paper reading Context Pruning and beyond hard pruning#异曲同工">​</a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="从hyde到投机解码">从HyDE到“投机解码”<a class="hash-link" aria-label="Direct link to 从HyDE到“投机解码”" title="Direct link to 从HyDE到“投机解码”" href="/blog/Paper reading Context Pruning and beyond hard pruning#从hyde到投机解码">​</a></h4>
<p>另一个有趣的工作是广义上的“裁剪”，或者就是更好的搜索吧。我们知道HyDE的思想是原始query一般都比较短，而生成的假设文档可能会更好地与索引文档对齐，所以使用 q‘ = q + generated d 来进行搜索。</p>
<p>而智谱的<a href="https://arxiv.org/abs/2409.05591" target="_blank" rel="noopener noreferrer">memorag</a> 则提出了这样一种场景，我们是否能以低成本训练一个小模型，来根据源文本生成这个假设答案呢？（例如，使用Llama3-8B在哈利波特上训练比用Deepseek-R1在哈利波特上训练成本要低廉的多，将HyDE的生成方从R1自己换成小模型）这就非常像是投机解码的思想了</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="外接模块-memory-decoder-catridges">外接模块: memory decoder, catridges<a class="hash-link" aria-label="Direct link to 外接模块: memory decoder, catridges" title="Direct link to 外接模块: memory decoder, catridges" href="/blog/Paper reading Context Pruning and beyond hard pruning#外接模块-memory-decoder-catridges">​</a></h4>
<p>其实这种将memory训为embedding的方法确实不少，如果说前面的压缩器是在训一个meta network，能够从doc生成embedding的话，外接模块的工作就是在训练embedding本身 -&gt; 我能否直接从一个大的文档库中训练出一个参数化的memory?</p>
<p>最近的<a href="https://www.arxiv.org/abs/2508.09874" target="_blank" rel="noopener noreferrer">memory decoder</a>选择的是直接扭曲生成过程，将一个小模型在目标适配数据集上训练，在大模型生成token时，将小模型的概率和大模型的概率相加（再重归一化），认为这样会带来领域知识的纠正（比较暴力www）</p>
<p>而另一篇<a href="https://arxiv.org/abs/2506.06266" target="_blank" rel="noopener noreferrer">catridges</a> 则是在使用类似P-tuning的方式训一个Prefix KVCache，在推理时实时加载，而希望这个KV中有相关的memory</p>
<p>包括一系列的kvcache evict的工作也是在做类似的东西，为了决定evict哪些甚至都把搜索又搬上来了，比如clusterKV的knn(笑)</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="总结">总结<a class="hash-link" aria-label="Direct link to 总结" title="Direct link to 总结" href="/blog/Paper reading Context Pruning and beyond hard pruning#总结">​</a></h3>
<p>总体而言，我感觉相关工作已经进入了深水区了，硬裁剪可能在某些程度上到头了，现在主流在探索一些牺牲解释性的，更能scale out的方法来进行参数化memory来解决长上下文、领域适配等一系列问题</p>
<p>而大家方法逐渐趋向于无标签学习的统一也再次证明了scale out能力在广义embedding能力的训练上的重要性</p>
<p>另一个很有意思的是，可以看到搜索中的多向量和多memory token有一些很有趣的相似性，或许在后续的一些工作中，我们能看到一些多向量的方法被用到memory之中，希望会让memory这个很多时候靠prompt编故事的领域更多可验证性吧</p>
<p>而从另一个方面，正如这里列出的部分文章说的，自从eagle在投机解码中得到了确实很好的效果之后，大家都开始用 token + embedding的混合来捕捉更强的信息了，还有HyDE和投机解码这种很有趣的对应</p></div><footer class="docusaurus-mt-lg"><div class="row margin-top--sm theme-blog-footer-edit-meta-row"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/rag">rag</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/ai">ai</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/llm">llm</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/agent">agent</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/embedding">embedding</a></li></ul></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/blog/speculative-decode-overview"><div class="pagination-nav__sublabel">Newer Post</div><div class="pagination-nav__label">投机解码简述</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/blog/结构化输出"><div class="pagination-nav__sublabel">Older Post</div><div class="pagination-nav__label">结构化输出与AI工具与Agent</div></a></nav><div>Loading Comments...</div></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a class="table-of-contents__link toc-highlight" href="/blog/Paper reading Context Pruning and beyond hard pruning#引子">引子</a><ul><li><a class="table-of-contents__link toc-highlight" href="/blog/Paper reading Context Pruning and beyond hard pruning#离线硬裁剪">离线硬裁剪</a></li><li><a class="table-of-contents__link toc-highlight" href="/blog/Paper reading Context Pruning and beyond hard pruning#离线软裁剪">离线软裁剪</a></li><li><a class="table-of-contents__link toc-highlight" href="/blog/Paper reading Context Pruning and beyond hard pruning#在线硬裁剪">在线硬裁剪</a></li><li><a class="table-of-contents__link toc-highlight" href="/blog/Paper reading Context Pruning and beyond hard pruning#在线软裁剪">在线软裁剪</a></li><li><a class="table-of-contents__link toc-highlight" href="/blog/Paper reading Context Pruning and beyond hard pruning#异曲同工">异曲同工</a><ul><li><a class="table-of-contents__link toc-highlight" href="/blog/Paper reading Context Pruning and beyond hard pruning#从hyde到投机解码">从HyDE到“投机解码”</a></li><li><a class="table-of-contents__link toc-highlight" href="/blog/Paper reading Context Pruning and beyond hard pruning#外接模块-memory-decoder-catridges">外接模块: memory decoder, catridges</a></li></ul></li><li><a class="table-of-contents__link toc-highlight" href="/blog/Paper reading Context Pruning and beyond hard pruning#总结">总结</a></li></ul></li></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">notes</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/RL">课程笔记</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a class="footer__link-item" href="/personal-essays">Personal Essays</a></li><li class="footer__item"><a href="https://github.com/facebook/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Ayanami, Inc. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>